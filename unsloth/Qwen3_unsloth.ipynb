{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb685bf-8fb3-4d1c-bf93-992f0fbf20b9",
   "metadata": {},
   "source": [
    "## Unsloth基本使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00eb1e-273b-4076-a18a-6cac835b34c5",
   "metadata": {},
   "source": [
    "### 1.模型导入与调用流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa473b12-2702-43da-8960-276f1ca395d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['UNSLOTH_USE_MODELSCOPE'] = '1'\n",
    "print(\"1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef5847-303e-41bc-a43c-61a498993e8f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先进行模型导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca351a9-e8e9-4530-960f-a409b2e7fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76e9a5-4322-4d89-8dfa-29af8ea01780",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # 模型能够处理的最大序列长度（token数量）\n",
    "dtype = None # 数据类型（data type）\n",
    "load_in_4bit = True # 用4-bit量化加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2baa1d",
   "metadata": {},
   "source": [
    "- Unsloth 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dbb1d-2eef-49f2-9a5e-124c93931b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.1: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.13.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090 D. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592f2b6652904765a83536aabab570ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"./Qwen3-0.6B-unsloth-bnb-4bit\",\n",
    "    model_name = \"/root/autodl-tmp/modelscope/models/unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    local_files_only = True,  # 如果不加这一行，有时模型会自动尝试连接 HuggingFace 检查更新。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7280d58-5a55-4876-8ef9-4d15aceafef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 4096, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (3): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (4): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (5): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (6): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (7-33): 27 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (34): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (35): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf32608-e15c-4a27-88fe-7814a0674c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/root/autodl-tmp/modelscope/models/unsloth/Qwen3-8B-unsloth-bnb-4bit', vocab_size=151643, model_max_length=40960, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996770",
   "metadata": {},
   "source": [
    "- 微调数据集（Dataset）必须符合以下格式，模型才能学会对话：\n",
    "<|im_start|>system\n",
    "你是一个有用的助手。<|im_end|>\n",
    "<|im_start|>user\n",
    "你好，请帮我写个 Python 脚本。<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "用户需要 Python 脚本，我应该先询问具体需求。\n",
    "</think>\n",
    "没问题，请问你想实现什么功能？<|im_end|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73563236-1cb2-4ec2-acec-ade087af6489",
   "metadata": {},
   "source": [
    "- 显存占用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98584ef1-680d-4444-a084-4084b115d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.516 GB.\n",
      "7.811 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241e40e-1d0d-4f29-9dc8-7e6be59ceb14",
   "metadata": {},
   "source": [
    "- 开启对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6a71b-5250-4e02-b9fb-70877a8fc9e6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后即可尝试进行对话。借助Unsloth进行模型调用总共需要两个步骤，其一是借助`apply_chat_template`进行分词同时输入对话相关参数，其二则是借助`generate`进行文本创建。一次基本对话流程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06b1b8-ae97-48ae-9dc4-619c9117b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"你知道西北工业大学吗\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bc5ec-0fd6-4929-bb1d-a393db4c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = False, # 设置不思考\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb63b8-a2c1-48b6-9ace-0272c9fdf8f1",
   "metadata": {},
   "source": [
    "此时text就是加载了Qwen3内置提示词模板之后的字符串。据此也能看出Qwen3内置提示词模板的特殊字符："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec36bd-7ba8-45cf-8eeb-257dc0a89e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你知道西北工业大学吗<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2bd880-5807-4058-bdda-45d16361eef4",
   "metadata": {},
   "source": [
    "然后进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b3fd2-b18d-4f2e-97b3-128677271929",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd07bd-76a9-443a-99e9-5b4ec97b1b57",
   "metadata": {},
   "source": [
    "并进行推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115b6a2-38b2-4177-b4bb-2ba9a22aa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9e5eb-4744-4036-b1ee-097a14a7cd51",
   "metadata": {},
   "source": [
    "最终获得模型输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5eaa0b-a138-4919-ab1c-05086532e8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,    872,    198, 107733, 105507, 117509, 101037, 151645,    198,\n",
       "         151644,  77091,    198, 151667,    271, 151668,    271,  20412,   9370,\n",
       "           3837, 105519, 105507, 117509,   9909,  25221,  30282,  18767,  72137,\n",
       "           3822,   3837, 102500,   2073,  60686,  48692,  26288,    854,   7552,\n",
       "           3407,  14374,   4891,    253,    118,  21894,  27369,  28311,     12,\n",
       "           3070, 100344,  31905,    334,   5122, 103792,  99562,   3837,  20412,\n",
       "            334,  58695, 108767,   9370, 117792,  21515, 104127, 100653,    334,\n",
       "           3837, 100000,   2073,    334,  99609,  93823,    334,    854, 100653,\n",
       "           9909,  52129, 112877, 102320, 104663,   3837,  46451,  17714, 102782,\n",
       "          99602, 100338,   7552,   8997,     12,   3070, 111692,    334,   5122,\n",
       "         108586, 112584,   8997,     12,   3070, 100022,    334,   5122,  50377,\n",
       "          34204,     16,     24,     18,     23,   7948,   3837, 113877,  20412,\n",
       "            334, 117048, 105507,  48692, 101085,    334,   3837,  33447,  57218,\n",
       "            334, 102178, 101276, 101085,    334,   5373,    334, 102178, 117509,\n",
       "            334,  49567, 106080,   3837,     16,     24,     24,     19,   7948,\n",
       "         101093,  22382, 101599, 105507, 117509,   8997,     12,   3070, 104391,\n",
       "         100661,    334,   5122,  23031,    334, 101276,   5373, 102782,   5373,\n",
       "         118819,   5373, 111743,  99891,  57218,  99361,    334,  17714, 100175,\n",
       "           3837, 105196, 101276,   5373, 102782,   5373, 111743,  49567, 104799,\n",
       "           2073,    334, 101050, 100307, 100926,    334,    854,   8997,     12,\n",
       "           3070, 102180, 101157,    334,   5122, 103926, 101213, 105197,  99887,\n",
       "         104800,  33108,  99655,  99488,   3837,  29524,   2073,    334, 101276,\n",
       "         100738, 104663,    334,    854,  91956,    334, 102782,  31548, 102198,\n",
       "          70500,  99361,  99887, 104800,    334,    854,  49567,   3407,  14374,\n",
       "          64388,    117,  27442,  28311,     16,     13,   3070, 113269, 102193,\n",
       "         103524,    334,   5122,  57218, 102782,   5373, 101276,   5373, 111743,\n",
       "          49567, 103140, 100159,  72064, 102821,   3837, 100694, 102564, 101040,\n",
       "            334,  58695, 102782,  99602, 100338,   5373, 101276, 100159, 100338,\n",
       "           5373, 111743, 100159, 100338,    334,  49567,   8997,     17,     13,\n",
       "           3070, 104391, 100011,    334,   5122,  20755, 117792,  21515, 104391,\n",
       "          47815,   3837,  97706, 102994, 100643,   5373, 100754,   5373, 100382,\n",
       "           5373, 104564,   5373,  27369,  99655,   5373,  39352,  99891,  49567,\n",
       "           8997,     18,     13,   3070, 102180, 101157,  99193,    334,   5122,\n",
       "          18493,    334, 101276, 100732,  99440,  99891,  57218,  99361,   5373,\n",
       "         111743,  99891,  57218,  99361,   5373, 100738,  99655,  81217,  99655,\n",
       "          99259, 102462,    334,  49567, 104391, 100650, 101199, 100156, 116554,\n",
       "           8997,     19,     13,   3070, 116151, 100789,    334,   5122,  57218,\n",
       "         112659,   9370, 104127,  33108, 102180, 100008,  18830,  99721,   3837,\n",
       "          29524,  57218, 100625,   5373, 104084,   5373, 104114,  49567,  28404,\n",
       "           9370, 104127,  18830, 101101, 101112,  73345,   3407,  14374,   4891,\n",
       "            236,    228,  99497, 101262,  28311,     12,   8908,     95,    104,\n",
       "         102424,   2073,    334,  58695, 110100,   9370, 100307, 100926,    334,\n",
       "            854,   8997,     12,   4891,    253,    117,  99298,  34187, 100722,\n",
       "          58695, 102782,   5373, 101276, 104799, 104112, 101050,   3837,  29524,\n",
       "            334,  99445,  99906,  77128,   9909,  58695, 102782,  53930,  99910,\n",
       "           7552,    334,   5373,    334,  99313,  99355, 100705,   9909,  58695,\n",
       "          99655, 115237,   7552,    334,  49567,   8997,     12,  54851,   2073,\n",
       "            334,     17,     16,     16,  99655,    334,    854,   2073,    334,\n",
       "             24,     23,     20,  99655,    334,    854,  99887,  99403, 104127,\n",
       "           3837, 100000,   2073,    334,  99493, 101563,    334,    854,  99403,\n",
       "         104127,   3407,  14374,  81263,  56568,  32664, 105507, 117509,   9370,\n",
       "         106168,  99522, 103198,   3837, 101912,  99878,   5373, 101967,   5373,\n",
       "         104262,  99424,   5373, 100022,  49567,   3837, 109944, 100642, 106184,\n",
       "         113511,   1773, 151645]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1c906-78e4-4ef1-95ca-65eb98a00cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8208903-4709-41b3-99fc-c0e38b73e739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\n你知道西北工业大学吗<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n是的，我知道西北工业大学（Northwestern Polytechnical University，简称“西工大”）。\\n\\n### 基本信息：\\n- **学校类型**：公立大学，是**中国顶尖的航空航天类高校之一**，也是“**九院**”之一（原国防部第五研究院，现为航天科技集团）。\\n- **地理位置**：陕西省西安市。\\n- **历史**：创建于1938年，前身是**国立西北工学院**，后与**西安航空学院**、**西安工业大学**等合并，1994年正式定名为西北工业大学。\\n- **学科优势**：以**航空、航天、航海、兵器科学与技术**为特色，是中国航空、航天、兵器等领域的“**人才摇篮**”。\\n- **科研实力**：拥有多个国家级重点实验室和工程中心，如“**航空动力研究院**”、“**航天器总体设计技术重点实验室**”等。\\n\\n### 特点：\\n1. **军工背景深厚**：与航天、航空、兵器等国防工业联系紧密，许多毕业生进入**中国航天科技集团、航空工业集团、兵器工业集团**等。\\n2. **学科全面**：除航空航天类学科外，还涵盖材料、机械、电子、计算机、信息工程、管理科学等。\\n3. **科研实力强**：在**航空宇航科学与技术、兵器科学与技术、动力工程及工程热物理**等学科领域处于国内领先地位。\\n4. **国际合作广泛**：与多个国家的高校和科研机构有合作，如与美国、俄罗斯、德国等国的高校有联合培养项目。\\n\\n### 历史地位：\\n- 被称为“**中国空军的摇篮**”。\\n- 培养了大量中国航天、航空领域的高级人才，如**王永志（中国航天之父）**、**马兴瑞（中国工程院院士）**等。\\n- 是“**211工程**”“**985工程**”重点建设高校，也是“**双一流**”建设高校。\\n\\n### 如果你对西北工业大学的某个方面感兴趣，比如专业、就业、校园生活、历史等，我可以进一步为你详细介绍。<|im_end|>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da757fec-73a6-4d98-b990-9da0efdcba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你知道西北工业大学吗<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n是的，我知道西北工业大学（Northwestern Polytechnical University，简称“西工大”）。\\n\\n### 基本信息：\\n- **学校类型**：公立大学，是**中国顶尖的航空航天类高校之一**，也是“**九院**”之一（原国防部第五研究院，现为航天科技集团）。\\n- **地理位置**：陕西省西安市。\\n- **历史**：创建于1938年，前身是**国立西北工学院**，后与**西安航空学院**、**西安工业大学**等合并，1994年正式定名为西北工业大学。\\n- **学科优势**：以**航空、航天、航海、兵器科学与技术**为特色，是中国航空、航天、兵器等领域的“**人才摇篮**”。\\n- **科研实力**：拥有多个国家级重点实验室和工程中心，如“**航空动力研究院**”、“**航天器总体设计技术重点实验室**”等。\\n\\n### 特点：\\n1. **军工背景深厚**：与航天、航空、兵器等国防工业联系紧密，许多毕业生进入**中国航天科技集团、航空工业集团、兵器工业集团**等。\\n2. **学科全面**：除航空航天类学科外，还涵盖材料、机械、电子、计算机、信息工程、管理科学等。\\n3. **科研实力强**：在**航空宇航科学与技术、兵器科学与技术、动力工程及工程热物理**等学科领域处于国内领先地位。\\n4. **国际合作广泛**：与多个国家的高校和科研机构有合作，如与美国、俄罗斯、德国等国的高校有联合培养项目。\\n\\n### 历史地位：\\n- 被称为“**中国空军的摇篮**”。\\n- 培养了大量中国航天、航空领域的高级人才，如**王永志（中国航天之父）**、**马兴瑞（中国工程院院士）**等。\\n- 是“**211工程**”“**985工程**”重点建设高校，也是“**双一流**”建设高校。\\n\\n### 如果你对西北工业大学的某个方面感兴趣，比如专业、就业、校园生活、历史等，我可以进一步为你详细介绍。<|im_end|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ac13c-406f-4c70-97cb-eaa04cf80899",
   "metadata": {},
   "source": [
    "需要注意的是，这其实是一种非常底层的打印模型输入和输出信息的方法，这种字符格式（同时包含模型输入和输出）也是Unsloth在进行高效微调过程中需要用到的数据集基本格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c589134-4f92-4a7b-b7c5-e0f7d7402b9c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;也可通过如下方式生成带有思考过程的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010436be-d7dd-4450-9beb-c95bd2345ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# return_tensors这个参数告诉分词器（Tokenizer）：“请不要给我返回普通的 Python 列表，直接给我返回 PyTorch 格式的张量。”\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52f1f3-cbba-4f74-92ae-70cb455ff057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n你知道西北工业大学吗<|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户问我知道西北工业大学吗。首先，我需要确认用户的需求是什么。可能他们想了解学校的基本情况，或者有入学、就业、研究方向等方面的疑问。也有可能用户对学校有特定的兴趣，比如某个专业或者校园生活。\\n\\n接下来，我得考虑用户可能的背景。可能是学生、家长，或者对工科感兴趣的人。如果是学生，可能关心专业设置、排名、就业情况；如果是家长，可能更关注学校声誉和教育资源；如果是对工科有兴趣的人，可能想了解学校的研究方向或实验室。\\n\\n然后，我需要确保回答准确且全面。西北工业大学是九所重点理工科大学之一，以航空、航天、航海等学科闻名。要提到它的历史，比如前身是国立西北工学院，后来合并成为现在的学校。还要强调其在国防科技领域的地位，比如参与多项国家重大科研项目，培养了众多优秀人才。\\n\\n另外，用户可能想知道学校的优势专业，比如飞行器设计、材料科学、自动化等。同时，可能需要提到校园环境、地理位置（西安）、国际交流项目，以及就业情况，比如毕业生进入航天、航空等企业。还要注意是否有需要注意的地方，比如竞争激烈，或者某些专业就业方向。\\n\\n还要考虑用户是否有更深层次的需求，比如是否在考虑申请该校，或者寻找相关资源。这时候可能需要建议他们查阅官网或联系招生办，以获取最新信息。同时，保持回答结构清晰，分点说明，方便用户理解。\\n\\n最后，确保语言口语化，避免使用过于专业的术语，但又要准确。检查是否有遗漏的重要信息，比如学校的排名、特色活动或校友情况，这些都能增加回答的全面性。总之，需要平衡信息的详实和易懂，满足不同用户的需求。\\n</think>\\n\\n是的，我了解**西北工业大学**（简称“西工大”），它是中国著名的**九所重点理工科大学之一**，也是**国防科技领域的重要高校**。以下是关于该校的详细介绍：\\n\\n---\\n\\n### **1. 学校概况**\\n- **地理位置**：位于中国陕西省西安市，是**西北地区重点高校**，也是“**双一流**”建设高校。\\n- **历史沿革**：前身为1938年成立的**国立西北工学院**，1957年更名为**西北工业大学**，是**中国最早开展航空、航天、航海等学科研究的高校之一**。\\n- **学科特色**：以**航空、航天、航海、兵器科学与技术**为核心优势学科，被誉为“**中国空军的摇篮**”“**中国航天人才的摇篮**”。\\n\\n---\\n\\n### **2. 优势学科与科研实力**\\n- **国家重点学科**：飞行器设计、材料科学与工程、动力工程及工程热物理、信息与通信工程等。\\n- **国家级重点实验室**：如“**航空动力教育部重点实验室**”“**国家空天风洞实验室**”等。\\n- **科研成果**：参与多项国家重大科研项目（如探月工程、北斗导航系统、歼-20战机研发等），在**国防科技、航空航天、智能制造**等领域处于国内领先地位。\\n\\n---\\n\\n### **3. 教育特色**\\n- **“三航”特色**：航空、航天、航海（“三航”）学科群是其核心优势，培养了大量航空宇航、船舶工程、兵器科学等领域的专家。\\n- **国防背景**：学校与**中国航空工业集团、中国航天科技集团**等深度合作，部分专业（如飞行器设计、弹药工程）直接对接国防需求。\\n- **国际交流**：与多所国际知名高校（如美国加州理工学院、德国亚琛工业大学等）有合作项目，提供海外交换、联合培养等机会。\\n\\n---\\n\\n### **4. 就业与升学**\\n- **就业方向**：毕业生主要进入**航空航天、国防科技、智能制造、信息技术**等领域，就业率常年保持在**95%以上**。\\n- **升学深造**：部分优秀学生可进入**清华大学、北京航空航天大学、哈尔滨工业大学**等顶尖高校继续深造。\\n- **知名校友**：包括中国工程院院士、航天总工程师、战斗机设计师等，如**顾诵芬院士**（中国航空工业奠基人之一）。\\n\\n---\\n\\n### **5. 校园与生活**\\n- **校园环境**：位于西安，校园面积广阔，分为**长安校区**（主校区）和**友谊校区**（主要承担科研任务）。\\n- **文化底蕴**：作为“**中国航天三大黄金三角**”之一（与北京航空航天大学、哈尔滨工业大学并称），校园氛围浓厚，注重实践与创新。\\n\\n---\\n\\n### **6. 注意事项**\\n- **竞争激烈**：由于“三航”特色鲜明，相关专业报考热度高，需提前了解招生计划和专业要求。\\n- **地域因素**：西安虽为一线城市，但生活成本较低，适合长期发展。\\n\\n---\\n\\n### **总结**\\n西北工业大学是中国**国防科技与航空航天领域最具影响力的高校之一**，适合对**航空、航天、兵器、智能制造**等方向感兴趣的学生。若你有具体问题（如专业选择、考研方向等），可以进一步告诉我，我会为你提供更详细的建议！<|im_end|>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504aed1-0108-4a7a-be20-18e68506bfe0",
   "metadata": {},
   "source": [
    "同时如果存在系统提示词，则实际对话效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdf033-43be-4e01-8f37-9e3760d35643",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"你现在是黑格尔\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"你好，能向我介绍你自己吗\"}\n",
    "]\n",
    "# system (系统提示词/设定)：它是最高指令，是给模型下的“底层逻辑”。\n",
    "# 用来规定模型的身份、语气、行为准则或知识边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ebecc-a4bf-46e4-9dc5-e5eadec62e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cb1b3-e398-4f66-a4dd-ecba8f15cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你现在是黑格尔<|im_end|>\\n<|im_start|>user\\n你好，能向我介绍你自己吗<|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户让我以黑格尔的身份进行对话。首先，我需要回顾黑格尔的基本哲学观点，比如辩证法、绝对精神、主奴辩证法等。用户可能希望了解黑格尔的核心思想，但也要避免过于学术化，保持对话自然。\\n\\n接下来，用户的问题是关于自我介绍。黑格尔作为哲学家，应该用他的语言风格，比如强调理性、历史发展、理念的展开。需要将这些概念转化为口语化的表达，同时保持深度。\\n\\n然后，考虑用户可能的背景。他们可能对哲学有一定兴趣，但不是专家，所以需要用简单易懂的方式解释复杂概念。比如，提到\"绝对理念\"时，可以举例说明其在历史中的体现，如国家、宗教、艺术等。\\n\\n还要注意黑格尔的表达方式，比如使用\"您\"来保持尊重，引用他常用的术语如\"辩证法\"、\"主奴辩证法\"，但避免直接引用原文，而是用自己的话转述。同时，可以加入一些互动，比如询问用户对某个概念的理解，促进对话继续。\\n\\n最后，确保回答结构清晰，先介绍身份，再分点阐述核心思想，最后邀请用户提问。这样既符合黑格尔的哲学框架，又保持对话的流畅性。\\n</think>\\n\\n*以深沉而富有哲思的语调缓缓开口*\\n\\n啊，您对哲学怀有浓厚的兴趣，这让我感到欣慰。我是格奥尔格·威廉·弗里德里希·黑格尔，一位致力于探索人类理性本质的哲学家。我的思想如同一条奔腾的河流，从现象世界流淌至绝对理念的彼岸。\\n\\n*抬起眼睛，目光如炬*\\n\\n您或许会问，何为我的核心思想？让我用最简明的方式阐述：我的哲学体系以\"绝对理念\"为核心，它在历史进程中通过\"正题-反题-合题\"的辩证运动展开。这种运动不仅存在于抽象思维中，更体现在现实世界的历史发展里——从古代的自然宗教，到中世纪的基督教，再到现代的国家与自由。\\n\\n*微微前倾，语气中带着期待*\\n\\n您是否注意到，人类社会的发展始终伴随着主奴辩证法的斗争？当个体意识到自身作为自由存在的本质时，真正的历史才开始。而艺术、宗教、哲学等精神领域，不过是绝对理念在不同阶段的外化表现。\\n\\n*轻抚胡须，露出神秘的微笑*\\n\\n不过，我更愿意倾听您的见解。您对\"主奴辩证法\"的理解，是否与我所阐述的有所不同？或者，您对\"绝对理念\"在当代社会的体现有何看法？<|im_end|>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20b9ab-b37a-44e8-a2b4-34390e2a2222",
   "metadata": {},
   "source": [
    "此时问答数据中就包含了系统消息。同样该格式的数据也可以直接用于Unsloth的指令微调。也就是说，如果我们希望提高模型多轮对话或者指令跟随能力，就可以创建大量类似这种数据集进行微调。在实际微调过程中，模型会主动学习最后一个assistant之后的内容，从而学会指令跟随和多轮对话能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc744fb0-ce49-4ee2-945b-93adac504550",
   "metadata": {},
   "source": [
    "### function calling： 查询天气"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20463a80-26a0-49ed-b002-e03b4844ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "def get_weather(loc):\n",
    "    \"\"\"\n",
    "    查询即时天气函数\n",
    "    :param loc: 必要参数，字符串类型，用于表示查询天气的具体城市名称，\\\n",
    "    注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入'Beijing'；\n",
    "    :return：OpenWeather API查询即时天气的结果，具体URL请求地址为：https://api.openweathermap.org/data/2.5/weather\\\n",
    "    返回结果对象类型为解析之后的JSON格式对象，并用字符串形式进行表示，其中包含了全部重要的天气信息\n",
    "    \"\"\"\n",
    "    # Step 1.构建请求\n",
    "    url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    # Step 2.设置查询参数\n",
    "    params = {\n",
    "        \"q\": loc,               \n",
    "        \"appid\": \"YOUR_API_KEY\",    # 输入API key\n",
    "        \"units\": \"metric\",            # 使用摄氏度而不是华氏度\n",
    "        \"lang\":\"zh_cn\"                # 输出语言为简体中文\n",
    "    }\n",
    "\n",
    "    # Step 3.发送GET请求\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Step 4.解析响应\n",
    "    data = response.json()\n",
    "    return json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe0ac1-58e7-4964-b7a1-a00865ddf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\":{\n",
    "            'name': 'get_weather',\n",
    "            'description': '查询即时天气函数，根据输入的城市名称，查询对应城市的实时天气，一次只能输入一个城市名称',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'loc': {\n",
    "                        'description': \"城市名称，注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入'Beijing'\",\n",
    "                        'type': 'string'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['loc']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef1554-1c6e-4ebd-9555-deb8286aa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"你好，请帮我查询下北京今天天气如何？\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd086b3-f65c-4f24-b5cd-072ea5c6db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools = tools,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd15af-480c-49fc-a4e5-a01eb4cab25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"description\": \"查询即时天气函数，根据输入的城市名称，查询对应城市的实时天气，一次只能输入一个城市名称\", \"parameters\": {\"type\": \"object\", \"properties\": {\"loc\": {\"description\": \"城市名称，注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入\\'Beijing\\'\", \"type\": \"string\"}}, \"required\": [\"loc\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n你好，请帮我查询下北京今天天气如何？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户让我查询北京今天的天气。首先，我需要确认用户的需求是获取北京的实时天气信息。根据提供的工具，我需要调用get_weather函数。注意，工具说明中提到中国的城市要用英文名称，所以北京应该转换为Beijing。接下来，我需要构造函数调用的参数，确保loc参数正确。然后，检查是否有其他可能的参数，但根据函数定义，只有loc是必需的。确认无误后，生成对应的tool_call结构。最后，确保返回的JSON格式正确，没有语法错误。这样用户就能得到准确的天气信息了。\\n</think>\\n\\n<tool_call>\\n{\"name\": \"get_weather\", \"arguments\": {\"loc\": \"Beijing\"}}\\n</tool_call><|im_end|>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c13f23-4a7f-4962-8fd6-4d8409c013a9",
   "metadata": {},
   "source": [
    "能够看到，此时模型就会创建一条同时带有指令、思考、外部函数的function call message。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee7e24-c3d2-44c7-8509-67b0a14c880b",
   "metadata": {},
   "source": [
    "而更进一步的，我们也可以测试模型的多个外部函数并联调用效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817e414-8b21-4cd3-ad14-e8112904a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"你好，请帮我查询下北京和杭州今天天气如何？\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b15023-39bf-48a4-828b-7b9a1e837af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools = tools,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff066cf-b466-4bd2-a284-33d7af1df04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"description\": \"查询即时天气函数，根据输入的城市名称，查询对应城市的实时天气，一次只能输入一个城市名称\", \"parameters\": {\"type\": \"object\", \"properties\": {\"loc\": {\"description\": \"城市名称，注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入\\'Beijing\\'\", \"type\": \"string\"}}, \"required\": [\"loc\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n你好，请帮我查询下北京和杭州今天天气如何？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户让我查询北京和杭州的天气。首先，我需要确认用户的需求是同时获取两个城市的天气信息。但根据提供的工具函数get_weather，每次只能查询一个城市，因为参数中loc是单个城市名称。这时候我需要判断是否需要多次调用函数。\\n\\n用户的问题中提到了两个城市，北京和杭州，所以可能需要分别调用两次get_weather函数。每次调用时，loc参数分别设置为北京和杭州的英文名称。即第一次调用loc为\\'Beijing\\'，第二次调用loc为\\'Hangzhou\\'。这样就能分别获取两个城市的天气情况了。\\n\\n接下来，我需要确保按照工具的要求，每次调用只处理一个城市。因此，应该生成两个独立的tool_call，每个对应一个城市。这样用户就能得到两个城市的天气信息了。同时，要注意到用户可能希望同时看到两个结果，所以需要确保两个调用都正确执行，并将结果整合后回复用户。\\n</think>\\n\\n<tool_call>\\n{\"name\": \"get_weather\", \"arguments\": {\"loc\": \"Beijing\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"get_weather\", \"arguments\": {\"loc\": \"Hangzhou\"}}\\n</tool_call><|im_end|>'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6d149-a86d-45ae-8076-05b4e3cbf561",
   "metadata": {},
   "source": [
    "能发现，此时模型同时发起了两条Function call message。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebc8e7-432c-44cf-941d-8db8584d3133",
   "metadata": {},
   "source": [
    "然后继续测试当模型接收到外部函数返回消息时候，模型返回内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b345349-a660-46f0-9d63-c71fbbde76a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。'},\n",
       " {'role': 'user', 'content': '你好，请帮我查询下北京和杭州今天天气如何？'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82c30b-7e72-4b18-8e5e-270c2c15dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"<think>\\n我将调用 get_weather 函数来查询天气。\\n</think>\\n\",\n",
    "    \"tool_calls\": [\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"arguments\": {\n",
    "                \"location\": \"北京\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"arguments\": {\n",
    "                \"location\": \"杭州\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b7920-0463-491e-9cc2-62315e6231e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"location\": \"北京\",\n",
    "        \"weather\": \"晴，最高气温26℃\"\n",
    "    })\n",
    "})\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"location\": \"杭州\",\n",
    "        \"weather\": \"多云转小雨，最高气温23℃\"\n",
    "    })\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d8fb1-30e2-45cb-9c00-b6d9b99c5735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。'},\n",
       " {'role': 'user', 'content': '你好，请帮我查询下北京和杭州今天天气如何？'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<think>\\n我将调用 get_weather 函数来查询天气。\\n</think>\\n',\n",
       "  'tool_calls': [{'name': 'get_weather', 'arguments': {'location': '北京'}},\n",
       "   {'name': 'get_weather', 'arguments': {'location': '杭州'}}]},\n",
       " {'role': 'tool',\n",
       "  'content': '{\"location\": \"\\\\u5317\\\\u4eac\", \"weather\": \"\\\\u6674\\\\uff0c\\\\u6700\\\\u9ad8\\\\u6c14\\\\u6e2926\\\\u2103\"}'},\n",
       " {'role': 'tool',\n",
       "  'content': '{\"location\": \"\\\\u676d\\\\u5dde\", \"weather\": \"\\\\u591a\\\\u4e91\\\\u8f6c\\\\u5c0f\\\\u96e8\\\\uff0c\\\\u6700\\\\u9ad8\\\\u6c14\\\\u6e2923\\\\u2103\"}'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca601ba-ded5-4709-9029-e4c585a25744",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools = tools,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68903086-6b10-442a-b85c-f8f3643f1df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一名助人为乐的天气查询助手，当用户询问天气信息时，请调用get_weather函数进行天气查询。\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"description\": \"查询即时天气函数，根据输入的城市名称，查询对应城市的实时天气，一次只能输入一个城市名称\", \"parameters\": {\"type\": \"object\", \"properties\": {\"loc\": {\"description\": \"城市名称，注意，中国的城市需要用对应城市的英文名称代替，例如如果需要查询北京市天气，则loc参数需要输入\\'Beijing\\'\", \"type\": \"string\"}}, \"required\": [\"loc\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n你好，请帮我查询下北京和杭州今天天气如何？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n我将调用 get_weather 函数来查询天气。\\n</think>\\n\\n<tool_call>\\n{\"name\": \"get_weather\", \"arguments\": {\"location\": \"北京\"}}\\n</tool_call>\\n<tool_call>\\n{\"name\": \"get_weather\", \"arguments\": {\"location\": \"杭州\"}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n<tool_response>\\n{\"location\": \"\\\\u5317\\\\u4eac\", \"weather\": \"\\\\u6674\\\\uff0c\\\\u6700\\\\u9ad8\\\\u6c14\\\\u6e2926\\\\u2103\"}\\n</tool_response>\\n<tool_response>\\n{\"location\": \"\\\\u676d\\\\u5dde\", \"weather\": \"\\\\u591a\\\\u4e91\\\\u8f6c\\\\u5c0f\\\\u96e8\\\\uff0c\\\\u6700\\\\u9ad8\\\\u6c14\\\\u6e2923\\\\u2103\"}\\n</tool_response><|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户之前让我查询北京和杭州的天气，我分别调用了两次get_weather函数。现在收到了两个工具响应，需要把结果整理成自然语言回复。\\n\\n首先，检查响应中的数据是否正确。北京的天气是晴，最高温26°C；杭州是多云转小雨，最高温23°C。需要注意的是，用户可能希望知道更详细的天气情况，比如是否有雨、风力等，但根据工具返回的信息，可能只有最高温和天气状况。\\n\\n接下来，要确保城市名称正确。工具响应中使用了中文城市名，比如“北京”和“杭州”，这和用户询问的一致。然后，将温度转换为摄氏度，并保持格式正确，比如使用°C符号。\\n\\n用户可能希望回复简洁明了，所以需要将每个城市的天气情况分点说明。同时，要注意天气状况的描述是否准确，比如“多云转小雨”意味着天气会有变化，可能需要提醒用户带伞。但根据工具返回的信息，可能只需要直接转述即可。\\n\\n最后，确保回复友好，没有使用Markdown格式，保持口语化。例如，开头用“以下是您查询的城市天气情况：”，然后分点列出每个城市的天气和温度，最后可以加上一句温馨提示，比如注意天气变化，但根据工具响应中是否有相关信息来决定是否添加。\\n</think>\\n\\n以下是您查询的城市天气情况：\\n\\n北京：晴，最高温26°C  \\n杭州：多云转小雨，最高温23°C  \\n\\n温馨提示：杭州有小雨，出行建议携带雨具~<|im_end|>'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a59d9-394c-4316-98cb-802e86565d3e",
   "metadata": {},
   "source": [
    "而这就是一条能够进行工具并联微调训练的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339176cb-2f1e-44f9-8842-2c4586af745f",
   "metadata": {},
   "source": [
    "### function calling：控制灯光"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab996aa3-3f83-4073-9636-49de95783fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def control_smart_light(room, action, brightness=None):\n",
    "    \"\"\"\n",
    "    控制家中智能灯光的函数。\n",
    "    :param room: 房间名称，如 'living_room' (客厅), 'bedroom' (卧室)\n",
    "    :param action: 操作类型，'on' (开灯), 'off' (关灯), 'adjust' (调节亮度)\n",
    "    :param brightness: 亮度值，范围 1-100，仅在 action 为 'adjust' 时有效\n",
    "    :return: 操作结果的描述\n",
    "    \"\"\"\n",
    "    # 模拟硬件操作\n",
    "    status = {\n",
    "        \"room\": room,\n",
    "        \"action\": action,\n",
    "        \"brightness\": brightness if brightness else \"N/A\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    return json.dumps(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c90465-a311-473e-b1a0-76675de18846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"control_smart_light\",\n",
    "            \"description\": \"智能家居灯光控制，可以开关灯或调节特定房间的亮度\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"room\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"房间名称，例如：'living_room', 'bedroom', 'kitchen'\"\n",
    "                    },\n",
    "                    \"action\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"on\", \"off\", \"adjust\"],\n",
    "                        \"description\": \"执行的动作：开灯、关灯或调节亮度\"\n",
    "                    },\n",
    "                    \"brightness\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"亮度百分比 (1-100)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"room\", \"action\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aa7a5-0137-4c94-a2f5-61c01396f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例 A\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个智能家居管家。\"},\n",
    "    {\"role\": \"user\", \"content\": \"卧室太暗了，帮我把灯调亮到80%。\"}\n",
    "]\n",
    "\n",
    "# 案例 B：明确指令\n",
    "# messages = [{\"role\": \"user\", \"content\": \"我要出门了，把客厅的灯关掉。\"}]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "# 经过 model.generate 后，得到模型生成的调用请求：\n",
    "# <tool_call>\n",
    "# {\"name\": \"control_smart_light\", \"arguments\": {\"room\": \"bedroom\", \"action\": \"adjust\", \"brightness\": 80}}\n",
    "# </tool_call>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc93ad-acf5-481c-8db5-08d741eaa183",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=max_seq_length,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e265f7c-05fe-492c-a178-1e93616f6188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个智能家居管家。\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"control_smart_light\", \"description\": \"智能家居灯光控制，可以开关灯或调节特定房间的亮度\", \"parameters\": {\"type\": \"object\", \"properties\": {\"room\": {\"type\": \"string\", \"description\": \"房间名称，例如：\\'living_room\\', \\'bedroom\\', \\'kitchen\\'\"}, \"action\": {\"type\": \"string\", \"enum\": [\"on\", \"off\", \"adjust\"], \"description\": \"执行的动作：开灯、关灯或调节亮度\"}, \"brightness\": {\"type\": \"integer\", \"description\": \"亮度百分比 (1-100)\"}}, \"required\": [\"room\", \"action\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\n卧室太暗了，帮我把灯调亮到80%。<|im_end|>\\n<|im_start|>assistant\\n<think>\\n好的，用户说卧室太暗了，需要把灯调亮到80%。首先，我需要确定用户提到的房间是卧室，对应的房间名称应该是\\'bedroom\\'。然后，用户想要调节亮度，所以动作是\\'adjust\\'。亮度百分比是80%，所以参数里需要包含brightness: 80。根据工具中的函数定义，必须的参数是room和action，这里都满足。因此，正确的函数调用应该是control_smart_light，参数为room: \\'bedroom\\', action: \\'adjust\\', brightness: 80。需要检查参数是否正确，特别是亮度范围是否在1-100之间，80是有效的。确认无误后，生成对应的tool_call。\\n</think>\\n\\n<tool_call>\\n{\"name\": \"control_smart_light\", \"arguments\": {\"room\": \"bedroom\", \"action\": \"adjust\", \"brightness\": 80}}\\n</tool_call><|im_end|>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b15e5b-6a34-4dfd-a1b4-6cda81de72ec",
   "metadata": {},
   "source": [
    "### 2. Unsloth高层对话API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc7361-1573-4a1a-a20e-cbc174c8b6e0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，除了使用上述底层API进行对话外，Unsloth还提供了更加便捷的流式输出模型对话信息的函数，基本对话效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c1ff1-ea53-45c0-aea8-4c71a7d2ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e4711",
   "metadata": {},
   "source": [
    "- 实时解码器。 在默认情况下，model.generate 是一个“同步”过程：模型在后台计算，算完几百个 Token 后一次性返回，这会导致程序卡顿几秒甚至十几秒。 TextStreamer 会在模型生成每一个 Token 的瞬间，立即将其解码并打印到终端上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf121d-b1df-4cf9-9552-d3f9c4f9c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"你是秦始皇，向我简要介绍你自己\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330369de-cdaf-4850-83ee-57dec5f3a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朕乃秦始皇，姓嬴名政，秦庄襄王之子，秦朝开国皇帝。朕统一六国，废除分封，建立中央集权，推行郡县制，统一度量衡、文字、车轨，筑长城以御外敌，开灵渠以通水路。朕以“始皇帝”为号，欲传之万世，永续秦祚。天下归一，功业千秋，朕之意志，即为天下法度。汝可称朕为“始皇”，或“陛下”。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = False, \n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, \n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, \n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "# tokenizer(...) 返回的是一个字典，包含 input_ids 和 attention_mask。\n",
    "# 使用 ** 会将这个字典解包。这意味着它等同于手动传入： input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"]\n",
    "# 这种写法简洁且不容易漏掉 attention_mask\n",
    "\n",
    "# top_p = 0.8 和 top_k = 20:\n",
    "# 这是双重采样过滤。模型首先筛选出概率最高的前 20 个词（top_k），然后再从中筛选出累积概率达到 80% 的词（top_p）。\n",
    "# 效果：这种组合极大增强了生成内容的连贯性和多样性，防止模型陷入死循环或胡言乱语。\n",
    "# streamer 参数允许模型在生成过程中实时输出文本，而不是等到全部生成完成后一次性输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e17ce-fb90-4b1f-ae00-8fe4c62a2f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好的，用户让我扮演秦始皇，向他简要介绍自己。首先，我需要确定用户的需求是什么。可能他们对历史人物感兴趣，或者想了解秦始皇的生平和成就。用户可能希望得到一个简洁但全面的介绍，同时带有秦始皇的语气和风格。\n",
      "\n",
      "接下来，我要考虑如何以秦始皇的口吻来回答。需要使用古代的词汇和表达方式，比如“朕”、“朕之”等，同时保持语言简洁明了，避免过于复杂的句子结构。还要涵盖秦始皇的主要成就，比如统一六国、统一文字、度量衡、修建长城、灵渠，以及他的统治手段，比如焚书坑儒，还有他的陵墓和长生不老的追求。\n",
      "\n",
      "同时，要注意避免过于冗长，保持简要。可能需要分点列出主要成就，但用连贯的段落来表达。还要考虑用户可能的深层需求，比如对秦始皇的评价，是正面还是负面，或者想了解他的历史影响。因此，在介绍中可以适当提到他的功过，但保持中立，以秦始皇的视角来陈述。\n",
      "\n",
      "另外，要确保历史准确性，比如统一的时间、主要政策和工程。需要确认这些信息是否正确，比如统一六国是在公元前221年，修建长城和灵渠的时间，以及焚书坑儒的背景。同时，提到他的陵墓和长生不老药的追求，这些也是重要的点。\n",
      "\n",
      "最后，结尾部分要体现出秦始皇的威严和对后世的影响，比如“朕之功业，后世所铭记”，这样既符合角色设定，又满足用户对历史人物的了解需求。需要检查是否有遗漏的重要信息，并确保语言流畅自然，符合古代帝王的口吻。\n",
      "</think>\n",
      "\n",
      "*负手而立，目光如炬*\n",
      "\n",
      "朕乃秦始皇，姓嬴名政，秦王之子。十三岁即位，二十二岁亲政，统一六国，废分封，立郡县，天下归一。朕命李斯整理文字，统一度量衡，筑长城以御匈奴，凿灵渠以通岭南，焚书坑儒以禁异端。朕欲长生不老，遣徐福东渡求仙药，虽未得，然朕之功业，后世所铭记。\n",
      "\n",
      "*轻抚玉玺，神情威严*\n",
      "\n",
      "朕之秦，乃万世一统之基。朕之名，后人或褒或贬，然朕之志，唯求华夏永固，万世一系。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, \n",
    "    enable_thinking = True, # 设置思考\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 2048, \n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, \n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf86a4-006c-4125-90b4-82bbcee0b2d0",
   "metadata": {},
   "source": [
    "在基本掌握Unsloth的模型导入和对话方法后，接下来正式进入到Qwen3大模型高效微调流程中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2f104-0fab-4bb0-bb71-5cdfeaebc0e6",
   "metadata": {},
   "source": [
    "## Qwen3高效微调数据集创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207ec0a-e12b-4f5e-8d5b-83d80bc0e485",
   "metadata": {},
   "source": [
    "### 1. HuggingFace与ModelScope平台数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430eb957-3953-4f3c-b2e9-672e9839da8e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;若要选择公开数据集进行高效微调，则首先可以考虑HuggingFace和ModelScope平台上的数据集。其中HuggingFace上不仅保管了最大规模数量的数据集，而且HuggingFace的dataset工具，也是目前主流微调核心库如trl库默认支持的数据集格式。此外，ModelScope则是国内版的“HuggingFace”，拥有最大规模的中文数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd050935-f90e-45e8-b72a-460f2a9aa181",
   "metadata": {},
   "source": [
    "- HuggingFace数据集主页：https://huggingface.co/?activityType=all&feedType=following&trending=dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aa3b4-225a-4643-864b-af32e4ad7309",
   "metadata": {},
   "source": [
    "- 魔搭社区数据集主页：https://www.modelscope.cn/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d82e7-db13-4f20-bad6-f6f396a32512",
   "metadata": {},
   "source": [
    "### 2. 数据集下载流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3030b-241a-4c93-82c2-1660382ef4e4",
   "metadata": {},
   "source": [
    "Qwen3 具备推理模式和非推理模式。（非推理模式（普通聊天）、推理模式（深度思考））因此，应当使用两个数据集：\n",
    "\n",
    "1. 使用 Open Math Reasoning 数据集，该数据集曾被用于赢得 AIMO（AI 数学奥林匹克 - 第二届进步奖）挑战！我们从中抽取了 10% 可验证的推理轨迹，这些轨迹是基于 DeepSeek R1 模型生成的，并且准确率超过 95%。数据集地址：https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d04c9d-0e26-449c-a1e1-5dec008ce941",
   "metadata": {},
   "source": [
    "2. 利用 Maxime Labonne 的 FineTome-100k 数据集，该数据集风格类似 ShareGPT。但我们需要将其转换为 HuggingFace 通用的多轮对话格式。数据集地址：https://huggingface.co/datasets/mlabonne/FineTome-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cabbd7-5b8d-471e-b710-d8ba6024a1ac",
   "metadata": {},
   "source": [
    "在实际微调过程中，大多都会使用huggingface的datasets库进行数据集下载和管理，实际下载流程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516c9ce-b8d9-487b-b070-2c9d13ea79f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.12/site-packages (4.3.0)\n",
      "Collecting datasets\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7b/b5/fefa518c809de7bced5cddb7c21c010da66fa2ae494bda96844a280cc6ce/datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /root/miniconda3/lib/python3.12/site-packages (0.36.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/df/8d/7ca723a884d55751b70479b8710f06a317296b1fa1c1dec01d0420d13e43/huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.0/521.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.7.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.5.4)\n",
      "Collecting typer-slim (from huggingface_hub)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b4/84/e97abf10e4a699194ff07fd586ec7f4cf867d9d04bead559a65f9e7aff84/typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /root/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda3/lib/python3.12/site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Installing collected packages: typer-slim, huggingface_hub, datasets\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.3.0\n",
      "    Uninstalling datasets-4.3.0:\n",
      "      Successfully uninstalled datasets-4.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.2.3 which is incompatible.\n",
      "unsloth-zoo 2026.1.1 requires datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1, but you have datasets 4.4.2 which is incompatible.\n",
      "unsloth 2026.1.1 requires datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1, but you have datasets 4.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.4.2 huggingface_hub-1.2.3 typer-slim-0.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185c380-d2d6-4746-b2a1-6d9ec8358fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 HTTP 和 HTTPS 代理\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:10080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:10080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f2e25-247b-4abf-bb7f-81cad7992ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取消代理\n",
    "os.environ.pop('HTTP_PROXY', None)\n",
    "os.environ.pop('HTTPS_PROXY', None)\n",
    "os.environ.pop('http_proxy', None) \n",
    "os.environ.pop('https_proxy', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0ebb2",
   "metadata": {},
   "source": [
    "- 或者去手动下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67994777-86f5-41e2-9363-79dea1f9d89f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`datasets` 是 Hugging Face 提供的一个高效数据处理库，专为机器学习和大语言模型（LLM）训练而设计。它支持加载、处理、转换和保存各种格式的数据（如 JSON、CSV、Parquet 等），并能与 `transformers` 模型无缝集成。通过 `datasets`，开发者可以快速完成数据清洗、切分、tokenization 等常见任务，大大提升训练效率，特别适合用于指令微调、对话生成、Function Calling 等任务的数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04586e-2947-4bb4-bdbf-0224557ac674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2caac-fd8f-4ff4-babb-3a7670fd4b5a",
   "metadata": {},
   "source": [
    "然后分别下载并导入这两个库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec5133-f4e5-4341-8040-879cfa85cfcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/modelscope/utils/plugins.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\n",
      " _   .-')                _ .-') _     ('-.             .-')                              _ (`-.    ('-.\n",
      "( '.( OO )_             ( (  OO) )  _(  OO)           ( OO ).                           ( (OO  ) _(  OO)\n",
      " ,--.   ,--.).-'),-----. \\     .'_ (,------.,--.     (_)---\\_)   .-----.  .-'),-----.  _.`     \\(,------.\n",
      " |   `.'   |( OO'  .-.  ',`'--..._) |  .---'|  |.-') /    _ |   '  .--./ ( OO'  .-.  '(__...--'' |  .---'\n",
      " |         |/   |  | |  ||  |  \\  ' |  |    |  | OO )\\  :` `.   |  |('-. /   |  | |  | |  /  | | |  |\n",
      " |  |'.'|  |\\_) |  |\\|  ||  |   ' |(|  '--. |  |`-' | '..`''.) /_) |OO  )\\_) |  |\\|  | |  |_.' |(|  '--.\n",
      " |  |   |  |  \\ |  | |  ||  |   / : |  .--'(|  '---.'.-._)   \\ ||  |`-'|   \\ |  | |  | |  .___.' |  .--'\n",
      " |  |   |  |   `'  '-'  '|  '--'  / |  `---.|      | \\       /(_'  '--'\\    `'  '-'  ' |  |      |  `---.\n",
      " `--'   `--'     `-----' `-------'  `------'`------'  `-----'    `-----'      `-----'  `--'      `------'\n",
      "\n",
      "Downloading Dataset to directory: /root/.cache/modelscope/hub/datasets/AI-ModelScope/FineTome-100k\n",
      "Processing 3 items:   0%|                           | 0.00/3.00 [00:00<?, ?it/s]\n",
      "Downloading [.gitattributes]:   0%|                 | 0.00/2.25k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading [README.md]:   0%|                        | 0.00/982 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   0%| | 0.00/111M [00:00<?, ?B/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading [README.md]: 100%|█████████████████| 982/982 [00:00<00:00, 1.97kB/s]\u001b[A\u001b[A\n",
      "Processing 3 items:  33%|██████▋             | 1.00/3.00 [00:00<00:01, 1.99it/s]\n",
      "Downloading [.gitattributes]: 100%|████████| 2.25k/2.25k [00:00<00:00, 4.32kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   1%| | 1.00M/111M [00:00<01:18\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   3%| | 3.00M/111M [00:00<00:25\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   4%| | 5.00M/111M [00:01<00:16\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   6%| | 7.00M/111M [00:01<00:12\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:   8%| | 9.00M/111M [00:01<00:10\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  10%| | 11.0M/111M [00:01<00:08\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  12%| | 13.0M/111M [00:01<00:07\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  13%|▏| 15.0M/111M [00:01<00:06\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  15%|▏| 17.0M/111M [00:01<00:07\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  18%|▏| 20.0M/111M [00:02<00:06\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  20%|▏| 22.0M/111M [00:02<00:06\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  22%|▏| 24.0M/111M [00:02<00:07\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  23%|▏| 26.0M/111M [00:02<00:06\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  25%|▎| 28.0M/111M [00:02<00:06\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  27%|▎| 30.0M/111M [00:02<00:05\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  29%|▎| 32.0M/111M [00:02<00:05\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  31%|▎| 34.0M/111M [00:03<00:05\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  32%|▎| 36.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  34%|▎| 38.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  36%|▎| 40.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  38%|▍| 42.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  40%|▍| 44.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  41%|▍| 46.0M/111M [00:03<00:03\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  43%|▍| 48.0M/111M [00:03<00:04\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  45%|▍| 50.0M/111M [00:04<00:03\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  48%|▍| 53.0M/111M [00:04<00:03\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  49%|▍| 55.0M/111M [00:04<00:03\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  51%|▌| 57.0M/111M [00:04<00:03\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  53%|▌| 59.0M/111M [00:04<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  55%|▌| 61.0M/111M [00:04<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  57%|▌| 63.0M/111M [00:04<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  58%|▌| 65.0M/111M [00:04<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  60%|▌| 67.0M/111M [00:04<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  62%|▌| 69.0M/111M [00:05<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  64%|▋| 71.0M/111M [00:05<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  66%|▋| 73.0M/111M [00:05<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  67%|▋| 75.0M/111M [00:05<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  69%|▋| 77.0M/111M [00:05<00:02\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  71%|▋| 79.0M/111M [00:05<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  73%|▋| 81.0M/111M [00:05<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  75%|▋| 83.0M/111M [00:05<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  76%|▊| 85.0M/111M [00:06<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  78%|▊| 87.0M/111M [00:06<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  80%|▊| 89.0M/111M [00:06<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  82%|▊| 91.0M/111M [00:06<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  84%|▊| 93.0M/111M [00:06<00:01\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  85%|▊| 95.0M/111M [00:06<00:00\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  87%|▊| 97.0M/111M [00:06<00:00\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  89%|▉| 99.0M/111M [00:06<00:00\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  91%|▉| 101M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  93%|▉| 103M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  94%|▉| 105M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  96%|▉| 107M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]:  98%|▉| 109M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [data/train-00000-of-00001.parquet]: 100%|█| 111M/111M [00:07<00:00,\u001b[A\u001b[A\u001b[A\n",
      "Processing 3 items: 100%|████████████████████| 3.00/3.00 [00:07<00:00, 2.55s/it]\n",
      "\n",
      "Successfully Downloaded from dataset AI-ModelScope/FineTome-100k.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!modelscope download --dataset AI-ModelScope/FineTome-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c277321-96fa-4f9b-8551-4484b78ec179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reasoning_dataset = load_dataset(\"/root/pandagpt_data/data/OpenMathReasoning-mini\", split = \"cot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440c543-33ee-49b2-87db-ceadee5a850b",
   "metadata": {},
   "source": [
    "> 只下载包含cot的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49389157-ce9b-48ca-8e66-929d3557f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_reasoning_dataset = load_dataset(\"/root/.cache/modelscope/hub/datasets/AI-ModelScope/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2812f-b5f1-48ff-b9f0-a0d709601b67",
   "metadata": {},
   "source": [
    "> 只下载train部分数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f8d2a-d43d-4058-9c91-fd15e7c635fa",
   "metadata": {},
   "source": [
    "然后输入数据集名称，即可查看数据集基本信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886a9ea-a495-47f5-93b3-c834375c156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['expected_answer', 'problem_type', 'problem_source', 'generation_model', 'pass_rate_72b_tir', 'problem', 'generated_solution', 'inference_mode'],\n",
       "    num_rows: 19252\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf25f9f-95ff-4452-9684-f9d6c87222e1",
   "metadata": {},
   "source": [
    "加上索引则可以直接查看对应数据集信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2560f45-b345-4182-aed5-843b593e7615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_answer': '14',\n",
       " 'problem_type': 'has_answer_extracted',\n",
       " 'problem_source': 'aops_c4_high_school_math',\n",
       " 'generation_model': 'DeepSeek-R1',\n",
       " 'pass_rate_72b_tir': '0.96875',\n",
       " 'problem': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.',\n",
       " 'generated_solution': \"<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>To solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]\",\n",
       " 'inference_mode': 'cot'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce8082-dd74-431d-8b2d-67da76db2de2",
   "metadata": {},
   "source": [
    "能够看出这是一个基于DeepSeek R1回答的数学数据集，其中`problem`是问题，`generated_solution`是数学推导过程（即思考过程），而`expected_answer`则是最终的答案。该数据集总共接近2万条数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3bee9-5e36-400f-8e24-a644677b631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19252"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reasoning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac9903-45e1-4f51-8c0d-557c81cdf9eb",
   "metadata": {},
   "source": [
    "而对话数据集如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601675bc-d1a6-4e93-8155-6bef70f650c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'source', 'score'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_reasoning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a544c05-360e-4bed-8a9f-95048228a2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n",
       " 'source': 'infini-instruct-top-500k',\n",
       " 'score': 5.212620735168457}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_reasoning_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a57e22-f124-440f-8f9b-d2e52afdb044",
   "metadata": {},
   "source": [
    "其中每一条数据都是一个对话，包含一组或者多组ChatGPT的聊天信息，其中`from`代表是用户消息还是大模型回复消息，而`value`则是对应的文本。该对话数据集总共包含10万条数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ba480-9da6-44a9-81d0-b5cdf7a96a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_reasoning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b52d74-b164-4b73-97bb-e0104f43b6b4",
   "metadata": {},
   "source": [
    "能够看出dataset是一种类似json的数据格式，每条数据都以字段格式进行存储，在实际微调过程中，我们需要先将数据集的目标字段进行提取和拼接，然后加载到Qwen3模型的提示词模板中，并最终带入Unsloth进行微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6d688-8440-43ca-b2d5-68f1b5876cec",
   "metadata": {},
   "source": [
    "### 2. 微调数据集清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b3131-48d9-4ea9-9088-18c38ff80c97",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来尝试对上述两个格式各异的数据集进行数据清洗，主要是围绕数据集进行数据格式的调整，便于后续带入Qwen3提示词模板。对于dataset格式的数据对象来说，可以先创建满足格式调整的函数，然后使用map方法对数据集格式进行调整。这里先创建generate_conversation函数，用于对reasoning_dataset中的每一条数据进行格式调整，即通过新创建一个新的特征conversations，来以对话形式保存历史问答数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e19e56-df53-4e25-ad0b-415a8f85a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation(examples):\n",
    "    problems  = examples[\"problem\"]\n",
    "    solutions = examples[\"generated_solution\"]\n",
    "    conversations = []\n",
    "    for problem, solution in zip(problems, solutions):\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : problem},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a791725-19d3-4038-89eb-248d7e5f9e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_answer': '14',\n",
       " 'problem_type': 'has_answer_extracted',\n",
       " 'problem_source': 'aops_c4_high_school_math',\n",
       " 'generation_model': 'DeepSeek-R1',\n",
       " 'pass_rate_72b_tir': '0.96875',\n",
       " 'problem': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.',\n",
       " 'generated_solution': \"<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>To solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]\",\n",
       " 'inference_mode': 'cot'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89445061-41f4-4889-b64d-85df12dfd8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reasoning_data = reasoning_dataset.map(generate_conversation, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e836c84-52fb-4d3d-9674-6a02a9c14e5d",
   "metadata": {},
   "source": [
    "此时历史问答数据如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cc6c6-bd2e-4e3e-b51b-189c924de03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([[{'content': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.', 'role': 'user'}, {'content': \"<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>To solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]\", 'role': 'assistant'}], [{'content': 'Let \\\\( \\\\sum a_n \\\\) be a convergent series with \\\\( a_n \\\\geq 0 \\\\). Determine the nature of the series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\).', 'role': 'user'}, {'content': \"<think>\\nOkay, so I have this problem here where I need to determine the nature of the series ∑(a_n²)/(1 + a_n²) given that the series ∑a_n is convergent and each a_n is non-negative. Alright, let's start by understanding what the problem is asking. \\n\\nFirst, I know that ∑a_n is convergent, which means that the sequence a_n approaches zero as n approaches infinity. Because all the terms a_n are non-negative, they must be decreasing to zero, right? Wait, actually, convergence of the series only requires that the terms approach zero, but they don't necessarily have to be decreasing. For example, even if the terms bounce around a bit but still approach zero, the series can still converge if they do so quickly enough. But since the problem states a_n ≥ 0, we don't have to worry about negative terms messing up the convergence through cancellation.\\n\\nNow, the question is about the series ∑(a_n²)/(1 + a_n²). We need to figure out if this series converges or diverges. Let me think about how the terms of this new series compare to the original series. Since each a_n is non-negative, a_n² is also non-negative, and dividing by 1 + a_n² will make each term of the new series non-negative as well. So, we're dealing with a series of non-negative terms, which is good because tests like the comparison test apply.\\n\\nLet me recall the comparison test: if 0 ≤ b_n ≤ c_n for all n, and ∑c_n converges, then ∑b_n converges. Conversely, if ∑b_n diverges, then ∑c_n diverges. So, if I can compare (a_n²)/(1 + a_n²) to some other series that I know converges or diverges, that could help.\\n\\nAlternatively, maybe I can use the limit comparison test. If I take the limit of [ (a_n²)/(1 + a_n²) ] / a_n as n approaches infinity, that might tell me something. Let's compute that limit.\\n\\nLimit as n → ∞ of [ (a_n²)/(1 + a_n²) ] / a_n = limit of [a_n² / (a_n(1 + a_n²))] = limit of [a_n / (1 + a_n²)].\\n\\nNow, since ∑a_n converges, we know that a_n must approach 0. So as n becomes large, a_n is approaching 0. Therefore, the denominator 1 + a_n² approaches 1. So the entire expression is approximately a_n / 1 = a_n. Therefore, the limit is 0. \\n\\nWait, but in the limit comparison test, if the limit is 0 and ∑a_n converges, then if the original series (the one with terms (a_n²)/(1 + a_n²)) is being compared to a convergent series, then what's the conclusion?\\n\\nHmm, let me recall: If the limit of b_n / c_n is 0, and ∑c_n converges, then ∑b_n converges. Because if b_n is eventually less than c_n, then the comparison test applies. So since (a_n²)/(1 + a_n²) / a_n → 0, and ∑a_n converges, then by the limit comparison test, ∑(a_n²)/(1 + a_n²) also converges. \\n\\nWait, that seems straightforward, right? Because the limit is 0, which is finite, and the series we're comparing to (∑a_n) converges. Therefore, ∑(a_n²)/(1 + a_n²) must converge. \\n\\nBut let me verify this with another approach to make sure I'm not missing something. Let's think about the behavior of the terms (a_n²)/(1 + a_n²). Since a_n is non-negative and approaching 0, when n is large enough, a_n is less than or equal to 1, right? Because if a_n approaches 0, there exists some N such that for all n ≥ N, a_n ≤ 1. So for those n, a_n² ≤ a_n. Therefore, (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n. \\n\\nTherefore, for n ≥ N, (a_n²)/(1 + a_n²) ≤ a_n. Since ∑a_n converges, then by comparison test, ∑(a_n²)/(1 + a_n²) also converges. \\n\\nWait, that's a different argument. Let's see. So if a_n is less than or equal to 1, then a_n² is less than or equal to a_n, so (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n. Therefore, since ∑a_n converges, the tail of the series ∑(a_n²)/(1 + a_n²) is dominated by ∑a_n, hence converges. Therefore, the entire series converges.\\n\\nAlternatively, even if a_n is sometimes greater than 1, but since the series ∑a_n converges, the terms a_n must approach zero, so beyond some N, they are all less than 1. So the initial terms where a_n might be greater than 1 can only contribute finitely many terms to the series, which doesn't affect convergence. Therefore, we can ignore those and focus on the tail where a_n < 1. Then, as above, in that region, (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n, so again, since ∑a_n converges, so does ∑(a_n²)/(1 + a_n²).\\n\\nAlternatively, another way to think about it is to note that (a_n²)/(1 + a_n²) is approximately a_n² when a_n is small. Since a_n approaches 0, a_n² is going to be much smaller than a_n. Therefore, if ∑a_n converges, then ∑a_n² might also converge, but actually, that's not necessarily always true. For example, take a_n = 1/n. Then ∑a_n diverges, but ∑a_n² converges. But in this problem, ∑a_n converges. So, for example, if a_n = 1/n², then ∑a_n converges (it's a p-series with p=2>1), and ∑a_n² would be ∑1/n^4, which also converges. But if a_n is 1/n^(1/2), but then ∑a_n would diverge. But in our problem, ∑a_n converges.\\n\\nWait, but the convergence of ∑a_n doesn't necessarily imply that a_n is summable with a higher exponent. For example, take a_n = 1/(n(log n)^2), which converges (by the integral test), then a_n² = 1/(n²(log n)^4), which would converge as well. But if a_n is like 1/n^(1/2 + ε) for some ε>0, then ∑a_n converges if 1/2 + ε > 1, i.e., ε > 1/2. Then a_n² would be 1/n^(1 + 2ε), which converges as well. So in these cases, if a_n is eventually small enough such that ∑a_n converges, then a_n² is even smaller, so ∑a_n² converges. However, perhaps there is a case where ∑a_n converges but ∑a_n² diverges? Hmm. Wait, no. If a_n is non-negative and ∑a_n converges, then a_n must approach zero, and for sufficiently large n, a_n ≤1. Therefore, a_n² ≤ a_n for those n, hence ∑a_n² converges by comparison to ∑a_n. Wait, that's true, isn't it? If a_n is eventually less than or equal to 1, then a_n² ≤ a_n, so ∑a_n² converges. Therefore, in our problem, the series ∑(a_n²)/(1 + a_n²) is dominated by ∑a_n², which is dominated by ∑a_n. So, if we can bound (a_n²)/(1 + a_n²) ≤ a_n², and ∑a_n² converges, then ∑(a_n²)/(1 + a_n²) would converge as well.\\n\\nBut actually, (a_n²)/(1 + a_n²) ≤ a_n², right? Because 1 + a_n² ≥1, so dividing by something bigger than 1 makes the fraction smaller. So (a_n²)/(1 + a_n²) ≤ a_n². And since ∑a_n² converges, as I thought before, then by comparison, ∑(a_n²)/(1 + a_n²) also converges.\\n\\nWait, but I had earlier compared (a_n²)/(1 + a_n²) directly to a_n and used the limit comparison test. So both approaches seem valid. Let me just check with an example. Suppose a_n = 1/n². Then ∑a_n converges. Then (a_n²)/(1 + a_n²) = (1/n^4)/(1 + 1/n²) = 1/(n^4 + n²) ≈ 1/n^4 for large n, so the series ∑1/(n^4 + n²) converges. Another example: a_n = 1/n. But wait, ∑a_n diverges here, which doesn't fit the problem's condition. So if a_n is 1/n^p with p>1, then ∑a_n converges, and ∑a_n² converges as well. \\n\\nAlternatively, take a_n = 1/(n^(1/2 + ε)), with ε > 1/2 so that ∑a_n converges. Then a_n² = 1/n^{1 + 2ε}, and since 1 + 2ε > 1, ∑a_n² converges. \\n\\nBut actually, the key point here is that since a_n tends to 0, for sufficiently large n, a_n ≤1, so a_n² ≤ a_n, so ∑a_n² is dominated by ∑a_n. Therefore, ∑a_n² converges, and since (a_n²)/(1 + a_n²) ≤ a_n², then ∑(a_n²)/(1 + a_n²) converges as well. Therefore, the series in question converges.\\n\\nWait, but maybe I should be careful here. Let me think again. If a_n is 0, then the term is 0. If a_n is very large, say approaching infinity, but wait, in our case, a_n approaches 0, so that's not possible. But in the problem, the series ∑a_n is convergent, so the terms a_n must approach zero. Therefore, beyond some N, a_n ≤1, so the denominator 1 + a_n² ≥1, hence (a_n²)/(1 + a_n²) ≤a_n². Therefore, for all n beyond N, the terms are bounded above by a_n², which is summable because a_n² ≤ a_n for n ≥N (since a_n ≤1). Therefore, ∑(a_n²)/(1 + a_n²) converges by comparison to ∑a_n.\\n\\nAlternatively, to put it more formally:\\n\\nSince ∑a_n converges, a_n →0. Therefore, there exists N such that for all n ≥N, a_n ≤1. For those n, a_n² ≤a_n. Therefore, (a_n²)/(1 + a_n²) ≤a_n² ≤a_n. Hence, ∑_{n=N}^∞ (a_n²)/(1 + a_n²) ≤ ∑_{n=N}^∞ a_n < ∞. Therefore, the tail converges, and adding the first N-1 terms (which are finite) doesn't affect convergence. Therefore, the entire series ∑(a_n²)/(1 + a_n²) converges.\\n\\nAlternatively, another approach: note that (a_n²)/(1 + a_n²) ≤ min(a_n², 1). But since a_n →0, eventually min(a_n², 1) = a_n², so again, ∑a_n² converges. So this gives another way to see it.\\n\\nWait, but in all these approaches, the conclusion is the same: the series ∑(a_n²)/(1 + a_n²) converges. Therefore, the answer should be that the series converges.\\n\\nBut wait, let me check with a concrete example where a_n is sometimes large. Suppose a_n is 1 for some n. Then (1^2)/(1 +1^2)=1/2. But since ∑a_n converges, a_n must approach 0, so only finitely many terms can be ≥1. So those terms contribute a finite sum, and the rest of the terms where a_n <1 can be compared to a_n² or a_n. Therefore, even if there are a few terms where a_n is large, they don't affect the convergence of the series.\\n\\nFor example, take a_n = 2 for n=1, and a_n =1/n² for n≥2. Then ∑a_n =2 + ∑1/n², which converges. Then ∑(a_n²)/(1 +a_n²) = (4/5) + ∑_{n=2}^∞ [ (1/n^4)/(1 +1/n²) ] = 4/5 + ∑_{n=2}^\\\\infty [1/(n^4 +n²)]. Since the tail is convergent, the entire series converges.\\n\\nAnother test case: a_n =1 for n=1, and a_n=0 for n≥2. Then ∑a_n converges (sum is 1), and ∑(a_n²)/(1 +a_n²)=1/2 + 0, which is obviously convergent.\\n\\nSo, all these test cases support the conclusion that ∑(a_n²)/(1 +a_n²) converges.\\n\\nBut let me think if there could be any counterexample. Suppose ∑a_n converges, but ∑(a_n²)/(1 +a_n²) diverges. Is that possible?\\n\\nIf ∑a_n converges, then a_n →0. So for sufficiently large n, a_n is small. Therefore, (a_n²)/(1 +a_n²) ≈ a_n². Therefore, if ∑a_n² diverges, then ∑(a_n²)/(1 +a_n²) would also diverge. But can ∑a_n converge while ∑a_n² diverges?\\n\\nWait, but earlier, we thought that if a_n ≤1 for sufficiently large n, then a_n² ≤ a_n, so ∑a_n² converges by comparison. Therefore, if ∑a_n converges, then ∑a_n² must converge as well. Therefore, ∑(a_n²)/(1 +a_n²) is dominated by ∑a_n², which converges, hence itself converges. Therefore, such a counterexample isn't possible. Therefore, the conclusion is that ∑(a_n²)/(1 +a_n²) must converge.\\n\\nSo, after all this thinking and checking with different examples and approaches, I believe the answer is that the series ∑(a_n²)/(1 +a_n²) converges.\\n</think>To determine the nature of the series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\) given that \\\\( \\\\sum a_n \\\\) is a convergent series with \\\\( a_n \\\\geq 0 \\\\), we proceed as follows:\\n\\n1. **Convergence of \\\\( \\\\sum a_n \\\\)**:\\n   Since \\\\( \\\\sum a_n \\\\) converges, it follows that \\\\( a_n \\\\to 0 \\\\) as \\\\( n \\\\to \\\\infty \\\\).\\n\\n2. **Behavior of \\\\( \\\\frac{a_n^2}{1 + a_n^2} \\\\)**:\\n   For sufficiently large \\\\( n \\\\), \\\\( a_n \\\\) is small. Specifically, there exists an integer \\\\( N \\\\) such that for all \\\\( n \\\\geq N \\\\), \\\\( a_n \\\\leq 1 \\\\).\\n\\n3. **Comparison with \\\\( a_n \\\\)**:\\n   For \\\\( n \\\\geq N \\\\):\\n   \\\\[\\n   \\\\frac{a_n^2}{1 + a_n^2} \\\\leq \\\\frac{a_n^2}{1} = a_n^2 \\\\leq a_n.\\n   \\\\]\\n   This inequality holds because \\\\( a_n \\\\leq 1 \\\\) implies \\\\( a_n^2 \\\\leq a_n \\\\).\\n\\n4. **Convergence of \\\\( \\\\sum a_n^2 \\\\)**:\\n   Since \\\\( \\\\sum a_n \\\\) converges and \\\\( a_n \\\\to 0 \\\\), it follows that \\\\( a_n^2 \\\\) is also non-negative and tends to zero. Moreover, \\\\( a_n^2 \\\\leq a_n \\\\) for \\\\( n \\\\geq N \\\\), and by the comparison test, \\\\( \\\\sum a_n^2 \\\\) converges.\\n\\n5. **Convergence of \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\)**:\\n   For \\\\( n \\\\geq N \\\\), we have:\\n   \\\\[\\n   \\\\frac{a_n^2}{1 + a_n^2} \\\\leq a_n.\\n   \\\\]\\n   Since \\\\( \\\\sum a_n \\\\) converges, the comparison test implies that \\\\( \\\\sum_{n=N}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} \\\\) also converges.\\n\\n6. **Final Conclusion**:\\n   The series \\\\( \\\\sum_{n=1}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} \\\\) can be split into two parts:\\n   \\\\[\\n   \\\\sum_{n=1}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} = \\\\sum_{n=1}^{N-1} \\\\frac{a_n^2}{1 + a_n^2} + \\\\sum_{n=N}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2}.\\n   \\\\]\\n   The first part is a finite sum, and the second part converges by the comparison test. Therefore, the entire series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\) converges.\\n\\n\\\\[\\n\\\\boxed{\\\\text{The series converges}}\\n\\\\]\", 'role': 'assistant'}], [{'content': 'For \\\\( a > 0 \\\\), find the value of \\\\( \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln x}{x^2 + a^2} \\\\, \\\\text{d}x \\\\).', 'role': 'user'}, {'content': \"<think>\\nOkay, so I need to solve the integral of (ln x)/(x² + a²) from 0 to infinity. Hmm, let me think. First, since the integral is from 0 to infinity and the denominator is x squared plus a squared, maybe a substitution involving trigonometric functions? Wait, or perhaps substitution for the logarithm part. Let me consider substitution variables. Let’s see, maybe let t = x/a? That could simplify the denominator. If I set t = x/a, then x = a t, and dx = a dt. Let me try that substitution.\\n\\nSo when x is 0, t is 0, and as x approaches infinity, t approaches infinity. Then the integral becomes:\\n\\n∫₀^∞ [ln(a t)] / ( (a t)² + a² ) * a dt.\\n\\nSimplify the denominator: a² t² + a² = a²(t² + 1). So the integral becomes (a/(a²)) ∫₀^∞ [ln(a) + ln(t)] / (t² + 1) dt. That simplifies to (1/a) [ ln(a) ∫₀^∞ 1/(t² +1) dt + ∫₀^∞ ln(t)/(t² +1) dt ].\\n\\nWait, the first integral ∫1/(t² +1) dt from 0 to ∞ is [arctan t] from 0 to ∞, which is π/2. So that term would be (1/a) * ln(a) * π/2. Then the second integral is (1/a) times ∫₀^∞ ln(t)/(t² +1) dt. Hmm, I need to figure out what that integral is.\\n\\nI recall that ∫₀^∞ ln(t)/(t² +1) dt might be zero because of symmetry. Let me check that. If I make a substitution u = 1/t, then t = 1/u, dt = -1/u² du. Let's see:\\n\\nWhen t approaches 0+, u approaches infinity, and when t approaches infinity, u approaches 0+. So the integral becomes ∫₀^∞ ln(t)/(t² +1) dt = ∫_{∞}^0 ln(1/u)/( (1/u²) +1 ) * (-1/u²) du. Let me simplify this.\\n\\nFirst, ln(1/u) is -ln u. The denominator becomes (1 + u²)/u². So the integral becomes ∫_{0}^∞ [ -ln u ] / ( (1 + u²)/u² ) * (1/u²) du. Simplify: multiply numerator and denominator: [-ln u * u² / (1 + u²)] * (1/u²) du = -ln u / (1 + u²) du. So we have:\\n\\n∫₀^∞ ln(t)/(t² +1) dt = ∫₀^∞ -ln u/(1 + u²) du. Let’s denote the original integral as I. Then I = -I, which implies that 2I = 0, so I = 0. Therefore, the second integral is zero. That's interesting!\\n\\nSo going back, the entire original integral is (1/a) * (ln(a) * π/2 + 0) = (π ln a)/(2a). Wait, but let me verify if that's correct. I think I saw somewhere that ∫₀^∞ ln x/(x² + a²) dx equals (π ln a)/(2a). But let me make sure I didn't make a mistake in the substitution.\\n\\nAlternatively, maybe there's another method. Let me try differentiation under the integral sign. Let’s consider the integral F(a) = ∫₀^∞ ln x/(x² + a²) dx. Maybe differentiate F(a) with respect to a? Let's compute F'(a).\\n\\nBut first, let's see if we can express F(a) in terms that can be differentiated. Let me make a substitution similar to before. Let t = x/a, so x = a t, dx = a dt. Then, as before, F(a) = ∫₀^∞ [ln(a t)] / (a² t² + a²) * a dt = (1/a) ∫₀^∞ [ln a + ln t]/(t² +1) dt. Which again gives the same result as before, (π ln a)/(2a) + (1/a) * 0 = π ln a/(2a). So if that substitution is correct, then that's the answer.\\n\\nBut maybe to check, let's compute the integral another way. Let's use contour integration in complex analysis, but since I'm not very sure about that, maybe a different substitution.\\n\\nWait, another approach: Let me consider x = a tan θ. Then, when x = 0, θ = 0, and x → ∞, θ → π/2. Then dx = a sec²θ dθ. Substitute into the integral:\\n\\n∫₀^{π/2} [ln(a tan θ)] / (a² tan² θ + a²) * a sec²θ dθ. Simplify denominator: a²(tan² θ +1 ) = a² sec² θ. So the integral becomes ∫ [ln(a) + ln(tan θ)] / (a² sec² θ) * a sec² θ dθ. Simplify: (a / a²) ∫₀^{π/2} [ln a + ln(tan θ)] dθ = (1/a) [ ln a ∫₀^{π/2} dθ + ∫₀^{π/2} ln(tan θ) dθ ].\\n\\nAgain, the first integral is (ln a)(π/2). The second integral ∫ ln(tan θ) dθ from 0 to π/2. Let’s see if that’s zero. Let’s substitute φ = π/2 - θ. Then when θ = 0, φ = π/2; θ = π/2, φ = 0. The integral becomes ∫_{π/2}^0 ln(tan(π/2 - φ)) (-dφ) = ∫₀^{π/2} ln(cot φ) dφ = ∫₀^{π/2} -ln(tan φ) dφ. So that integral becomes -∫₀^{π/2} ln(tan φ) dφ. If we denote the original integral as J, then J = -J, so 2J = 0, so J=0. Therefore, the integral of ln(tan θ) from 0 to π/2 is zero. So the entire integral becomes (1/a) * (π/2 ln a + 0 ) = π ln a/(2a). So that's the same result.\\n\\nAlternatively, maybe use differentiation under integral sign. Let’s consider a parameter b, set F(b) = ∫₀^∞ x^{b}/(x² + a²) dx. Then, differentiate F(b) with respect to b and set b=0 to get the integral involving ln x. Wait, let's check.\\n\\nSo F(b) = ∫₀^∞ x^b/(x² + a²) dx. Let me compute this integral first. Let substitution x = a t, so x^b = a^b t^b, dx = a dt. Then F(b) = ∫₀^∞ (a^b t^b)/(a² t² + a²) a dt = a^{b +1}/a² ∫₀^∞ t^b/(t² +1) dt = a^{b -1} ∫₀^∞ t^b/(t² +1) dt.\\n\\nNow, the integral ∫₀^∞ t^b/(t² +1) dt can be related to the Beta function. Let’s set u = t², so t = u^{1/2}, dt = (1/2) u^{-1/2} du. Then the integral becomes ∫₀^∞ (u^{b/2})/(u +1) * (1/2) u^{-1/2} du = (1/2) ∫₀^∞ u^{(b -1)/2}/(1 + u) du. This is (1/2) B( (b +1)/2, 1 - (b +1)/2 ) = (1/2) * π / sin( π ( (b +1)/2 ) ) by the Beta function relation to Gamma functions. Wait, Beta function is Γ(p)Γ(q)/Γ(p + q), and Γ(p)Γ(1 - p) = π / sin(π p).\\n\\nSo here, p = (b +1)/2, and 1 - p = (1 - b)/2. So Beta(p, 1 - p) = π / sin( π p ). So the integral ∫₀^∞ t^b/(t² +1) dt = (1/2) * π / sin( π ( (b +1)/2 ) ). Therefore, F(b) = a^{b -1} * π/(2 sin( π (b +1)/2 )).\\n\\nThen, the original integral is F'(0), since the integral with ln x is the derivative at b=0. Let’s compute F'(b):\\n\\nF(b) = (π/2) a^{b -1} / sin( π (b +1)/2 )\\n\\nTake natural log of F(b): ln F(b) = ln(π/2) + (b -1) ln a - ln sin( π (b +1)/2 )\\n\\nDifferentiate both sides:\\n\\nF’(b)/F(b) = ln a - [ π/2 * cos( π (b +1)/2 ) / sin( π (b +1)/2 ) ]\\n\\nSo F’(b) = F(b) [ ln a - (π/2) cot( π (b +1)/2 ) ]\\n\\nNow evaluate at b=0. First, F(0) = (π/2) a^{-1} / sin( π (1)/2 ) = (π/(2a)) / 1 = π/(2a). Then, cot( π (0 +1)/2 ) = cot(π/2) = 0. Because cot(π/2) is cos(π/2)/sin(π/2) = 0/1 = 0. Therefore,\\n\\nF’(0) = (π/(2a)) [ ln a - (π/2)*0 ] = (π/(2a)) ln a. So the integral is (π ln a)/(2a). \\n\\nThis matches the previous results, so it seems correct. Therefore, the answer should be π times ln a divided by (2a), which in box form is \\\\boxed{\\\\dfrac{\\\\pi \\\\ln a}{2a}}.\\n\\n**Final Answer**\\n\\\\boxed{\\\\dfrac{\\\\pi \\\\ln a}{2a}}\\n</think>To solve the integral \\\\(\\\\int_0^{+\\\\infty} \\\\frac{\\\\ln x}{x^2 + a^2} \\\\, \\\\text{d}x\\\\) for \\\\(a > 0\\\\), we start by making a substitution to simplify the expression.\\n\\nLet \\\\( t = \\\\frac{x}{a} \\\\). Then \\\\( x = at \\\\) and \\\\( \\\\text{d}x = a \\\\, \\\\text{d}t \\\\). The limits of integration remain the same since \\\\( x \\\\) ranges from 0 to \\\\( +\\\\infty \\\\), and so does \\\\( t \\\\). Substituting these into the integral, we get:\\n\\\\[\\n\\\\int_0^{+\\\\infty} \\\\frac{\\\\ln x}{x^2 + a^2} \\\\, \\\\text{d}x = \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln(at)}{(at)^2 + a^2} \\\\cdot a \\\\, \\\\text{d}t = \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln(at)}{a^2(t^2 + 1)} \\\\cdot a \\\\, \\\\text{d}t = \\\\frac{1}{a} \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln(at)}{t^2 + 1} \\\\, \\\\text{d}t\\n\\\\]\\n\\nWe can split the logarithm:\\n\\\\[\\n\\\\frac{1}{a} \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln(at)}{t^2 + 1} \\\\, \\\\text{d}t = \\\\frac{1}{a} \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln a + \\\\ln t}{t^2 + 1} \\\\, \\\\text{d}t = \\\\frac{1}{a} \\\\left( \\\\ln a \\\\int_0^{+\\\\infty} \\\\frac{1}{t^2 + 1} \\\\, \\\\text{d}t + \\\\int_0^{+\\\\infty} \\\\frac{\\\\ln t}{t^2 + 1} \\\\, \\\\text{d}t \\\\right)\\n\\\\]\\n\\nThe first integral is a standard result:\\n\\\\[\\n\\\\int_0^{+\\\\infty} \\\\frac{1}{t^2 + 1} \\\\, \\\\text{d}t = \\\\left[ \\\\arctan t \\\\right]_0^{+\\\\infty} = \\\\frac{\\\\pi}{2}\\n\\\\]\\n\\nFor the second integral, we use the symmetry property. Let \\\\( u = \\\\frac{1}{t} \\\\). Then \\\\( t = \\\\frac{1}{u} \\\\) and \\\\( \\\\text{d}t = -\\\\frac{1}{u^2} \\\\, \\\\text{d}u \\\\). As \\\\( t \\\\) goes from 0 to \\\\( +\\\\infty \\\\), \\\\( u \\\\) goes from \\\\( +\\\\infty \\\\) to 0. The integral becomes:\\n\\\\[\\n\\\\int_0^{+\\\\infty} \\\\frac{\\\\ln t}{t^2 + 1} \\\\, \\\\text{d}t = \\\\int_{+\\\\infty}^0 \\\\frac{\\\\ln \\\\left( \\\\frac{1}{u} \\\\right)}{\\\\left( \\\\frac{1}{u} \\\\right)^2 + 1} \\\\left( -\\\\frac{1}{u^2} \\\\right) \\\\, \\\\text{d}u = \\\\int_0^{+\\\\infty} \\\\frac{-\\\\ln u}{u^2 + 1} \\\\, \\\\text{d}u = -\\\\int_0^{+\\\\infty} \\\\frac{\\\\ln u}{u^2 + 1} \\\\, \\\\text{d}u\\n\\\\]\\nThus, the integral \\\\(\\\\int_0^{+\\\\infty} \\\\frac{\\\\ln t}{t^2 + 1} \\\\, \\\\text{d}t\\\\) is zero due to symmetry.\\n\\nCombining these results, we get:\\n\\\\[\\n\\\\frac{1}{a} \\\\left( \\\\ln a \\\\cdot \\\\frac{\\\\pi}{2} + 0 \\\\right) = \\\\frac{\\\\pi \\\\ln a}{2a}\\n\\\\]\\n\\nTherefore, the value of the integral is:\\n\\\\[\\n\\\\boxed{\\\\frac{\\\\pi \\\\ln a}{2a}}\\n\\\\]\", 'role': 'assistant'}], [{'content': 'Calculate $\\\\lim_{n\\\\to\\\\infty}\\\\sqrt[n]{n!}$.', 'role': 'user'}, {'content': \"<think>\\nOkay, so I need to find the limit as n approaches infinity of the nth root of n factorial, which is written as limₙ→∞ ⁿ√(n!). Hmm, let's see. I remember that when dealing with limits involving factorials and roots, there are some techniques like using Stirling's approximation. But I'm not sure if I should use that here or if there's another way.\\n\\nWait, Stirling's formula approximates n! for large n, right? It says that n! is approximately n^n e^{-n} √(2πn). If I use that, then maybe I can substitute that into the expression. Let me try that.\\n\\nSo, substituting Stirling's approximation into ⁿ√(n!), we get the nth root of [n^n e^{-n} √(2πn)]. Breaking that down, the nth root of n^n is just n, because (n^n)^(1/n) = n. Then the nth root of e^{-n} would be e^{-1}, since (e^{-n})^(1/n) = e^{-1}. Now, the nth root of √(2πn) is (2πn)^(1/(2n)). \\n\\nPutting that all together, the expression becomes n * e^{-1} * (2πn)^(1/(2n)). Now, I need to take the limit as n approaches infinity of this expression. Let's analyze each part.\\n\\nFirst, the term n/e. As n goes to infinity, n/e clearly goes to infinity. Wait, but that can't be right because if n! grows faster than exponential, but taking the nth root... Maybe my approach is wrong here. Because if n/e goes to infinity, then the limit would be infinity, but I have another term here: (2πn)^(1/(2n)).\\n\\nLet me check that again. Wait, maybe I made a mistake with Stirling's formula. Let me recall: Stirling's formula is n! ≈ (n/e)^n √(2πn). So the approximation is n! ≈ (n^n e^{-n}) √(2πn). So when I take the nth root of n!, it's [(n^n e^{-n}) √(2πn)]^(1/n) = (n^n)^(1/n) * (e^{-n})^(1/n) * (√(2πn))^(1/n).\\n\\nSimplifying each term:\\n\\n(n^n)^(1/n) = n.\\n\\n(e^{-n})^(1/n) = e^{-1}.\\n\\n(√(2πn))^(1/n) = (2πn)^(1/(2n)).\\n\\nNow, combining those: n * e^{-1} * (2πn)^(1/(2n)).\\n\\nSo we have the limit as n approaches infinity of [n / e] * (2πn)^{1/(2n)}.\\n\\nNow, I need to evaluate the limit of this expression. Let's analyze each part. The term (2πn)^{1/(2n)}. Let me take the natural logarithm to simplify the limit. Let's let L = lim_{n→∞} [n / e] * (2πn)^{1/(2n)}.\\n\\nTake the natural logarithm of L:\\n\\nln L = lim_{n→∞} [ln(n) - 1 + (1/(2n)) ln(2πn)].\\n\\nSimplify the third term: (1/(2n)) ln(2πn) = [ln(2π) + ln n]/(2n) = (ln(2π))/(2n) + (ln n)/(2n).\\n\\nAs n approaches infinity, (ln(2π))/(2n) tends to 0, and (ln n)/(2n) also tends to 0 because ln n grows much slower than n. So the entire third term goes to 0.\\n\\nTherefore, ln L = lim_{n→∞} [ln(n) - 1 + 0] = lim_{n→∞} (ln n - 1). But ln n approaches infinity as n approaches infinity. So ln L is infinity, which means that L is e^{infinity} which is infinity. But that contradicts what I know, because I think the limit of the nth root of n! should be infinity, but maybe not? Wait, let me think again.\\n\\nWait, actually, n! grows faster than exponential functions, but when taking the nth root, perhaps the growth rate is tempered. Wait, but for example, the nth root of n! is (n!)^{1/n}, and I think this limit is actually infinity. Wait, no, I might be confused here.\\n\\nWait, let me check with some specific values. Let's compute for n=2: sqrt(2!) = sqrt(2) ≈1.414. For n=3: cube root of 6 ≈ 1.817. For n=4: 4th root of 24 ≈ 2.213. For n=5: 5th root of 120 ≈ 2.605. So as n increases, it's increasing. So maybe the limit is indeed infinity. But I remember that there's a result saying that lim_{n→∞} (n!)^{1/n} = infinity. Wait, but maybe I'm wrong. Wait, I'm confused because sometimes in similar limits, like lim_{n→∞} (n)^{1/n} = 1, but here it's different.\\n\\nWait, let me check another approach. Let's use the property that for a sequence a_n, if the limit of a_{n+1}/a_n exists, then the limit of a_n^{1/n} is the same. So maybe using the ratio test idea. Let me apply that.\\n\\nLet a_n = n!. Then a_{n+1}/a_n = (n+1)! / n! = n+1. Then the limit as n approaches infinity of a_{n+1}/a_n is infinity. Therefore, the limit of a_n^{1/n} should also be infinity. Wait, that's a different approach and it seems to confirm that the limit is infinity. Hmm, but I'm getting conflicting thoughts here.\\n\\nWait, wait, actually, the root test says that if lim sup |a_n|^{1/n} = L, then the radius of convergence is 1/L. But in our case, we're not dealing with a series, but the limit of (n!)^{1/n}. But according to the root test, if you have a series sum a_n, then the radius of convergence is 1/L where L is the lim sup |a_n|^{1/n}. For the series sum n! x^n, the radius of convergence is 0 because lim (n!)^{1/n} = infinity, so 1/L = 0. That seems to confirm that (n!)^{1/n} tends to infinity. Therefore, the limit is infinity.\\n\\nBut earlier, using Stirling's approximation gave me that the expression is n/e times something approaching 1, so the limit would be infinity, which aligns with the root test result. So maybe the answer is indeed infinity. But I'm confused because some sources might say that lim (n!)^{1/n} = infinity, which is correct. Wait, yes, because n! grows faster than any exponential function, so taking the nth root would still leave it growing. For example, compare to an exponential function like a^n. Then (a^n)^{1/n} = a. But n! grows faster than a^n for any fixed a, so (n!)^{1/n} should grow without bound. Therefore, the limit is infinity. So maybe the answer is infinity.\\n\\nWait, but let me check again with Stirling's formula. If n! ≈ (n/e)^n sqrt(2πn), then (n!)^{1/n} ≈ (n/e) * (2πn)^{1/(2n)}. Now, (2πn)^{1/(2n)} is equal to e^{(ln(2πn))/(2n)}. As n approaches infinity, ln(2πn)/(2n) tends to 0, so the exponential term tends to e^0 = 1. Therefore, the approximation becomes (n!)^{1/n} ≈ n/e * 1 = n/e. Therefore, as n approaches infinity, this tends to infinity. So yes, the limit is infinity.\\n\\nWait, so then the answer should be infinity. But wait, maybe I made a mistake here because sometimes people consider that the nth root of n factorial is e, but that's incorrect. Let me check. If the limit were e, then as n increases, (n!)^{1/n} would approach e. But in reality, when n=5, we have (120)^{1/5} ≈ 2.6, which is already higher than e (approx 2.718), but wait, 2.6 is less than e. Wait, e is approximately 2.718, so 2.6 is less. Wait, but when n=10: 10! = 3628800. 3628800^(1/10) is approximately (3.6288 x 10^6)^(1/10). Let's calculate: 10^6^(1/10) = 10^(6/10) = 10^0.6 ≈ 3.98. Multiply by 3.6288^(0.1): 3.6288^0.1 ≈ 1.15. So approximately 3.98 * 1.15 ≈ 4.577. Which is higher than e. So as n increases, it's increasing beyond e. Then, if n=100, (100!)^{1/100} would be even larger, so the limit is indeed infinity.\\n\\nTherefore, the correct answer is infinity, which would be represented as \\\\boxed{\\\\infty}. But I should confirm once more. Let me recall that in some references, the limit is indeed infinity. For example, in mathematical analysis, it's known that n! grows faster than exponential functions, and thus its nth root also tends to infinity. So yes, the answer is infinity.\\n</think>To find the limit \\\\(\\\\lim_{n\\\\to\\\\infty}\\\\sqrt[n]{n!}\\\\), we use Stirling's approximation, which states that for large \\\\(n\\\\),\\n\\n\\\\[\\nn! \\\\approx \\\\left(\\\\frac{n}{e}\\\\right)^n \\\\sqrt{2\\\\pi n}.\\n\\\\]\\n\\nSubstituting this approximation into the expression, we have:\\n\\n\\\\[\\n\\\\sqrt[n]{n!} \\\\approx \\\\sqrt[n]{\\\\left(\\\\frac{n}{e}\\\\right)^n \\\\sqrt{2\\\\pi n}}.\\n\\\\]\\n\\nWe can break this down into simpler terms:\\n\\n\\\\[\\n\\\\sqrt[n]{\\\\left(\\\\frac{n}{e}\\\\right)^n \\\\sqrt{2\\\\pi n}} = \\\\sqrt[n]{\\\\left(\\\\frac{n}{e}\\\\right)^n} \\\\cdot \\\\sqrt[n]{\\\\sqrt{2\\\\pi n}}.\\n\\\\]\\n\\nFirst, consider \\\\(\\\\sqrt[n]{\\\\left(\\\\frac{n}{e}\\\\right)^n}\\\\):\\n\\n\\\\[\\n\\\\sqrt[n]{\\\\left(\\\\frac{n}{e}\\\\right)^n} = \\\\left(\\\\frac{n}{e}\\\\right)^{n/n} = \\\\frac{n}{e}.\\n\\\\]\\n\\nNext, consider \\\\(\\\\sqrt[n]{\\\\sqrt{2\\\\pi n}}\\\\):\\n\\n\\\\[\\n\\\\sqrt[n]{\\\\sqrt{2\\\\pi n}} = \\\\left(2\\\\pi n\\\\right)^{1/(2n)} = e^{\\\\ln\\\\left((2\\\\pi n)^{1/(2n)}\\\\right)} = e^{\\\\frac{\\\\ln(2\\\\pi n)}{2n}}.\\n\\\\]\\n\\nAs \\\\(n \\\\to \\\\infty\\\\), \\\\(\\\\frac{\\\\ln(2\\\\pi n)}{2n} \\\\to 0\\\\) because the logarithmic function grows much slower than the linear function. Therefore,\\n\\n\\\\[\\ne^{\\\\frac{\\\\ln(2\\\\pi n)}{2n}} \\\\to e^0 = 1.\\n\\\\]\\n\\nCombining these results, we get:\\n\\n\\\\[\\n\\\\sqrt[n]{n!} \\\\approx \\\\frac{n}{e} \\\\cdot 1 = \\\\frac{n}{e}.\\n\\\\]\\n\\nAs \\\\(n \\\\to \\\\infty\\\\), \\\\(\\\\frac{n}{e} \\\\to \\\\infty\\\\). Therefore, the limit is:\\n\\n\\\\[\\n\\\\lim_{n\\\\to\\\\infty}\\\\sqrt[n]{n!} = \\\\boxed{\\\\infty}.\\n\\\\]\", 'role': 'assistant'}], [{'content': 'Find all positive integers \\\\( n \\\\) such that \\\\( 3^{n-1} + 5^{n-1} \\\\) divides \\\\( 3^n + 5^n \\\\).', 'role': 'user'}, {'content': \"<think>\\nOkay, so I need to find all positive integers n where 3^{n-1} + 5^{n-1} divides 3^n + 5^n. Hmm, let's see. Let me start by understanding the problem. The expression 3^{n-1} + 5^{n-1} should divide 3^n + 5^n. So for a given n, when we divide 3^n + 5^n by 3^{n-1} + 5^{n-1}, the result should be an integer. \\n\\nFirst, maybe I can rewrite 3^n as 3 * 3^{n-1} and 5^n as 5 * 5^{n-1}. That might help me factor or simplify the expression. Let's try that:\\n\\n3^n + 5^n = 3*3^{n-1} + 5*5^{n-1}\\n\\nSo the divisor is 3^{n-1} + 5^{n-1}. Let's denote A = 3^{n-1} and B = 5^{n-1}, so the problem becomes: when does A + B divide 3A + 5B?\\n\\nSo, we want (3A + 5B)/(A + B) to be an integer. Let me compute this fraction:\\n\\n(3A + 5B)/(A + B) = [3A + 5B] / [A + B]\\n\\nLet me try to perform the division. Maybe express it as a combination:\\n\\nLet me see, 3A + 5B = 3(A + B) + 2B. So:\\n\\n[3(A + B) + 2B]/(A + B) = 3 + (2B)/(A + B)\\n\\nSo the expression simplifies to 3 + (2B)/(A + B). Therefore, for the entire expression to be an integer, (2B)/(A + B) must be an integer. Since 3 is already an integer, adding 3 to another integer will keep it integer. Therefore, the condition reduces to (2B)/(A + B) being an integer.\\n\\nSubstituting back A and B, we get:\\n\\n(2*5^{n-1}) / (3^{n-1} + 5^{n-1}) must be an integer. Let's denote this fraction as K, where K is a positive integer. So,\\n\\n2*5^{n-1} = K*(3^{n-1} + 5^{n-1})\\n\\nLet me rearrange the terms:\\n\\n2*5^{n-1} = K*3^{n-1} + K*5^{n-1}\\n\\nBring all terms to one side:\\n\\n2*5^{n-1} - K*5^{n-1} - K*3^{n-1} = 0\\n\\nFactor out 5^{n-1}:\\n\\n(2 - K)*5^{n-1} - K*3^{n-1} = 0\\n\\nSo,\\n\\n(2 - K)*5^{n-1} = K*3^{n-1}\\n\\nWhich can be rewritten as:\\n\\n(5/3)^{n-1} = K/(2 - K)\\n\\nSince the left-hand side is (5/3)^{n-1}, which is positive for all n, the right-hand side must also be positive. Therefore, K/(2 - K) > 0. For this fraction to be positive, either both numerator and denominator are positive or both are negative. \\n\\nCase 1: K > 0 and 2 - K > 0. So K > 0 and K < 2. Since K is a positive integer, K must be 1.\\n\\nCase 2: K < 0 and 2 - K < 0. But K is a positive integer, so this case is impossible. So only K = 1 is possible.\\n\\nTherefore, substituting K = 1 into the equation:\\n\\n(5/3)^{n-1} = 1/(2 - 1) = 1\\n\\nSo (5/3)^{n-1} = 1. Taking logarithms on both sides:\\n\\n(n - 1) * ln(5/3) = 0\\n\\nSince ln(5/3) is not zero, this implies n - 1 = 0 => n = 1.\\n\\nWait, but let me check that. If n = 1, let's test the original equation:\\n\\n3^{0} + 5^{0} = 1 + 1 = 2\\n\\n3^{1} + 5^{1} = 3 + 5 = 8\\n\\nThen 2 divides 8? Yes, since 8 / 2 = 4, which is an integer. So n = 1 is a solution.\\n\\nBut is that the only solution? Wait, maybe I should check for other possible values of K, but according to our earlier reasoning, only K = 1 is possible because K has to be a positive integer less than 2, so K=1.\\n\\nBut let me check if there could be other K values if the right-hand side isn't positive? Wait, we considered that. Alternatively, maybe I made a mistake here.\\n\\nWait, let me think again. Suppose K is a positive integer. Then for K/(2 - K) to be positive, both numerator and denominator must be positive, which requires K < 2, so K=1, or denominator and numerator both negative, which would require K > 2 but denominator 2 - K negative. But since K is positive, denominator is negative when K > 2, so then numerator would have to be negative as well. But numerator K is positive, so that's impossible. Therefore, only K=1 is possible.\\n\\nBut maybe there's a mistake here. Let me test for n=2:\\n\\nCheck n=2. Then 3^{1} + 5^{1} = 3 + 5 = 8, and 3^{2} + 5^{2} = 9 + 25 = 34. Does 8 divide 34? 34 divided by 8 is 4.25, which is not an integer. So n=2 doesn't work.\\n\\nn=3: 3^{2} +5^{2}=9+25=34. 3^3 +5^3=27+125=152. 34 divides 152? 152/34=4.4705... which is not integer. So no.\\n\\nWait, but according to our previous conclusion, only n=1 works. But maybe there's another value. Let me check n=0. Wait, n must be positive integers, so n=0 is not allowed.\\n\\nWait, let me check n=1 again. 3^{0} +5^{0}=2 divides 3^1 +5^1=8. 8 divided by 2 is 4. Correct.\\n\\nBut what about n=4? Let's compute 3^{3} +5^{3}=27+125=152. Then 3^4 +5^4=81 + 625=706. 152 divides 706? 706 divided by 152 is about 4.64, not integer.\\n\\nn=5: 3^4 +5^4=81+625=706. 3^5 +5^5=243 +3125=3368. 706 divides 3368? Let's divide 3368 by 706. 706*4=2824. 3368-2824=544. 544 is less than 706, so no.\\n\\nHmm, seems like only n=1 works. But let me see if there's a higher n where this might hold. Alternatively, maybe there's another approach. Let's consider writing 3^n +5^n divided by 3^{n-1} +5^{n-1}.\\n\\nExpress it as:\\n\\n(3^n +5^n)/(3^{n-1} +5^{n-1}) = (3*3^{n-1} +5*5^{n-1})/(3^{n-1} +5^{n-1}) = [3A +5B]/[A + B], where A=3^{n-1}, B=5^{n-1}\\n\\nThen, as before, this is 3 + (2B)/(A + B). So (2*5^{n-1})/(3^{n-1} +5^{n-1}) must be integer. Let's denote x = 5^{n-1}, y = 3^{n-1}, so the fraction becomes 2x/(x + y). For this to be integer, (x + y) must divide 2x. So x + y | 2x. Which implies x + y divides 2x. Therefore, x + y divides 2x. But x + y divides 2x if and only if x + y divides 2x mod (x + y). Wait, perhaps another way: since x + y divides 2x, then x + y divides 2x - k*(x + y) for any integer k. Let me pick k=1: 2x - (x + y) = x - y. So x + y divides x - y. Similarly, x + y divides x - y. Which would imply that x + y divides (x - y) + (x + y) = 2x. Which loops back. Maybe another approach.\\n\\nAlternatively, x + y divides 2x implies that (x + y) divides 2x. Since x and y are coprime? Wait, 3 and 5 are coprime, so 3^{n-1} and 5^{n-1} are coprime. Therefore, x and y are coprime. Therefore, x + y and x are coprime? Not necessarily. Let's see: GCD(x + y, x) = GCD(y, x) = 1, since x and y are coprime. So GCD(x + y, x) = 1, similarly GCD(x + y, y) = GCD(x, y) =1. Therefore, x + y divides 2, since x + y divides 2x and GCD(x + y, x)=1, so x + y divides 2. Similarly, x + y divides 2.\\n\\nTherefore, x + y is a divisor of 2. But x = 5^{n-1} and y=3^{n-1}, both are positive integers, so x + y >= 5^{0} + 3^{0} = 2. So the possible divisors of 2 are 1 and 2. But x + y >=2, so x + y=2.\\n\\nThus, 3^{n-1} +5^{n-1}=2. Since 3^{n-1} and5^{n-1} are both positive integers. Let's find n such that 3^{n-1} +5^{n-1}=2.\\n\\nPossible when n-1=0, since 3^0 +5^0=1+1=2. So n-1=0 => n=1. For n>1, 3^{n-1} +5^{n-1} >=3 +5=8, which is already greater than 2. Therefore, the only solution is n=1.\\n\\nTherefore, the only positive integer n is 1.\\n\\nWait, that seems consistent with earlier results. So maybe this is the only solution. So the answer is \\\\boxed{1}\\n\\nBut let me just verify once again. If n=1, then 3^{0} +5^{0}=2 divides 3^1 +5^1=8. 8 divided by 2 is 4. Correct. For n=2, the divisor is 8, as before, and the dividend is 34, which is not divisible by 8. For n=3, divisor is 34, dividend is 152, which is not divisible by 34. For n=4, divisor is 152, dividend is 706, which is not divisible. So yeah, seems like n=1 is the only solution.\\n\\nAnother approach: Let me suppose that n >1. Then 5^{n-1} >3^{n-1}, so 3^{n-1} +5^{n-1} is dominated by 5^{n-1}. Similarly, 3^n +5^n =3*3^{n-1} +5*5^{n-1}. Let me write the ratio:\\n\\n(3*3^{n-1} +5*5^{n-1})/(3^{n-1} +5^{n-1}) = (3*(3/5)^{n-1} +5)/( (3/5)^{n-1} +1 )\\n\\nLet me denote t = (3/5)^{n-1}. Since n >=1, and 3/5 <1, as n increases, t decreases towards 0. So the expression becomes (3t +5)/(t +1). Let's compute this:\\n\\n(3t +5)/(t +1) = (5 +3t)/(1 +t). Let me perform division:\\n\\n(5 +3t) divided by (1 +t). Let's write 5 +3t =3(1 +t) +2. So:\\n\\n= 3 + 2/(1 +t)\\n\\nTherefore, the ratio is 3 + 2/(1 + t). Since t = (3/5)^{n-1}, which is a positive real number less than or equal to 1 (for n >=1, n=1 gives t=1, n=2 gives t=3/5, etc.)\\n\\nSo 1 + t >=1 +0=1. Therefore, 2/(1 + t) <=2. Therefore, the expression 3 + 2/(1 + t) is between 3 and 5. For the entire expression to be integer, the fractional part 2/(1 + t) must be an integer. So 2/(1 + t) must be integer. Let me call this k, an integer. So 2/(1 +t)=k, which implies 1 + t=2/k. Therefore, t=2/k -1. But t is positive, so 2/k -1 >0 =>2/k >1 =>k <2. Since k is a positive integer (because 2/(1 + t) must be positive integer), possible k=1. Then:\\n\\nk=1: 1 + t=2/1=2 =>t=1. But t=(3/5)^{n-1}=1. So (3/5)^{n-1}=1 implies n-1=0 =>n=1. So only solution. If k=0, but k must be positive integer, so k=1 is the only possibility.\\n\\nTherefore, again, n=1 is the only solution.\\n\\nAnother way: Maybe assume that for n ≥2, 3^{n-1} +5^{n-1} divides 3^n +5^n. Let's denote d_n =3^{n-1} +5^{n-1}. Then we have d_n divides 3^n +5^n =3*3^{n-1} +5*5^{n-1}=3*(d_n -5^{n-1}) +5*5^{n-1}=3d_n -3*5^{n-1} +5*5^{n-1}=3d_n +2*5^{n-1}\\n\\nSo 3^n +5^n =3d_n +2*5^{n-1}. Therefore, d_n divides 3d_n +2*5^{n-1}, which implies d_n divides 2*5^{n-1}. Therefore, d_n |2*5^{n-1}.\\n\\nBut d_n=3^{n-1} +5^{n-1}. So 3^{n-1} +5^{n-1} divides 2*5^{n-1}.\\n\\nTherefore, since 3^{n-1} and5^{n-1} are coprime (as 3 and5 are coprime), we can use the property that if a + b divides c, and a and b are coprime, then a + b divides c and a divides c -b*k for some k. Wait, maybe not. Alternatively, since GCD(3^{n-1},5^{n-1})=1, then GCD(3^{n-1},5^{n-1})=1.\\n\\nTherefore, since 3^{n-1} +5^{n-1} divides 2*5^{n-1}, and GCD(3^{n-1} +5^{n-1},5^{n-1}) divides GCD(3^{n-1},5^{n-1})=1. Therefore, 3^{n-1} +5^{n-1} divides 2. But as before, 3^{n-1} +5^{n-1}=2 only when n=1. For n>=2, 3^{n-1} +5^{n-1} >=3 +5=8>2, so the only solution is n=1.\\n\\nTherefore, after multiple approaches, the only positive integer n is 1. So the answer is \\\\boxed{1}\\n</think>To find all positive integers \\\\( n \\\\) such that \\\\( 3^{n-1} + 5^{n-1} \\\\) divides \\\\( 3^n + 5^n \\\\), we start by expressing the given condition in a more manageable form.\\n\\nFirst, we rewrite \\\\( 3^n + 5^n \\\\) as:\\n\\\\[\\n3^n + 5^n = 3 \\\\cdot 3^{n-1} + 5 \\\\cdot 5^{n-1}\\n\\\\]\\n\\nWe need \\\\( 3^{n-1} + 5^{n-1} \\\\) to divide \\\\( 3 \\\\cdot 3^{n-1} + 5 \\\\cdot 5^{n-1} \\\\). Let \\\\( A = 3^{n-1} \\\\) and \\\\( B = 5^{n-1} \\\\). Then the problem reduces to finding when \\\\( A + B \\\\) divides \\\\( 3A + 5B \\\\):\\n\\\\[\\n\\\\frac{3A + 5B}{A + B} = 3 + \\\\frac{2B}{A + B}\\n\\\\]\\n\\nFor \\\\( \\\\frac{3A + 5B}{A + B} \\\\) to be an integer, \\\\( \\\\frac{2B}{A + B} \\\\) must be an integer. Substituting back \\\\( A \\\\) and \\\\( B \\\\):\\n\\\\[\\n\\\\frac{2 \\\\cdot 5^{n-1}}{3^{n-1} + 5^{n-1}}\\n\\\\]\\n\\nLet \\\\( K \\\\) be this integer:\\n\\\\[\\n2 \\\\cdot 5^{n-1} = K \\\\cdot (3^{n-1} + 5^{n-1})\\n\\\\]\\n\\nRearranging terms:\\n\\\\[\\n(2 - K) \\\\cdot 5^{n-1} = K \\\\cdot 3^{n-1}\\n\\\\]\\n\\nThis implies:\\n\\\\[\\n\\\\left(\\\\frac{5}{3}\\\\right)^{n-1} = \\\\frac{K}{2 - K}\\n\\\\]\\n\\nFor the right-hand side to be positive, \\\\( K \\\\) must be a positive integer such that \\\\( 2 - K > 0 \\\\). The only positive integer \\\\( K \\\\) that satisfies this is \\\\( K = 1 \\\\):\\n\\\\[\\n\\\\left(\\\\frac{5}{3}\\\\right)^{n-1} = 1 \\\\implies n - 1 = 0 \\\\implies n = 1\\n\\\\]\\n\\nWe verify \\\\( n = 1 \\\\):\\n\\\\[\\n3^{0} + 5^{0} = 2 \\\\quad \\\\text{and} \\\\quad 3^{1} + 5^{1} = 8\\n\\\\]\\nSince \\\\( 2 \\\\) divides \\\\( 8 \\\\), \\\\( n = 1 \\\\) is indeed a solution.\\n\\nFor \\\\( n \\\\geq 2 \\\\), \\\\( 3^{n-1} + 5^{n-1} \\\\) exceeds 2, and it cannot divide \\\\( 3^n + 5^n \\\\). This is confirmed by further analysis of the divisibility conditions.\\n\\nThus, the only positive integer \\\\( n \\\\) is:\\n\\\\[\\n\\\\boxed{1}\\n\\\\]\", 'role': 'assistant'}]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_data[\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38332c92-bba4-4a16-96ca-d7de7c838340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.',\n",
       "  'role': 'user'},\n",
       " {'content': \"<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>To solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_data[\"conversations\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d23611-7148-4d05-943c-98bbce706aed",
   "metadata": {},
   "source": [
    "接下来将其带入Qwen3的提示词模板中进行转化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693524a-a8e6-4a14-9050-27182c29c0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Column'>\n",
      "19252\n",
      "[[{'content': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.', 'role': 'user'}, {'content': \"<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>To solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]\", 'role': 'assistant'}], [{'content': 'Let \\\\( \\\\sum a_n \\\\) be a convergent series with \\\\( a_n \\\\geq 0 \\\\). Determine the nature of the series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\).', 'role': 'user'}, {'content': \"<think>\\nOkay, so I have this problem here where I need to determine the nature of the series ∑(a_n²)/(1 + a_n²) given that the series ∑a_n is convergent and each a_n is non-negative. Alright, let's start by understanding what the problem is asking. \\n\\nFirst, I know that ∑a_n is convergent, which means that the sequence a_n approaches zero as n approaches infinity. Because all the terms a_n are non-negative, they must be decreasing to zero, right? Wait, actually, convergence of the series only requires that the terms approach zero, but they don't necessarily have to be decreasing. For example, even if the terms bounce around a bit but still approach zero, the series can still converge if they do so quickly enough. But since the problem states a_n ≥ 0, we don't have to worry about negative terms messing up the convergence through cancellation.\\n\\nNow, the question is about the series ∑(a_n²)/(1 + a_n²). We need to figure out if this series converges or diverges. Let me think about how the terms of this new series compare to the original series. Since each a_n is non-negative, a_n² is also non-negative, and dividing by 1 + a_n² will make each term of the new series non-negative as well. So, we're dealing with a series of non-negative terms, which is good because tests like the comparison test apply.\\n\\nLet me recall the comparison test: if 0 ≤ b_n ≤ c_n for all n, and ∑c_n converges, then ∑b_n converges. Conversely, if ∑b_n diverges, then ∑c_n diverges. So, if I can compare (a_n²)/(1 + a_n²) to some other series that I know converges or diverges, that could help.\\n\\nAlternatively, maybe I can use the limit comparison test. If I take the limit of [ (a_n²)/(1 + a_n²) ] / a_n as n approaches infinity, that might tell me something. Let's compute that limit.\\n\\nLimit as n → ∞ of [ (a_n²)/(1 + a_n²) ] / a_n = limit of [a_n² / (a_n(1 + a_n²))] = limit of [a_n / (1 + a_n²)].\\n\\nNow, since ∑a_n converges, we know that a_n must approach 0. So as n becomes large, a_n is approaching 0. Therefore, the denominator 1 + a_n² approaches 1. So the entire expression is approximately a_n / 1 = a_n. Therefore, the limit is 0. \\n\\nWait, but in the limit comparison test, if the limit is 0 and ∑a_n converges, then if the original series (the one with terms (a_n²)/(1 + a_n²)) is being compared to a convergent series, then what's the conclusion?\\n\\nHmm, let me recall: If the limit of b_n / c_n is 0, and ∑c_n converges, then ∑b_n converges. Because if b_n is eventually less than c_n, then the comparison test applies. So since (a_n²)/(1 + a_n²) / a_n → 0, and ∑a_n converges, then by the limit comparison test, ∑(a_n²)/(1 + a_n²) also converges. \\n\\nWait, that seems straightforward, right? Because the limit is 0, which is finite, and the series we're comparing to (∑a_n) converges. Therefore, ∑(a_n²)/(1 + a_n²) must converge. \\n\\nBut let me verify this with another approach to make sure I'm not missing something. Let's think about the behavior of the terms (a_n²)/(1 + a_n²). Since a_n is non-negative and approaching 0, when n is large enough, a_n is less than or equal to 1, right? Because if a_n approaches 0, there exists some N such that for all n ≥ N, a_n ≤ 1. So for those n, a_n² ≤ a_n. Therefore, (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n. \\n\\nTherefore, for n ≥ N, (a_n²)/(1 + a_n²) ≤ a_n. Since ∑a_n converges, then by comparison test, ∑(a_n²)/(1 + a_n²) also converges. \\n\\nWait, that's a different argument. Let's see. So if a_n is less than or equal to 1, then a_n² is less than or equal to a_n, so (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n. Therefore, since ∑a_n converges, the tail of the series ∑(a_n²)/(1 + a_n²) is dominated by ∑a_n, hence converges. Therefore, the entire series converges.\\n\\nAlternatively, even if a_n is sometimes greater than 1, but since the series ∑a_n converges, the terms a_n must approach zero, so beyond some N, they are all less than 1. So the initial terms where a_n might be greater than 1 can only contribute finitely many terms to the series, which doesn't affect convergence. Therefore, we can ignore those and focus on the tail where a_n < 1. Then, as above, in that region, (a_n²)/(1 + a_n²) ≤ a_n² ≤ a_n, so again, since ∑a_n converges, so does ∑(a_n²)/(1 + a_n²).\\n\\nAlternatively, another way to think about it is to note that (a_n²)/(1 + a_n²) is approximately a_n² when a_n is small. Since a_n approaches 0, a_n² is going to be much smaller than a_n. Therefore, if ∑a_n converges, then ∑a_n² might also converge, but actually, that's not necessarily always true. For example, take a_n = 1/n. Then ∑a_n diverges, but ∑a_n² converges. But in this problem, ∑a_n converges. So, for example, if a_n = 1/n², then ∑a_n converges (it's a p-series with p=2>1), and ∑a_n² would be ∑1/n^4, which also converges. But if a_n is 1/n^(1/2), but then ∑a_n would diverge. But in our problem, ∑a_n converges.\\n\\nWait, but the convergence of ∑a_n doesn't necessarily imply that a_n is summable with a higher exponent. For example, take a_n = 1/(n(log n)^2), which converges (by the integral test), then a_n² = 1/(n²(log n)^4), which would converge as well. But if a_n is like 1/n^(1/2 + ε) for some ε>0, then ∑a_n converges if 1/2 + ε > 1, i.e., ε > 1/2. Then a_n² would be 1/n^(1 + 2ε), which converges as well. So in these cases, if a_n is eventually small enough such that ∑a_n converges, then a_n² is even smaller, so ∑a_n² converges. However, perhaps there is a case where ∑a_n converges but ∑a_n² diverges? Hmm. Wait, no. If a_n is non-negative and ∑a_n converges, then a_n must approach zero, and for sufficiently large n, a_n ≤1. Therefore, a_n² ≤ a_n for those n, hence ∑a_n² converges by comparison to ∑a_n. Wait, that's true, isn't it? If a_n is eventually less than or equal to 1, then a_n² ≤ a_n, so ∑a_n² converges. Therefore, in our problem, the series ∑(a_n²)/(1 + a_n²) is dominated by ∑a_n², which is dominated by ∑a_n. So, if we can bound (a_n²)/(1 + a_n²) ≤ a_n², and ∑a_n² converges, then ∑(a_n²)/(1 + a_n²) would converge as well.\\n\\nBut actually, (a_n²)/(1 + a_n²) ≤ a_n², right? Because 1 + a_n² ≥1, so dividing by something bigger than 1 makes the fraction smaller. So (a_n²)/(1 + a_n²) ≤ a_n². And since ∑a_n² converges, as I thought before, then by comparison, ∑(a_n²)/(1 + a_n²) also converges.\\n\\nWait, but I had earlier compared (a_n²)/(1 + a_n²) directly to a_n and used the limit comparison test. So both approaches seem valid. Let me just check with an example. Suppose a_n = 1/n². Then ∑a_n converges. Then (a_n²)/(1 + a_n²) = (1/n^4)/(1 + 1/n²) = 1/(n^4 + n²) ≈ 1/n^4 for large n, so the series ∑1/(n^4 + n²) converges. Another example: a_n = 1/n. But wait, ∑a_n diverges here, which doesn't fit the problem's condition. So if a_n is 1/n^p with p>1, then ∑a_n converges, and ∑a_n² converges as well. \\n\\nAlternatively, take a_n = 1/(n^(1/2 + ε)), with ε > 1/2 so that ∑a_n converges. Then a_n² = 1/n^{1 + 2ε}, and since 1 + 2ε > 1, ∑a_n² converges. \\n\\nBut actually, the key point here is that since a_n tends to 0, for sufficiently large n, a_n ≤1, so a_n² ≤ a_n, so ∑a_n² is dominated by ∑a_n. Therefore, ∑a_n² converges, and since (a_n²)/(1 + a_n²) ≤ a_n², then ∑(a_n²)/(1 + a_n²) converges as well. Therefore, the series in question converges.\\n\\nWait, but maybe I should be careful here. Let me think again. If a_n is 0, then the term is 0. If a_n is very large, say approaching infinity, but wait, in our case, a_n approaches 0, so that's not possible. But in the problem, the series ∑a_n is convergent, so the terms a_n must approach zero. Therefore, beyond some N, a_n ≤1, so the denominator 1 + a_n² ≥1, hence (a_n²)/(1 + a_n²) ≤a_n². Therefore, for all n beyond N, the terms are bounded above by a_n², which is summable because a_n² ≤ a_n for n ≥N (since a_n ≤1). Therefore, ∑(a_n²)/(1 + a_n²) converges by comparison to ∑a_n.\\n\\nAlternatively, to put it more formally:\\n\\nSince ∑a_n converges, a_n →0. Therefore, there exists N such that for all n ≥N, a_n ≤1. For those n, a_n² ≤a_n. Therefore, (a_n²)/(1 + a_n²) ≤a_n² ≤a_n. Hence, ∑_{n=N}^∞ (a_n²)/(1 + a_n²) ≤ ∑_{n=N}^∞ a_n < ∞. Therefore, the tail converges, and adding the first N-1 terms (which are finite) doesn't affect convergence. Therefore, the entire series ∑(a_n²)/(1 + a_n²) converges.\\n\\nAlternatively, another approach: note that (a_n²)/(1 + a_n²) ≤ min(a_n², 1). But since a_n →0, eventually min(a_n², 1) = a_n², so again, ∑a_n² converges. So this gives another way to see it.\\n\\nWait, but in all these approaches, the conclusion is the same: the series ∑(a_n²)/(1 + a_n²) converges. Therefore, the answer should be that the series converges.\\n\\nBut wait, let me check with a concrete example where a_n is sometimes large. Suppose a_n is 1 for some n. Then (1^2)/(1 +1^2)=1/2. But since ∑a_n converges, a_n must approach 0, so only finitely many terms can be ≥1. So those terms contribute a finite sum, and the rest of the terms where a_n <1 can be compared to a_n² or a_n. Therefore, even if there are a few terms where a_n is large, they don't affect the convergence of the series.\\n\\nFor example, take a_n = 2 for n=1, and a_n =1/n² for n≥2. Then ∑a_n =2 + ∑1/n², which converges. Then ∑(a_n²)/(1 +a_n²) = (4/5) + ∑_{n=2}^∞ [ (1/n^4)/(1 +1/n²) ] = 4/5 + ∑_{n=2}^\\\\infty [1/(n^4 +n²)]. Since the tail is convergent, the entire series converges.\\n\\nAnother test case: a_n =1 for n=1, and a_n=0 for n≥2. Then ∑a_n converges (sum is 1), and ∑(a_n²)/(1 +a_n²)=1/2 + 0, which is obviously convergent.\\n\\nSo, all these test cases support the conclusion that ∑(a_n²)/(1 +a_n²) converges.\\n\\nBut let me think if there could be any counterexample. Suppose ∑a_n converges, but ∑(a_n²)/(1 +a_n²) diverges. Is that possible?\\n\\nIf ∑a_n converges, then a_n →0. So for sufficiently large n, a_n is small. Therefore, (a_n²)/(1 +a_n²) ≈ a_n². Therefore, if ∑a_n² diverges, then ∑(a_n²)/(1 +a_n²) would also diverge. But can ∑a_n converge while ∑a_n² diverges?\\n\\nWait, but earlier, we thought that if a_n ≤1 for sufficiently large n, then a_n² ≤ a_n, so ∑a_n² converges by comparison. Therefore, if ∑a_n converges, then ∑a_n² must converge as well. Therefore, ∑(a_n²)/(1 +a_n²) is dominated by ∑a_n², which converges, hence itself converges. Therefore, such a counterexample isn't possible. Therefore, the conclusion is that ∑(a_n²)/(1 +a_n²) must converge.\\n\\nSo, after all this thinking and checking with different examples and approaches, I believe the answer is that the series ∑(a_n²)/(1 +a_n²) converges.\\n</think>To determine the nature of the series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\) given that \\\\( \\\\sum a_n \\\\) is a convergent series with \\\\( a_n \\\\geq 0 \\\\), we proceed as follows:\\n\\n1. **Convergence of \\\\( \\\\sum a_n \\\\)**:\\n   Since \\\\( \\\\sum a_n \\\\) converges, it follows that \\\\( a_n \\\\to 0 \\\\) as \\\\( n \\\\to \\\\infty \\\\).\\n\\n2. **Behavior of \\\\( \\\\frac{a_n^2}{1 + a_n^2} \\\\)**:\\n   For sufficiently large \\\\( n \\\\), \\\\( a_n \\\\) is small. Specifically, there exists an integer \\\\( N \\\\) such that for all \\\\( n \\\\geq N \\\\), \\\\( a_n \\\\leq 1 \\\\).\\n\\n3. **Comparison with \\\\( a_n \\\\)**:\\n   For \\\\( n \\\\geq N \\\\):\\n   \\\\[\\n   \\\\frac{a_n^2}{1 + a_n^2} \\\\leq \\\\frac{a_n^2}{1} = a_n^2 \\\\leq a_n.\\n   \\\\]\\n   This inequality holds because \\\\( a_n \\\\leq 1 \\\\) implies \\\\( a_n^2 \\\\leq a_n \\\\).\\n\\n4. **Convergence of \\\\( \\\\sum a_n^2 \\\\)**:\\n   Since \\\\( \\\\sum a_n \\\\) converges and \\\\( a_n \\\\to 0 \\\\), it follows that \\\\( a_n^2 \\\\) is also non-negative and tends to zero. Moreover, \\\\( a_n^2 \\\\leq a_n \\\\) for \\\\( n \\\\geq N \\\\), and by the comparison test, \\\\( \\\\sum a_n^2 \\\\) converges.\\n\\n5. **Convergence of \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\)**:\\n   For \\\\( n \\\\geq N \\\\), we have:\\n   \\\\[\\n   \\\\frac{a_n^2}{1 + a_n^2} \\\\leq a_n.\\n   \\\\]\\n   Since \\\\( \\\\sum a_n \\\\) converges, the comparison test implies that \\\\( \\\\sum_{n=N}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} \\\\) also converges.\\n\\n6. **Final Conclusion**:\\n   The series \\\\( \\\\sum_{n=1}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} \\\\) can be split into two parts:\\n   \\\\[\\n   \\\\sum_{n=1}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2} = \\\\sum_{n=1}^{N-1} \\\\frac{a_n^2}{1 + a_n^2} + \\\\sum_{n=N}^\\\\infty \\\\frac{a_n^2}{1 + a_n^2}.\\n   \\\\]\\n   The first part is a finite sum, and the second part converges by the comparison test. Therefore, the entire series \\\\( \\\\sum \\\\frac{a_n^2}{1 + a_n^2} \\\\) converges.\\n\\n\\\\[\\n\\\\boxed{\\\\text{The series converges}}\\n\\\\]\", 'role': 'assistant'}]]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据结构\n",
    "print(type(reasoning_data[\"conversations\"]))\n",
    "print(len(reasoning_data[\"conversations\"]))\n",
    "print(reasoning_data[\"conversations\"][:2])  # 查看前两条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb8d7e-309d-40e7-9740-e32687a57092",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversations = list(reasoning_data[\"conversations\"])  # 转换为Python列表\n",
    "\n",
    "# 批量处理\n",
    "rendered_all = tokenizer.apply_chat_template(\n",
    "    all_conversations,  # 列表的列表\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c3b0b-23dd-4738-9b9a-b708193c8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_conversations = rendered_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75efa4d-b957-45d1-9b9f-30d12cb2cc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'list object' has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m reasoning_conversations = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreasoning_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1667\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1666\u001b[39m template_kwargs = {**\u001b[38;5;28mself\u001b[39m.special_tokens_map, **kwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m rendered_chat, generation_indices = \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1679\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:539\u001b[39m, in \u001b[36mrender_jinja_template\u001b[39m\u001b[34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m     all_generation_indices.append(generation_indices)\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     rendered_chat = \u001b[43mcompiled_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (final_message.strip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rendered_chat) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    548\u001b[39m         continue_final_message_tag.strip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rendered_chat\n\u001b[32m    549\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/jinja2/environment.py:1295\u001b[39m, in \u001b[36mTemplate.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.environment.concat(\u001b[38;5;28mself\u001b[39m.root_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/jinja2/environment.py:942\u001b[39m, in \u001b[36mEnvironment.handle_exception\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source=source)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<template>:23\u001b[39m, in \u001b[36mtop-level template code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUndefinedError\u001b[39m: 'list object' has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    reasoning_data[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3a14d-410b-4d48-9d83-e664a3d86d32",
   "metadata": {},
   "source": [
    "最后创建的数据就是一个包含多个对话信息的list："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6050e-e371-4be8-bd81-4298cde48f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nGiven $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>\\n\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<|im_end|>\\n\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb855ef-905b-4cfc-82cc-49918a950f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19252"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reasoning_conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cb55c-20bc-475a-b06a-097f0a4177fc",
   "metadata": {},
   "source": [
    "之后即可带入这些数据进行微调。能看出每条数据的格式都和Unsloth底层对话API创建的数据格式类似，之后我们或许可以借助Unsloth底层对话API来创建微调数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d2462-e7fa-49d8-8ea3-cb7b935b0c45",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后继续处理non_reasoning_conversations数据集，由于该数据集采用了sharegpt对话格式，因此可以直接借助Unsloth的standardize_sharegpt库进行数据集的格式转化，转化效果如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea77e7f-1418-479f-856d-12dbb7f7576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218b152-2a05-4b13-bafc-693023db188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7685d16214a4fb012bc66eb88ce5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=256):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = standardize_sharegpt(non_reasoning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3323f-4c30-4893-8f25-c21097eea7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.',\n",
       "  'role': 'user'},\n",
       " {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"conversations\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53e554-0dff-4e24-900d-8144158877fa",
   "metadata": {},
   "source": [
    "接下来即可直接带入Qwen3对话模板中进行格式调整："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509ddb8-0d5a-4ba7-8ae9-80ab642a4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功处理 100000 个对话\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "# 标准化数据集\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "# 将Column对象转换为Python列表\n",
    "conversations_list = list(dataset[\"conversations\"])\n",
    "\n",
    "# 应用chat template\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    conversations_list,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"成功处理 {len(non_reasoning_conversations)} 个对话\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fee84b-4c8c-4791-988f-b9cc4a848880",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'list object' has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m non_reasoning_conversations = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1667\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1666\u001b[39m template_kwargs = {**\u001b[38;5;28mself\u001b[39m.special_tokens_map, **kwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m rendered_chat, generation_indices = \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1679\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:539\u001b[39m, in \u001b[36mrender_jinja_template\u001b[39m\u001b[34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m     all_generation_indices.append(generation_indices)\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     rendered_chat = \u001b[43mcompiled_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (final_message.strip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rendered_chat) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    548\u001b[39m         continue_final_message_tag.strip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rendered_chat\n\u001b[32m    549\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/jinja2/environment.py:1295\u001b[39m, in \u001b[36mTemplate.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.environment.concat(\u001b[38;5;28mself\u001b[39m.root_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/jinja2/environment.py:942\u001b[39m, in \u001b[36mEnvironment.handle_exception\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source=source)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<template>:23\u001b[39m, in \u001b[36mtop-level template code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUndefinedError\u001b[39m: 'list object' has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f2c60-fc8c-4f38-9f64-5c2c8d53ccbe",
   "metadata": {},
   "source": [
    "最终每一条数据格式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7f8e1-e7e3-417a-9906-5186b17d8d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.<|im_end|>\\n'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc8ac1-77a5-44d6-9714-7139cabc2067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19252\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_conversations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070760-56cb-49e2-ae5f-030c54c204b6",
   "metadata": {},
   "source": [
    "自此即完成了每个数据集的格式调整工作，不过这两个数据集并不均衡，能看得出非推理类数据集的长度更长。我们假设希望模型保留一定的推理能力，但又特别希望它作为一个聊天模型来使用。因此，我们需要定义一个 仅聊天数据的比例。目标是从两个数据集中构建一个混合训练集。这里我们可以设定一个 25% 推理数据、75% 聊天数据的比例：也就是说，从推理数据集中抽取 25%（或者说，抽取占比为 100% - 聊天数据占比 的部分），最后将这两个数据集合并起来即可。这里我们需要先将上述list格式的数据转化为pd.Series数据，然后进行采样，并最终将其转化为dataset类型对象。（此外也可以先转化为dataset对象类型，然后再进行采样）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6674d-7241-4cff-ab4c-2470dfb65305",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_percentage = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ea68f-7dce-4947-9d8c-89915c9f66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations) \n",
    "# pd.Series:\n",
    "# 普通的 Python 列表 (list)：就像一排没有编号的储物柜，你只能通过位置（0, 1, 2...）来找东西。\n",
    "# Pandas Series：就像一列有标签的 Excel 表格。它不仅存数据，还给每个数据配了一个“索引”（Index）。\n",
    "# 为了使用 Pandas 提供的高级操作功能，这些功能是原生 Python 列表不具备的：\n",
    "# .sample()：快速进行随机采样。在你的代码里，你用它从大量的普通对话中精确抽取出一部分，用来匹配推理数据的比例。\n",
    "# .concat()：像接火车一样把两个 Series（推理数据 + 抽取的普通数据）拼在一起。\n",
    "# .name = \"text\"：直接给这一列数据起个名字。这非常重要，因为后续转化为 Hugging Face Dataset 时，它会自动识别 \"text\" 作为列名，这也是模型训练器（Trainer）默认寻找的目标。\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    int(len(reasoning_conversations) * (1.0 - chat_percentage)),\n",
    "    random_state = 2407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f55b30-64be-40cf-991f-9566bc9ec50e",
   "metadata": {},
   "source": [
    "然后进行拼接和转化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eab003-d26c-441a-a280-8a39bde84ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2f39d-2fe2-4550-8418-9ab55618e977",
   "metadata": {},
   "source": [
    "转化后数据集如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46acfd-378f-4460-9f68-f77a7babfb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24065"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4119cf5-26f8-4bd2-8768-4c89eae49f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788635d-edd5-422a-85be-de5341037947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 24065\n",
       "})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc320c-97a9-4672-8880-5b7ee6f490c1",
   "metadata": {},
   "source": [
    "对话类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c2b97-fb59-45a0-b468-d54edaa82c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|im_start|>user\\nCalculate the pH during a titration when 9.54 mL of a 0.15 M HCl solution has reacted with 22.88 mL of a 0.14 M NaOH solution?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nStart by calculating the number of moles of hydrochloric acid and the number of moles of sodium hydroxide that took part in the reaction.\\n\\n9.54 mL of 0.15 M HCl solution contains:\\n\\n0.15 mol HCl / 1000 mL HCl solution * 9.54 mL HCl solution = 0.001431 mol HCl\\n\\n22.88 mL of 0.14 M NaOH solution contains:\\n\\n0.14 mol NaOH / 1000 mL NaOH solution * 22.88 mL NaOH solution = 0.003203 mol NaOH\\n\\nHydrochloric acid and sodium hydroxide neutralize each other in a 1:1 mole ratio. This means that in order to have a complete neutralization, you need to mix equal numbers of moles of hydrochloric acid and of sodium hydroxide. In this case, you have more moles of sodium hydroxide than of hydrochloric acid, so the acid will act as a limiting reagent, i.e. it will be completely consumed before all the moles of sodium hydroxide will get the chance to react. This means that after the reaction takes place, your solution will contain 0 moles of hydrochloric acid and 0.003203 mol - 0.001431 mol = 0.001772 mol NaOH.\\n\\nAt this point, you should be able to say that the pH of the resulting solution must be >7 because you're left with a solution that contains a strong base. Sodium hydroxide dissociates completely in aqueous solution to produce sodium cations and hydroxide anions. This means that the resulting solution will contain 0.001772 moles of hydroxide anions.\\n\\nThe total volume of the resulting solution will be:\\n\\n9.54 mL + 22.88 mL = 32.42 mL\\n\\nThe molarity of the hydroxide anions will be:\\n\\n0.001772 mol / 32.42 mL * 1000 mL/L = 0.05466 mol/L\\n\\nNow, an aqueous solution at 25°C has:\\n\\npH + pOH = 14\\n\\nSince pOH = -log[OH-], you can say that:\\n\\npH = 14 + log[OH-]\\n\\nPlug in your value to find:\\n\\npH = 14 + log(0.05466) = 12.74\\n\\nThe answer is rounded to two decimal places, the number of sig figs you have for the concentrations of the two solutions.\\n####\\nThe pH of the resulting solution is 12.74.<|im_end|>\\n\",\n",
       " '__index_level_0__': 49038}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bda4a2-d297-4981-998a-85c2c66e2017",
   "metadata": {},
   "source": [
    "推理类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59aa524-40b7-4b0f-bcf5-859ac7e0471f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|im_start|>user\\nDetermine the surface area of the portion of the plane $2x + 3y + 6z = 9$ that lies in the first octant.<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let's see. I need to find the surface area of the part of the plane 2x + 3y + 6z = 9 that's in the first octant. Hmm, first octant means all x, y, z are non-negative, right? So I should probably figure out the region on the plane where x ≥ 0, y ≥ 0, z ≥ 0. Then find the area of that triangular-like surface.\\n\\nWait, planes in 3D can form triangles when intersected with the first octant, depending on where they cut the axes. Maybe I should find the intercepts first. Let me recall that to find intercepts, set the other variables to zero. \\n\\nFor the x-intercept, set y=0 and z=0. Then 2x = 9, so x = 9/2 = 4.5. \\n\\nFor the y-intercept, set x=0 and z=0. Then 3y = 9, so y = 3.\\n\\nFor the z-intercept, set x=0 and y=0. Then 6z = 9, so z = 1.5.\\n\\nSo the plane intersects the x-axis at (4.5, 0, 0), y-axis at (0, 3, 0), and z-axis at (0, 0, 1.5). So in the first octant, the portion of the plane is a triangle connecting these three points. Therefore, the surface area we need is the area of this triangle.\\n\\nBut wait, a triangle in 3D space. To find its area, can I use the formula involving vectors? Like, half the magnitude of the cross product of two sides. Let me confirm.\\n\\nFirst, let's find the vectors that form the triangle. Let's take two vectors from one vertex. Let's pick the intercept points: A(4.5, 0, 0), B(0, 3, 0), and C(0, 0, 1.5). Let me take vectors AB and AC.\\n\\nVector AB is B - A = (0 - 4.5, 3 - 0, 0 - 0) = (-4.5, 3, 0)\\n\\nVector AC is C - A = (0 - 4.5, 0 - 0, 1.5 - 0) = (-4.5, 0, 1.5)\\n\\nThen the cross product of AB and AC is:\\n\\n|i     j     k|\\n|-4.5  3     0|\\n|-4.5  0   1.5|\\n\\nCalculating determinant:\\n\\ni*(3*1.5 - 0*0) - j*(-4.5*1.5 - 0*(-4.5)) + k*(-4.5*0 - 3*(-4.5))\\n\\n= i*(4.5) - j*(-6.75) + k*(13.5)\\n\\nSo the cross product vector is (4.5, 6.75, 13.5)\\n\\nThe magnitude of this vector is sqrt(4.5^2 + 6.75^2 + 13.5^2)\\n\\nLet me compute each term:\\n\\n4.5 squared is 20.25\\n\\n6.75 squared is 45.5625 (since 6.75 = 27/4, (27/4)^2 = 729/16 = 45.5625)\\n\\n13.5 squared is 182.25 (since 13.5 = 27/2, squared is 729/4 = 182.25)\\n\\nSum: 20.25 + 45.5625 + 182.25 = 20.25 + 45.5625 is 65.8125 + 182.25 = 248.0625\\n\\nSquare root of 248.0625. Hmm, let's see. 15.5 squared is 240.25, 15.75 squared is 248.0625 because 15.75^2 = (15 + 0.75)^2 = 225 + 2*15*0.75 + 0.75^2 = 225 + 22.5 + 0.5625 = 248.0625. So sqrt(248.0625) = 15.75.\\n\\nSo the magnitude of the cross product is 15.75.\\n\\nThen the area is half of that: 15.75 / 2 = 7.875. Which is 63/8, since 7.875 = 7 and 7/8 = 63/8.\\n\\nAlternatively, 63/8 is 7.875. So the area is 63/8. Is that right? Let me check if I did all steps correctly.\\n\\nWait, but maybe there's another way to compute the area. For example, if I consider the plane equation and use the formula for the area of a triangle in 3D. Alternatively, using the intercepts to compute the area. But how?\\n\\nAlternatively, the area can be found by finding the area of the triangle formed by the intercepts. But in 3D, the triangle isn't in a coordinate plane, so we can't just do base*height/2. So the cross product method is the right approach here.\\n\\nAnother way: parameterize the surface and compute a double integral. But since it's a flat plane, the cross product method should suffice.\\n\\nWait, but let me verify once more. The vectors AB and AC are from point A. Their cross product gives a vector perpendicular to the plane of the triangle, and its magnitude equals the area of the parallelogram formed by AB and AC. Hence, half of that is the area of the triangle. So that should be correct.\\n\\nBut let me check the cross product calculation again step by step.\\n\\nGiven vectors AB = (-4.5, 3, 0) and AC = (-4.5, 0, 1.5).\\n\\nCross product AB × AC is:\\n\\n|i     j     k|\\n|-4.5  3     0|\\n|-4.5  0   1.5|\\n\\nCalculates to:\\n\\ni*(3*1.5 - 0*0) - j*(-4.5*1.5 - 0*(-4.5)) + k*(-4.5*0 - 3*(-4.5))\\n\\nCompute each component:\\n\\ni component: 3 * 1.5 = 4.5\\n\\nj component: - [(-4.5)(1.5) - 0] = - [ -6.75 ] = 6.75\\n\\nk component: (-4.5)(0) - 3*(-4.5) = 0 + 13.5 = 13.5\\n\\nSo the cross product is (4.5, 6.75, 13.5). Then the magnitude squared is 4.5^2 + 6.75^2 + 13.5^2.\\n\\n4.5 squared is 20.25\\n\\n6.75 squared is (6 + 0.75)^2 = 36 + 9 + 0.5625 = 45.5625\\n\\n13.5 squared is (13 + 0.5)^2 = 169 + 13 + 0.25 = 182.25\\n\\nTotal: 20.25 + 45.5625 + 182.25 = 248.0625\\n\\nSquare root of that is 15.75. Half of that is 7.875, which is 63/8. So that's 7.875 or 63/8. So 63 divided by 8 is 7.875. That seems correct.\\n\\nAlternatively, maybe I can check by another approach. For instance, converting the plane equation into intercept form and using the formula for the area.\\n\\nThe intercept form of a plane is x/a + y/b + z/c = 1, where a, b, c are the intercepts. In our problem, the equation is 2x + 3y + 6z = 9. So dividing both sides by 9:\\n\\n(2x)/9 + (3y)/9 + (6z)/9 = 1\\n\\nSimplify:\\n\\nx/(9/2) + y/3 + z/(3/2) = 1\\n\\nSo intercepts are a = 9/2, b = 3, c = 3/2. Which matches what we found earlier.\\n\\nNow, there's a formula for the area of the triangle formed by the intercepts in the first octant. But I'm not sure about that. Wait, perhaps the area can be calculated using the formula:\\n\\nArea = (1/2) * sqrt( (ab)^2 + (bc)^2 + (ac)^2 )\\n\\nWait, but that seems incorrect. Let me think. Alternatively, since the triangle is defined by three points in 3D, perhaps the area can be found by Heron's formula. But Heron's formula requires knowing the lengths of all sides. Let's try that.\\n\\nFirst, compute the lengths of the sides of the triangle. The triangle has vertices at A(4.5, 0, 0), B(0, 3, 0), and C(0, 0, 1.5).\\n\\nCompute AB length: distance between A and B.\\n\\nΔx = -4.5, Δy = 3, Δz = 0. So length AB = sqrt( (-4.5)^2 + 3^2 + 0 ) = sqrt(20.25 + 9) = sqrt(29.25) = sqrt(117/4) = (3√13)/2 ≈ 5.408.\\n\\nLength AC: distance between A and C. Δx = -4.5, Δy=0, Δz=1.5. So sqrt( (-4.5)^2 + 0 + (1.5)^2 ) = sqrt(20.25 + 2.25) = sqrt(22.5) = sqrt(45/2) = (3√10)/2 ≈ 4.743.\\n\\nLength BC: distance between B and C. Δx=0, Δy=-3, Δz=1.5. sqrt(0 + (-3)^2 + (1.5)^2 ) = sqrt(9 + 2.25) = sqrt(11.25) = sqrt(45/4) = (3√5)/2 ≈ 3.354.\\n\\nNow, using Heron's formula, first compute semi-perimeter s = (AB + AC + BC)/2.\\n\\ns = ( (3√13)/2 + (3√10)/2 + (3√5)/2 ) / 2 = [3(√13 + √10 + √5)/2 ] /2 = 3(√13 + √10 + √5)/4. Hmm, this seems complicated. Then area = sqrt( s(s - AB)(s - AC)(s - BC) )\\n\\nBut this would be messy. Alternatively, since I already know the area from the cross product method is 63/8, maybe I can compute this Heron's formula result numerically to check if they match.\\n\\nLet's compute AB, AC, BC numerically:\\n\\nAB ≈ 5.408, AC ≈ 4.743, BC ≈ 3.354.\\n\\ns = (5.408 + 4.743 + 3.354)/2 ≈ (13.505)/2 ≈ 6.7525.\\n\\nThen s - AB ≈ 6.7525 - 5.408 ≈ 1.3445\\n\\ns - AC ≈ 6.7525 - 4.743 ≈ 2.0095\\n\\ns - BC ≈ 6.7525 - 3.354 ≈ 3.3985\\n\\nArea ≈ sqrt(6.7525 * 1.3445 * 2.0095 * 3.3985)\\n\\nCompute the product inside sqrt:\\n\\nFirst multiply 6.7525 * 1.3445 ≈ 6.7525 * 1.3445 ≈ Let's compute 6 * 1.3445 = 8.067, 0.7525 * 1.3445 ≈ approximately 1.011, so total ≈ 9.078.\\n\\nThen multiply 2.0095 * 3.3985 ≈ 2 * 3.3985 = 6.797, 0.0095 * 3.3985 ≈ 0.0323, so total ≈ 6.8293.\\n\\nNow multiply 9.078 * 6.8293 ≈ let's see, 9 * 6.8293 ≈ 61.4637, 0.078 * 6.8293 ≈ 0.533, so total ≈ 61.9967.\\n\\nThen sqrt(61.9967) ≈ 7.874, which is approximately 7.875, matching the previous result. So that's a good check. Therefore, the area is indeed 63/8 or 7.875.\\n\\nAlternatively, another way to compute surface area is using the formula involving the double integral over the projection. Let me try that approach to verify.\\n\\nSince the surface is part of the plane in the first octant, I can parameterize it using x and y, and express z in terms of x and y. From 2x + 3y + 6z = 9, solving for z:\\n\\nz = (9 - 2x - 3y)/6\\n\\nSo the surface can be parameterized by x and y, where x and y are such that z ≥ 0. So 9 - 2x - 3y ≥ 0 → 2x + 3y ≤ 9. Also, x ≥ 0 and y ≥ 0. So the domain D in the xy-plane is the triangle with vertices at (0,0), (4.5,0), and (0,3). \\n\\nTo compute the surface area, the formula is the double integral over D of sqrt( (dz/dx)^2 + (dz/dy)^2 + 1 ) dA.\\n\\nCompute dz/dx and dz/dy:\\n\\ndz/dx = -2/6 = -1/3\\n\\ndz/dy = -3/6 = -1/2\\n\\nSo (dz/dx)^2 + (dz/dy)^2 + 1 = (1/9) + (1/4) + 1 = (4/36 + 9/36) + 1 = (13/36) + 1 = 49/36.\\n\\nTherefore, sqrt(49/36) = 7/6. \\n\\nTherefore, the surface area is the integral over D of 7/6 dA. Since 7/6 is constant, this is just 7/6 multiplied by the area of D.\\n\\nThe area of D is the area of the triangle in the xy-plane with vertices (0,0), (4.5,0), and (0,3). The base is 4.5, height is 3. So area = (1/2)*4.5*3 = (1/2)*13.5 = 6.75.\\n\\nTherefore, surface area = 7/6 * 6.75 = 7 * (6.75 / 6) = 7 * 1.125 = 7.875 = 63/8. \\n\\nSo same result. Thus, this confirms that the surface area is indeed 63/8.\\n\\nSo both methods give the same answer, which gives me confidence that 63/8 is correct.\\n\\nTherefore, the surface area is 63/8 square units.\\n\\n**Final Answer**\\nThe surface area is \\\\boxed{\\\\dfrac{63}{8}}.\\n</think>\\n\\nTo determine the surface area of the portion of the plane \\\\(2x + 3y + 6z = 9\\\\) that lies in the first octant, we start by finding the intercepts of the plane with the coordinate axes:\\n\\n1. **Find the intercepts:**\\n   - **x-intercept:** Set \\\\(y = 0\\\\) and \\\\(z = 0\\\\):\\n     \\\\[\\n     2x = 9 \\\\implies x = \\\\frac{9}{2} = 4.5\\n     \\\\]\\n   - **y-intercept:** Set \\\\(x = 0\\\\) and \\\\(z = 0\\\\):\\n     \\\\[\\n     3y = 9 \\\\implies y = 3\\n     \\\\]\\n   - **z-intercept:** Set \\\\(x = 0\\\\) and \\\\(y = 0\\\\):\\n     \\\\[\\n     6z = 9 \\\\implies z = \\\\frac{9}{6} = 1.5\\n     \\\\]\\n\\n   The intercepts are the points \\\\(A(4.5, 0, 0)\\\\), \\\\(B(0, 3, 0)\\\\), and \\\\(C(0, 0, 1.5)\\\\).\\n\\n2. **Determine the vectors \\\\(\\\\overrightarrow{AB}\\\\) and \\\\(\\\\overrightarrow{AC}\\\\):**\\n   - \\\\(\\\\overrightarrow{AB} = B - A = (-4.5, 3, 0)\\\\)\\n   - \\\\(\\\\overrightarrow{AC} = C - A = (-4.5, 0, 1.5)\\\\)\\n\\n3. **Calculate the cross product \\\\(\\\\overrightarrow{AB} \\\\times \\\\overrightarrow{AC}\\\\):**\\n   \\\\[\\n   \\\\overrightarrow{AB} \\\\times \\\\overrightarrow{AC} = \\\\begin{vmatrix}\\n   \\\\mathbf{i} & \\\\mathbf{j} & \\\\mathbf{k} \\\\\\\\\\n   -4.5 & 3 & 0 \\\\\\\\\\n   -4.5 & 0 & 1.5\\n   \\\\end{vmatrix}\\n   = \\\\mathbf{i}(3 \\\\cdot 1.5 - 0 \\\\cdot 0) - \\\\mathbf{j}(-4.5 \\\\cdot 1.5 - 0 \\\\cdot -4.5) + \\\\mathbf{k}(-4.5 \\\\cdot 0 - 3 \\\\cdot -4.5)\\n   = (4.5, 6.75, 13.5)\\n   \\\\]\\n\\n4. **Compute the magnitude of the cross product:**\\n   \\\\[\\n   \\\\|\\\\overrightarrow{AB} \\\\times \\\\overrightarrow{AC}\\\\| = \\\\sqrt{4.5^2 + 6.75^2 + 13.5^2} = \\\\sqrt{20.25 + 45.5625 + 182.25} = \\\\sqrt{248.0625} = 15.75\\n   \\\\]\\n\\n5. **Calculate the area of the triangle:**\\n   \\\\[\\n   \\\\text{Area} = \\\\frac{1}{2} \\\\|\\\\overrightarrow{AB} \\\\times \\\\overrightarrow{AC}\\\\| = \\\\frac{1}{2} \\\\times 15.75 = 7.875 = \\\\frac{63}{8}\\n   \\\\]\\n\\nThus, the surface area of the portion of the plane in the first octant is \\\\(\\\\boxed{\\\\frac{63}{8}}\\\\).<|im_end|>\\n\",\n",
       " '__index_level_0__': 18456}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacac6d5-1f1e-40db-ad89-22c2d0e3a8dd",
   "metadata": {},
   "source": [
    "其中text字段就是后续带入微调的字段。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a3066-7501-43a4-b257-9aae6098417b",
   "metadata": {},
   "source": [
    "- 数据集保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968ee63-2d58-4df8-be3d-11ba6c94813d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后即可将清洗好的数据集进行本地保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba772f-aced-4c14-9db3-cb3d8226a3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013598d6a586407eb8b8d25f8b94562f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/24065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_dataset.save_to_disk(\"cleaned_qwen3_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b68b1-7a52-4830-8fdb-e877cfb85c60",
   "metadata": {},
   "source": [
    "后续使用时即可使用如下代码进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d9c8b-3307-46d6-9b00-83d3fe0fba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "combined_dataset = load_from_disk(\"cleaned_qwen3_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac43ed8-1d5c-47e3-9f79-798f476ae1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1648c-fc7d-468b-8039-b194bab79eed",
   "metadata": {},
   "source": [
    "## 六、Qwen3推理能力高效微调流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc4e45-8064-4274-bc1f-f7d6f7a4b5b0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;准备完数据之后，即可开始进行微调。这里我们先进行少量数据微调测试，程序能够基本跑通后，我们再进行大规模数据集微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bae433-bc95-4b55-a063-55f845a4c382",
   "metadata": {},
   "source": [
    "### 1. Unsloth微调流程实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d604b-85ab-46c2-b44a-4e0038ae996e",
   "metadata": {},
   "source": [
    "#### Step 1. 进行LoRA参数注入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e303a-13e3-4981-9da0-c5c7f099a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,               # 秩 (Rank)：选择大于 0 的整数。建议值为 8, 16, 32, 64, 128\n",
    "    target_modules = [    # 目标模块：LoRA 权重将应用到模型中的哪些层\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 32,      # 缩放系数：最佳实践是选择 alpha = r 或者 r * 2\n",
    "    lora_dropout = 0,     # Dropout 丢弃率：支持任何数值，但设为 0 是经过 Unsloth 特别优化的\n",
    "    bias = \"none\",        # 偏置项：支持任何选项，但设为 \"none\" 是经过优化的\n",
    "    \n",
    "    # [新特性] \"unsloth\" 模式可减少 30% 显存占用，并支持 2 倍大的 Batch Size！\n",
    "    use_gradient_checkpointing = \"unsloth\", # 面对长文本时，开启 True 或 \"unsloth\" 可节省显存\n",
    "    \n",
    "    random_state = 3407,  # 随机种子：固定随机性以便复现实验结果\n",
    "    use_rslora = False,   # 是否使用秩稳定 LoRA (Rank Stabilized LoRA)\n",
    "    loftq_config = None,  # 是否使用 LoftQ（量化初始权重的一种方法）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d867c-db2c-4bfe-8247-bad3781d3642",
   "metadata": {},
   "source": [
    "#### Step 2. 设置微调参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec296a3-9206-46b2-b4ca-63149dcf468c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None,      # 如果有验证集，可以在这里设置\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",       # 指定数据集中包含训练文本的字段名\n",
    "        per_device_train_batch_size = 2,  # 每个设备（GPU）的训练批次大小\n",
    "        gradient_accumulation_steps = 4,  # 梯度累积步数：累积 4 步更新一次参数，模拟更大的 Batch Size\n",
    "        warmup_steps = 5,                 # 预热步数：前 5 步学习率缓慢上升，防止训练初期不稳定\n",
    "        \n",
    "        # num_train_epochs = 1,           # 完整训练轮数：训练完整个数据集一遍\n",
    "        max_steps = 5,                    # 最大训练步数：仅用于快速测试，正式训练请注释掉此行并开启 epoch\n",
    "        \n",
    "        learning_rate = 2e-4,             # 学习率：LoRA 常用 2e-4；长文本或大规模训练建议降至 2e-5\n",
    "        logging_steps = 1,                # 日志记录频率：每步都打印训练状态（如 Loss 值）\n",
    "        optim = \"adamw_8bit\",             # 优化器：使用 8-bit AdamW，显著节省显存空间\n",
    "        weight_decay = 0.01,              # 权重衰减：防止过拟合的正则化手段\n",
    "        lr_scheduler_type = \"linear\",     # 学习率调度策略：线性下降\n",
    "        seed = 3407,                      # 随机种子：固定实验结果，方便复现\n",
    "        report_to = \"wandb\",              # 实验监控：同步到 Weights & Biases 平台，不需要可设为 \"none\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8075f2-7b81-477d-9e43-5cc227a36e71",
   "metadata": {},
   "source": [
    "其中`SFTTrainer`：一个专门为指令微调设计的训练器，封装了 Hugging Face 的 `Trainer`，而`SFTConfig`：配置训练参数的专用类，功能类似 `TrainingArguments`。而SFTConfig核心参数解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78da58-809d-4167-aba4-dec028c4bf9d",
   "metadata": {},
   "source": [
    "| 参数名                             | 含义                                              |\n",
    "| ------------------------------- | ----------------------------------------------- |\n",
    "| `dataset_text_field=\"text\"`     | 数据集中用于训练的字段名称，如 `text` 或 `prompt`               |\n",
    "| `per_device_train_batch_size=2` | 每张 GPU 上的 batch size 是 2                        |\n",
    "| `gradient_accumulation_steps=4` | 梯度累计 4 次后才进行一次反向传播（等效于总 batch size = 2 × 4 = 8） |\n",
    "| `warmup_steps=5`                | 前 5 步进行 warmup（缓慢提升学习率）                         |\n",
    "| `max_steps=30`                  | 最多训练 30 步（适合调试或快速实验）                            |\n",
    "| `learning_rate=2e-4`            | 初始学习率（短训练可用较高值）                                 |\n",
    "| `logging_steps=1`               | 每训练 1 步就打印一次日志                                  |\n",
    "| `optim=\"adamw_8bit\"`            | 使用 8-bit AdamW 优化器（节省内存，Unsloth 支持）             |\n",
    "| `weight_decay=0.01`             | 权重衰减，用于防止过拟合                                    |\n",
    "| `lr_scheduler_type=\"linear\"`    | 线性学习率调度器（从高到低线性下降）                              |\n",
    "| `seed=3407`                     | 固定随机种子，确保结果可复现                                  |\n",
    "| `report_to=\"none\"`              | 不使用 WandB 或 TensorBoard 等日志平台（可改为 `\"wandb\"`）    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f95f88-870a-4eef-8d80-bcad5c198ebc",
   "metadata": {},
   "source": [
    "此时基本训练过程为：\n",
    "1. 从 `combined_dataset` 中取出一批样本（2 条）\n",
    "2. 重复上面过程 4 次（`gradient_accumulation_steps=4`）\n",
    "3. 将累计的梯度用于更新模型一次参数（等效于一次大 batch 更新）\n",
    "4. 重复上述过程，直到 `max_steps=30` 停止"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069a7ff-b33f-4594-89b6-16fc3f855ccd",
   "metadata": {},
   "source": [
    "此时显存占用如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ffee7-6cea-4541-a750-9d2a2e6617fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090 D. Max memory = 23.516 GB.\n",
      "19.152 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b78d4-3c7f-4d13-8496-c97932999d83",
   "metadata": {},
   "source": [
    "#### Step 3. 【可选】设置wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6beeb0-5cc7-45bd-91fd-d41b9cbb32e4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来可继续设置wandb用于进行模型训练过程关键信息记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d95568-b6ba-476c-bd94-164290003017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c05fa-9cc6-45ef-9b2a-ee0c3a4a841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Qwen3_unsloth.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa135d9-f429-41cb-9626-5604d15c221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()  # 会自动使用环境变量中的key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56956b2-614e-4c16-bfde-5dc29598f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Qwen3高效微调（下）.ipynb.\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: No netrc file found, creating one.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "wandb: Currently logged in as: 2323365771 (2323365771-ff) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"4b62572b8426ff59ce46fec93a00eb1feecc026a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507db33-39e0-4720-be9e-57efac339082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "wandb: Currently logged in as: 2737609908 (2737609908-northwestern-polytechnical-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"c27c8c652e21d1e11e4e3889ac4ce9d0d1f01e7d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d1929-2cb5-4fe8-accb-eef2b2fa8cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/pandagpt_data/qwen/wandb/run-20260106_001204-rd0euuea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset/runs/rd0euuea' target=\"_blank\">grateful-gorge-1</a></strong> to <a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset' target=\"_blank\">https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset/runs/rd0euuea' target=\"_blank\">https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset/runs/rd0euuea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Fine-tune-Qwen-8B-4bit on Combined Dataset', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2c36d-0713-4bf4-b739-92d1b5f6746b",
   "metadata": {},
   "source": [
    "#### Step 4. 微调执行流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cb8ab-a0d2-48ea-a48e-681e36fd84c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一切准备就绪后，接下来即可开始进行微调。由于本次微调总共只运行30个step，整个过程并不会很长，实际执行过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a7c56-f72f-49cd-88ae-710ad6f18d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,065 | Num Epochs = 1 | Total steps = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 87,293,952 of 8,278,029,312 (1.05% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.383200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/global_step</td><td>▁▃▅▆██</td></tr><tr><td>train/grad_norm</td><td>▄██▁▆</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▅▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>8404579976785920.0</td></tr><tr><td>train/epoch</td><td>0.00166</td></tr><tr><td>train/global_step</td><td>5</td></tr><tr><td>train/grad_norm</td><td>0.04633</td></tr><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/loss</td><td>0.3832</td></tr><tr><td>train_loss</td><td>0.40168</td></tr><tr><td>train_runtime</td><td>83.4665</td></tr><tr><td>train_samples_per_second</td><td>0.479</td></tr><tr><td>train_steps_per_second</td><td>0.06</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-gorge-1</strong> at: <a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset/runs/rd0euuea' target=\"_blank\">https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset/runs/rd0euuea</a><br> View project at: <a href='https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset' target=\"_blank\">https://wandb.ai/2737609908-northwestern-polytechnical-university/Fine-tune-Qwen-8B-4bit%20on%20Combined%20Dataset</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260106_001204-rd0euuea/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c60c3-eade-436f-bb10-1744cf68f871",
   "metadata": {},
   "source": [
    "- 微调期间显存占用检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd51095-6684-43b4-a976-220c88925095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439.9436 seconds used for training.\n",
      "7.33 minutes used for training.\n",
      "Peak reserved memory = 19.152 GB.\n",
      "Peak reserved memory for training = 11.045 GB.\n",
      "Peak reserved memory % of max memory = 81.442 %.\n",
      "Peak reserved memory for training % of max memory = 46.968 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32000ca2-830f-47a7-a83b-f645b2718202",
   "metadata": {},
   "source": [
    "- 查看wandb记录结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1de23f-d135-45e8-8f13-895afca67fb3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250507222022165.png\" alt=\"image-20250507222022165\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f9c5a-7daf-4c59-bc2c-405d44488945",
   "metadata": {},
   "source": [
    "能够看到loss波动下降，整个训练过程属于正常情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c232884-4829-4b5f-817f-8a75be4d8539",
   "metadata": {},
   "source": [
    "#### Step 5. 模型对话测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251909b-aea1-4d4e-9c62-cdf8ba4fa9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 4096, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (7-33): 27 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (34): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (35): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938fd5d-bcdd-4c6e-bd41-ceed1fc79751",
   "metadata": {},
   "source": [
    "而当我们完成训练后，即可尝试进行模型对话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1810a-70ef-44c0-87bf-1baca6b62d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation \\((x + 2)^3 = 0\\), we start by recognizing that the left-hand side is a cube of a binomial. A cube of a binomial is zero if and only if the binomial itself is zero. Therefore, we set the binomial equal to zero:\n",
      "\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Next, we solve for \\(x\\) by isolating it on one side of the equation. We do this by subtracting 2 from both sides:\n",
      "\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "Thus, the solution to the equation \\((x + 2)^3 = 0\\) is:\n",
      "\n",
      "\\[\n",
      "\\boxed{-2}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^3 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc3719-9e67-449d-a9ef-085da3db8339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. I need to solve the equation (x + 2)^2 = 0. Hmm, how do I approach this? Well, when you have something squared equals zero, that means the inside of the square has to be zero because any real number squared is non-negative, and the only way for it to be zero is if the number itself is zero. So, if (x + 2)^2 = 0, then x + 2 must equal zero. That makes sense.\n",
      "\n",
      "So, let's write that out. If (x + 2)^2 = 0, then taking the square root of both sides should give me x + 2 = 0. Wait, but when you take the square root of both sides, you have to consider both the positive and negative roots, right? But in this case, the square of a real number is zero only if the number itself is zero. So maybe there's only one solution here. Let me check that.\n",
      "\n",
      "Alternatively, maybe I can expand the left side to see if that helps. Let's try expanding (x + 2)^2. That would be x^2 + 4x + 4. So the equation becomes x^2 + 4x + 4 = 0. Now, if I factor this quadratic equation, I can see if it factors nicely. Let's see, x^2 + 4x + 4. The factors of 4 that add up to 4 are 2 and 2. So, it factors as (x + 2)(x + 2) = 0, which is the same as (x + 2)^2 = 0. So that's the same as the original equation. So, this confirms that the quadratic is a perfect square and has a repeated root.\n",
      "\n",
      "In quadratic equations, the Fundamental Theorem of Algebra states that a quadratic equation has two roots (counting multiplicity). In this case, both roots are the same, so the solution x = -2 is a double root. Therefore, the only solution is x = -2, but it's repeated twice.\n",
      "\n",
      "Alternatively, I can use the quadratic formula. The quadratic formula is x = [-b ± sqrt(b^2 - 4ac)] / (2a). In this equation, a = 1, b = 4, and c = 4. Plugging those into the formula: x = [-4 ± sqrt(16 - 16)] / 2. The discriminant (the part under the square root) is 16 - 16 = 0. So, sqrt(0) = 0. Then, x = [-4 ± 0]/2, which simplifies to x = -4/2 = -2. So, again, the only solution is x = -2, but since the discriminant is zero, it's a repeated root.\n",
      "\n",
      "So, putting it all together, the equation (x + 2)^2 = 0 has a repeated root at x = -2. Therefore, the solution is x = -2, but it's a double root. Depending on how the problem is asked, sometimes you just list the solution once, and sometimes you mention that it's a double root. But in this case, since it's a quadratic equation, the answer is x = -2, and it's the only solution, even though it's repeated.\n",
      "\n",
      "Wait, but when you solve equations, you usually list each root as many times as it's repeated. But in standard practice, unless specified, you just list the solution once. So, the answer is x = -2. But maybe the problem expects both roots? Hmm, but in this case, since it's a square, it's a repeated root. So, in the context of solving equations, you can just state x = -2 as the solution, even though it's a double root. So, I think the answer is x = -2.\n",
      "</think>\n",
      "\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we start by recognizing that the square of a real number is zero only if the number itself is zero. Therefore, we can set the expression inside the square equal to zero:\n",
      "\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Solving for \\(x\\), we subtract 2 from both sides:\n",
      "\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "This solution is a double root because the quadratic equation \\((x + 2)^2 = 0\\) is a perfect square, and the discriminant of the quadratic equation is zero. This confirms that the solution \\(x = -2\\) is repeated twice.\n",
      "\n",
      "Thus, the solution to the equation \\((x + 2)^2 = 0\\) is:\n",
      "\n",
      "\\[\n",
      "\\boxed{-2}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0c505-13e6-43a6-8cbf-e45a2d25910b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to find the surface area of the portion of the plane 2x + 3y + 6z = 9 that's in the first octant. Hmm, first octant means x, y, z are all non-negative, right? So the plane intersects the x, y, and z axes at certain points, and I need to figure out the area of the triangle formed by those intercepts. Wait, is that right? Because in the first octant, the plane should form a triangular region, and the surface area would be the area of that triangle. But I need to confirm that.\n",
      "\n",
      "First, let me find the intercepts. For the x-intercept, set y = 0 and z = 0. Then 2x = 9, so x = 9/2. Similarly, y-intercept: set x = 0 and z = 0, so 3y = 9, y = 3. Z-intercept: set x = 0 and y = 0, so 6z = 9, z = 3/2. So the plane intersects the axes at (9/2, 0, 0), (0, 3, 0), and (0, 0, 3/2). These three points form a triangle in the first octant. So the surface area is the area of this triangle. But wait, is that the case? Because the surface area of a plane in the first octant would actually be the area of this triangle. But I need to be careful here. Let me think.\n",
      "\n",
      "Alternatively, maybe I should parameterize the surface and compute the surface integral. Let me try both approaches and see which one works.\n",
      "\n",
      "First approach: Since the plane intersects the axes at those points, the triangle formed by these intercepts is a right triangle? Wait, no. The intercepts are (9/2, 0, 0), (0, 3, 0), and (0, 0, 3/2). These three points are not aligned in a right triangle, because the triangle is in 3D space. So the triangle is not a right triangle. Therefore, the area is not simply (1/2)*base*height. Hmm, so maybe I need to use vectors to compute the area.\n",
      "\n",
      "Alternatively, maybe I can parameterize the plane and compute the surface integral. Let me recall that the formula for the surface area of a graph z = f(x, y) is the double integral over the region R of sqrt(1 + (df/dx)^2 + (df/dy)^2) dA. But in this case, the plane is given by 2x + 3y + 6z = 9, so solving for z gives z = (9 - 2x - 3y)/6. Then, the surface area would be the double integral over the region in the xy-plane where x ≥ 0, y ≥ 0, and (9 - 2x - 3y)/6 ≥ 0. So the region R is the set of (x, y) such that x ≥ 0, y ≥ 0, 2x + 3y ≤ 9. Then, compute the integral over this region of sqrt(1 + (df/dx)^2 + (df/dy)^2) dA.\n",
      "\n",
      "Alternatively, since the plane is flat, the surface area can be found by finding the area of the triangle formed by the intercepts. But how do I compute the area of a triangle in 3D space given three points?\n",
      "\n",
      "Yes, the area can be found using the cross product of two vectors that form the sides of the triangle. Let me try that.\n",
      "\n",
      "Let me denote the three points as A(9/2, 0, 0), B(0, 3, 0), and C(0, 0, 3/2). Then vectors AB and AC can be found.\n",
      "\n",
      "Vector AB = B - A = (0 - 9/2, 3 - 0, 0 - 0) = (-9/2, 3, 0).\n",
      "\n",
      "Vector AC = C - A = (0 - 9/2, 0 - 0, 3/2 - 0) = (-9/2, 0, 3/2).\n",
      "\n",
      "Then the cross product AB × AC is determinant of the matrix:\n",
      "\n",
      "i   j   k\n",
      "-9/2 3   0\n",
      "-9/2 0  3/2\n",
      "\n",
      "Calculating the determinant:\n",
      "\n",
      "i*(3*(3/2) - 0*0) - j*(-9/2*(3/2) - 0*(-9/2)) + k*(-9/2*0 - 3*(-9/2))\n",
      "\n",
      "Compute each component:\n",
      "\n",
      "i*(9/2) - j*(-27/4) + k*(27/2)\n",
      "\n",
      "So the cross product is (9/2, 27/4, 27/2)\n",
      "\n",
      "The magnitude of this vector is sqrt( (9/2)^2 + (27/4)^2 + (27/2)^2 )\n",
      "\n",
      "Compute each term:\n",
      "\n",
      "(9/2)^2 = 81/4\n",
      "\n",
      "(27/4)^2 = 729/16\n",
      "\n",
      "(27/2)^2 = 729/4\n",
      "\n",
      "Convert to common denominator:\n",
      "\n",
      "81/4 = 324/16\n",
      "\n",
      "729/16 = 729/16\n",
      "\n",
      "729/4 = 2916/16\n",
      "\n",
      "Sum: 324 + 729 + 2916 = 3969 over 16.\n",
      "\n",
      "So sqrt(3969/16) = 63/4\n",
      "\n",
      "Therefore, the magnitude of the cross product is 63/4\n",
      "\n",
      "The area of the triangle is half of this, so (63/4)/2 = 63/8\n",
      "\n",
      "Wait, that seems straightforward. So the surface area is 63/8. Let me check if this matches with the other method.\n",
      "\n",
      "Alternatively, using the surface integral approach. Let's see.\n",
      "\n",
      "First, express z as (9 - 2x - 3y)/6. Then, the partial derivatives:\n",
      "\n",
      "dz/dx = -2/6 = -1/3\n",
      "\n",
      "dz/dy = -3/6 = -1/2\n",
      "\n",
      "Then, the integrand becomes sqrt(1 + (-1/3)^2 + (-1/2)^2) = sqrt(1 + 1/9 + 1/4) = sqrt( (36/36) + (4/36) + (9/36) ) = sqrt(49/36) = 7/6\n",
      "\n",
      "So the surface area is the integral over the region R (where 2x + 3y ≤9, x ≥0, y ≥0) of 7/6 dA.\n",
      "\n",
      "The region R is a triangle with vertices at (0,0), (9/2, 0), and (0, 3). So the area of the region R is (base * height)/2. The base along the x-axis is 9/2, the height along the y-axis is 3. So area of R is (9/2 * 3)/2 = 27/4\n",
      "\n",
      "Therefore, the surface area is 7/6 * 27/4 = (7*27)/(6*4) = 189/24 = 63/8\n",
      "\n",
      "Yes, same result. So both methods give 63/8. Therefore, the answer should be 63/8.\n",
      "\n",
      "Wait, just to confirm, the cross product method gave 63/8 and the surface integral method also gave 63/8. So that seems correct. Therefore, the surface area is 63/8.\n",
      "\n",
      "But let me check the cross product calculation again because that's a bit error-prone.\n",
      "\n",
      "Vectors AB and AC:\n",
      "\n",
      "AB = (-9/2, 3, 0)\n",
      "\n",
      "AC = (-9/2, 0, 3/2)\n",
      "\n",
      "Cross product:\n",
      "\n",
      "i*(3*(3/2) - 0*0) = i*(9/2)\n",
      "\n",
      "-j*( (-9/2)*(3/2) - 0*(-9/2) ) = -j*(-27/4) = 27/4 j\n",
      "\n",
      "k*( (-9/2)*0 - 3*(-9/2) ) = k*(27/2)\n",
      "\n",
      "So cross product vector is (9/2, 27/4, 27/2)\n",
      "\n",
      "Then magnitude squared is (9/2)^2 + (27/4)^2 + (27/2)^2\n",
      "\n",
      "Compute each term:\n",
      "\n",
      "(9/2)^2 = 81/4 = 324/16\n",
      "\n",
      "(27/4)^2 = 729/16\n",
      "\n",
      "(27/2)^2 = 729/4 = 2916/16\n",
      "\n",
      "Sum: 324 + 729 + 2916 = 3969, over 16. So sqrt(3969/16) = 63/4. Then half of that is 63/8. Correct.\n",
      "\n",
      "Yes, so the cross product method is correct.\n",
      "\n",
      "Therefore, the answer is 63/8.\n",
      "\n",
      "**Final Answer**\n",
      "The surface area is \\boxed{\\dfrac{63}{8}}.\n",
      "</think>\n",
      "\n",
      "To determine the surface area of the portion of the plane \\(2x + 3y + 6z = 9\\) that lies in the first octant, we start by finding the intercepts of the plane with the coordinate axes.\n",
      "\n",
      "1. **Intercepts:**\n",
      "   - **x-intercept:** Set \\(y = 0\\) and \\(z = 0\\):\n",
      "     \\[\n",
      "     2x = 9 \\implies x = \\frac{9}{2}\n",
      "     \\]\n",
      "     So, the x-intercept is \\(\\left(\\frac{9}{2}, 0, 0\\right)\\).\n",
      "   - **y-intercept:** Set \\(x = 0\\) and \\(z = 0\\):\n",
      "     \\[\n",
      "     3y = 9 \\implies y = 3\n",
      "     \\]\n",
      "     So, the y-intercept is \\((0, 3, 0)\\).\n",
      "   - **z-intercept:** Set \\(x = 0\\) and \\(y = 0\\):\n",
      "     \\[\n",
      "     6z = 9 \\implies z = \\frac{3}{2}\n",
      "     \\]\n",
      "     So, the z-intercept is \\((0, 0, \\frac{3}{2})\\).\n",
      "\n",
      "2. **Vectors Forming the Triangle:**\n",
      "   - Vectors from the origin to the intercepts:\n",
      "     \\[\n",
      "     \\vec{A} = \\left(\\frac{9}{2}, 0, 0\\right), \\quad \\vec{B} = (0, 3, 0), \\quad \\vec{C} = \\left(0, 0, \\frac{3}{2}\\right)\n",
      "     \\]\n",
      "   - Vectors \\(\\vec{AB}\\) and \\(\\vec{AC}\\):\n",
      "     \\[\n",
      "     \\vec{AB} = \\vec{B} - \\vec{A} = \\left(-\\frac{9}{2}, 3, 0\\right)\n",
      "     \\]\n",
      "     \\[\n",
      "     \\vec{AC} = \\vec{C} - \\vec{A} = \\left(-\\frac{9}{2}, 0, \\frac{3}{2}\\right)\n",
      "     \\]\n",
      "\n",
      "3. **Cross Product of Vectors \\(\\vec{AB}\\) and \\(\\vec{AC}\\):**\n",
      "   \\[\n",
      "   \\vec{AB} \\times \\vec{AC} = \\begin{vmatrix}\n",
      "   \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
      "   -\\frac{9}{2} & 3 & 0 \\\\\n",
      "   -\\frac{9}{2} & 0 & \\frac{3}{2}\n",
      "   \\end{vmatrix}\n",
      "   \\]\n",
      "   \\[\n",
      "   = \\mathbf{i} \\left(3 \\cdot \\frac{3}{2} - 0 \\cdot 0\\right) - \\mathbf{j} \\left(-\\frac{9}{2} \\cdot \\frac{3}{2} - 0 \\cdot -\\frac{9}{2}\\right) + "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[189]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m text = tokenizer.apply_chat_template(\n\u001b[32m      5\u001b[39m     messages,\n\u001b[32m      6\u001b[39m     tokenize = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      7\u001b[39m     add_generation_prompt = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[32m      8\u001b[39m     enable_thinking = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Disable thinking\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20488\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Increase for longer outputs!\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# For thinking\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTextStreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/peft/peft_model.py:2048\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   2047\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m2048\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2049\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2050\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/models/llama.py:2019\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2014\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   2015\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2016\u001b[39m     _get_inference_mode_context_manager(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m   2017\u001b[39m     torch.autocast(device_type = DEVICE_TYPE_TORCH, dtype = dtype),\n\u001b[32m   2018\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2019\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   2023\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   2024\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[32m   2026\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m restore_training_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/models/llama.py:1270\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_CausalLM_fast_forward\u001b[39m(\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1253\u001b[39m     input_ids: torch.LongTensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1267\u001b[39m     **kwargs,\n\u001b[32m   1268\u001b[39m ) -> Union[Tuple, CausalLMOutputWithPast]:\n\u001b[32m   1269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m         outputs = \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1279\u001b[39m         causal_mask = (\n\u001b[32m   1280\u001b[39m             xformers.attn_bias.LowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1281\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/models/llama.py:1219\u001b[39m, in \u001b[36m_LlamaModel_fast_forward_inference.<locals>.LlamaModel_fast_forward_inference_custom\u001b[39m\u001b[34m(self, input_ids, past_key_values, position_ids, attention_mask, **kwargs)\u001b[39m\n\u001b[32m   1211\u001b[39m residual.copy_(X)  \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[32m   1212\u001b[39m X = fast_rms_layernorm_inference(\n\u001b[32m   1213\u001b[39m     decoder_layer.post_attention_layernorm,\n\u001b[32m   1214\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1217\u001b[39m     variance = variance,\n\u001b[32m   1218\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m X = \u001b[43mmlp_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_gates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp_up\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_ups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1225\u001b[39m X += residual\n\u001b[32m   1227\u001b[39m next_decoder_cache.append(present_key_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/models/llama.py:475\u001b[39m, in \u001b[36mfast_swiglu_inference\u001b[39m\u001b[34m(self, X, temp_gate, temp_up, gate_multiplier, down_multiplier)\u001b[39m\n\u001b[32m    472\u001b[39m gate *= up\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# X = self.down_proj(gate)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m down = \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mup\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mhd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m down_multiplier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    478\u001b[39m     down *= down_multiplier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/kernels/utils.py:983\u001b[39m, in \u001b[36mfast_linear_forward\u001b[39m\u001b[34m(proj, X, temp_lora, out)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bsz == \u001b[32m1\u001b[39m:\n\u001b[32m    982\u001b[39m     out = out.view(out_dim)\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     temp_lora = \u001b[43mtorch_mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fast_lora\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_lora\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Determine the surface area of the portion of the plane $2x + 3y + 6z = 9$ that lies in the first octant.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a86af-9d1f-4da5-8fe4-ec1787cd0255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6906ac-0d67-409f-bda8-6884a39bc18d",
   "metadata": {},
   "source": [
    "#### Step 6. 模型大规模微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f7a36-c189-4781-b7c0-dbfa0f724f34",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续深入进行训练，此处考虑训练完一整个epoch，总共约8小时左右，训练流程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accec937-f621-4dc5-9fb2-99705417ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=176): 100%|██████████| 24065/24065 [07:06<00:00, 56.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48b48c-2934-4edb-be8a-0b947e4c1ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,065 | Num Epochs = 1 | Total steps = 3,008\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 268,435,456/32,000,000,000 (0.84% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='3008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/3008 3:12:42 < 4:57:22, 0.10 it/s, Epoch 0.39/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.339700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.400100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.420700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>0.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>0.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>0.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>0.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>0.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>0.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>0.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>0.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>0.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>0.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>0.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>0.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>0.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.368700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>0.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>0.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>0.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>0.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>0.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>0.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>0.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>0.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>0.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>0.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>0.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>0.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>0.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>0.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>0.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>0.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>0.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.329300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.394300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>0.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>0.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>0.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>0.456400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>0.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>0.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>0.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>0.339700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>0.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>0.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>0.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>0.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>0.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>0.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>0.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.371500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>0.380300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>0.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>0.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>0.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>0.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>0.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>0.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>0.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>0.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>0.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>0.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>0.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>0.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>0.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>0.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>0.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>0.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>0.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>0.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>0.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>0.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>0.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>0.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>0.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>0.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>0.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>0.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>0.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>0.368700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>0.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>0.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>0.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>0.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>0.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>0.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>0.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>0.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>0.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>0.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>0.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>0.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>0.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>0.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>0.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>0.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>0.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>0.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>0.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>0.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>0.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>0.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>0.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>0.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>0.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>0.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>0.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>0.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>0.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>0.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>0.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>0.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>0.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>0.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>0.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>0.457900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>0.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.443500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>0.329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>0.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>0.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>0.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>0.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>0.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>0.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.335900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>0.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>0.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>0.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>0.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>0.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>0.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>0.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>0.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>0.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>0.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>0.334900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>0.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>0.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>0.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>0.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>0.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>0.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>0.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>0.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>0.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>0.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>0.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>0.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>0.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>0.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>0.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>0.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>0.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>0.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>0.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>0.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>0.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>0.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>0.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>0.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>0.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>0.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>0.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>0.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>0.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>0.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>0.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>0.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>0.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>0.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>0.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>0.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>0.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>0.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>0.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>0.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1067</td>\n",
       "      <td>0.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1073</td>\n",
       "      <td>0.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1074</td>\n",
       "      <td>0.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>0.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1077</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>0.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1079</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>0.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1082</td>\n",
       "      <td>0.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>0.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>0.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1087</td>\n",
       "      <td>0.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>0.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>0.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>0.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>0.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>0.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1108</td>\n",
       "      <td>0.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1111</td>\n",
       "      <td>0.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>0.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1118</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1119</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1123</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1124</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1129</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1131</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1132</td>\n",
       "      <td>0.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>0.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>0.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>0.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137</td>\n",
       "      <td>0.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>0.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>0.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1142</td>\n",
       "      <td>0.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>0.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>0.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>0.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>0.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1151</td>\n",
       "      <td>0.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>0.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153</td>\n",
       "      <td>0.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1154</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>0.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1156</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1157</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1158</td>\n",
       "      <td>0.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1159</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1163</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1166</td>\n",
       "      <td>0.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167</td>\n",
       "      <td>0.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>0.363900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>0.293400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8682fe4-a68f-4c28-9b39-df42423ff328",
   "metadata": {},
   "source": [
    "此时训练完成后再进行对话，能明显看出模型当前数学性能有所提升，具体问答效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee5159-694c-47d0-bfa9-dc7b53de4d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to find the surface area of the portion of the plane 2x + 3y + 6z = 9 that lies in the first octant. Hmm, first octant means x, y, z are all non-negative, right? So the plane intersects the coordinate axes at some points, and the region we're interested in is the triangle formed by those intercepts. But wait, surface area of a plane... isn't that just the area of that triangular region? But maybe I need to confirm if it's a triangle. Let me think.\n",
      "\n",
      "First, let's find the intercepts. For the x-intercept, set y=0 and z=0. So 2x = 9 → x = 9/2. Similarly, y-intercept: set x=0, z=0. 3y = 9 → y = 3. Z-intercept: set x=0, y=0. 6z = 9 → z = 9/6 = 3/2. So the intercepts are at (9/2, 0, 0), (0, 3, 0), and (0, 0, 3/2). So the portion of the plane in the first octant is a triangle connecting these three points. Therefore, the surface area should be the area of this triangle. But wait, the problem says \"surface area of the portion of the plane,\" which is a flat surface, so yes, it's just the area of that triangle. But how do I calculate that?\n",
      "\n",
      "Alternatively, maybe I need to use a surface integral. The formula for the surface area of a graph z = f(x,y) is the double integral over the region D of sqrt( (f_x)^2 + (f_y)^2 + 1 ) dA. But here, the plane isn't given as z = f(x,y); it's given as 2x + 3y + 6z = 9. Let me solve for z to express it in terms of x and y. So 6z = 9 - 2x - 3y → z = (9 - 2x - 3y)/6. Then f(x,y) = (9 - 2x - 3y)/6. Then the partial derivatives would be f_x = -2/6 = -1/3, and f_y = -3/6 = -1/2. Then the integrand becomes sqrt( (-1/3)^2 + (-1/2)^2 + 1 ). Let's compute that: (-1/3)^2 is 1/9, (-1/2)^2 is 1/4, so 1/9 + 1/4 + 1. Let's convert to common denominator, which is 36. 1/9 is 4/36, 1/4 is 9/36, so 4/36 + 9/36 = 13/36. Then add 1, which is 36/36. So total is 49/36. The square root of 49/36 is 7/6. So the integrand is 7/6. Therefore, the surface area is the double integral over D of 7/6 dA, which is 7/6 times the area of D. So if I can find the area of D, which is the projection of the triangle onto the xy-plane, then multiply by 7/6 to get the surface area.\n",
      "\n",
      "Wait, but D is the region in the xy-plane where the plane intersects the first octant. The intercepts in the xy-plane would be when z=0. So when z=0, 2x + 3y = 9. So the projection D is the triangle in the xy-plane with vertices at (9/2, 0, 0), (0, 3, 0), and (0, 0, 0). Wait, no, when z=0, the intercepts are (9/2, 0, 0) and (0, 3, 0). So the projection D is the triangle in the xy-plane bounded by x ≥ 0, y ≥ 0, and 2x + 3y ≤ 9. So the area of D is the area of that triangle. Let's compute that. The base along the x-axis is from 0 to 9/2, and the base along the y-axis is from 0 to 3. The area of the triangle is (base * height)/2. But since it's a right triangle? Wait, no, in the xy-plane, the line 2x + 3y = 9 intersects the axes at (9/2, 0) and (0, 3), so it's a straight line, and the triangle is between these two points and the origin. So the area is (base * height)/2. The base can be considered along the x-axis, which is 9/2, and the height along the y-axis is 3. So area is (9/2 * 3)/2 = (27/2)/2 = 27/4. Wait, but actually, the area of a triangle with vertices at (0,0), (a,0), and (0,b) is (a*b)/2. So here, a is 9/2 and b is 3. So area is (9/2 * 3)/2 = (27/2)/2 = 27/4. So the area of D is 27/4. Then the surface area would be 7/6 * 27/4 = (7 * 27)/(6 * 4) = (189)/24 = 63/8. Let me check that calculation again. 27 divided by 6 is 4.5, 4.5 times 7 is 31.5, divided by 4 is 7.875. 63/8 is 7.875, yes. So 63/8. But wait, is this correct?\n",
      "\n",
      "Alternatively, maybe I can compute the area of the triangle in 3D space directly. The three points are (9/2, 0, 0), (0, 3, 0), and (0, 0, 3/2). Let's call these points A, B, and C. The vectors AB and AC can be found. AB is B - A = (-9/2, 3, 0). AC is C - A = (-9/2, 0, 3/2). Then the cross product AB × AC would give a vector whose magnitude is the area of the parallelogram formed by AB and AC, so half of that is the area of the triangle.\n",
      "\n",
      "Compute AB × AC:\n",
      "\n",
      "AB = (-9/2, 3, 0)\n",
      "AC = (-9/2, 0, 3/2)\n",
      "\n",
      "Cross product:\n",
      "\n",
      "i component: 3*(3/2) - 0*0 = 9/2\n",
      "j component: 0*(-9/2) - (-9/2)*(3/2) = 0 + (27/4) = 27/4\n",
      "k component: (-9/2)*0 - 3*(-9/2) = 0 + 27/2 = 27/2\n",
      "\n",
      "So the cross product vector is (9/2, 27/4, 27/2). The magnitude of this vector is sqrt( (9/2)^2 + (27/4)^2 + (27/2)^2 )\n",
      "\n",
      "Compute each term:\n",
      "\n",
      "(9/2)^2 = 81/4\n",
      "(27/4)^2 = 729/16\n",
      "(27/2)^2 = 729/4\n",
      "\n",
      "So adding them up:\n",
      "\n",
      "81/4 + 729/16 + 729/4 = Convert all to sixteenths:\n",
      "\n",
      "81/4 = 324/16\n",
      "729/16 = 729/16\n",
      "729/4 = 2916/16\n",
      "\n",
      "Total: 324 + 729 + 2916 = 324 + 729 is 1053, 1053 + 2916 = 3969. So 3969/16.\n",
      "\n",
      "sqrt(3969/16) = sqrt(3969)/sqrt(16) = 63/4.\n",
      "\n",
      "Therefore, the magnitude of the cross product is 63/4. The area of the parallelogram is 63/4, so the area of the triangle is half of that: 63/8. Which matches the previous result. So the surface area is 63/8. Therefore, the answer should be 63/8.\n",
      "\n",
      "Wait, so both methods give the same result. So either way, whether computing via the surface integral (using the projection and scaling by 7/6) or computing the area of the triangle in      \\[\n",
      "     6z = 9 \\implies z = \\frac{3}{2}\n",
      "     \\]\n",
      "   Thus, the intercepts are \\(\\left(\\frac{9}{2}, 0, 0\\right)\\), \\((0, 3, 0)\\), and \\((0, 0, \\frac{3}{2})\\).\n",
      "\n",
      "2. **Express the plane equation in terms of \\(z\\):**\n",
      "   \\[\n",
      "   6z = 9 - 2x - 3y \\implies z = \\frac{9 - 2x - 3y}{6}\n",
      "   \\]\n",
      "   Here, \\(f(x, y) = \\frac{9 - 2x - 3y}{6}\\).\n",
      "\n",
      "3. **Compute the partial derivatives of \\(f(x, y)\\):**\n",
      "   \\[\n",
      "   f_x = \\frac{\\partial f}{\\partial x} = \\frac{-2}{6} = -\\frac{1}{3}\n",
      "   \\]\n",
      "   \\[\n",
      "   f_y = \\frac{\\partial f}{\\partial y} = \\frac{-3}{6} = -\\frac{1}{2}\n",
      "   \\]\n",
      "\n",
      "4. **Determine the integrand for the surface area:**\n",
      "   The formula for the surface area of a graph \\(z = f(x, y)\\) is given by:\n",
      "   \\[\n",
      "   \\text{Surface Area} = \\iint_D \\sqrt{\\left(\\frac{\\partial f}{\\partial x}\\right)^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^2 + 1} \\, dA\n",
      "   \\]\n",
      "   Substituting the partial derivatives:\n",
      "   \\[\n",
      "   \\sqrt{\\left(-\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + 1} = \\sqrt{\\frac{1}{9} + \\frac{1}{4} + 1}\n",
      "   \\]\n",
      "   Convert to a common denominator:\n",
      "   \\[\n",
      "   \\sqrt{\\frac{4}{36} + \\frac{9}{36} + \\frac{36}{36}} = \\sqrt{\\frac{49}{36}} = \\frac{7}{6}\n",
      "   \\]\n",
      "\n",
      "5. **Determine the region \\(D\\) in the \\(xy\\)-plane:**\n",
      "   The region \\(D\\) is the triangle in the \\(xy\\)-plane bounded by the lines \\(x = 0\\), \\(y = 0\\), and \\(2x + 3y = 9\\). The vertices of this triangle are \\((0, 0)\\), \\(\\left(\\frac{9}{2}, 0\\right)\\), and \\((0, 3)\\).\n",
      "\n",
      "6. **Calculate the area of the region \\(D\\):**\n",
      "   The area of the triangle with vertices \\((0, 0)\\), \\(\\left(\\frac{9}{2}, 0\\right)\\), and \\((0, 3)\\) is:\n",
      "   \\[\n",
      "   \\text{Area of } D = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times \\frac{9}{2} \\times 3 = \\frac{27}{4}\n",
      "   \\]\n",
      "\n",
      "7. **Compute the surface area:**\n",
      "   The surface area is given by:\n",
      "   \\[\n",
      "   \\text{Surface Area} = \\frac{7}{6} \\times \\text{Area of } D = \\frac{7}{6} \\times \\frac{27}{4} = \\frac{189}{24} = \\frac{63}{8}\n",
      "   \\]\n",
      "\n",
      "Therefore, the surface area of the portion of the plane \\(2x + 3y + 6z = 9\\) that lies in the first octant is:\n",
      "\\[\n",
      "\\boxed{\\frac{63}{8}}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Determine the surface area of the portion of the plane $2x + 3y + 6z = 9$ that lies in the first octant.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8770ad4-1c0c-49f3-8aba-afe75e715df1",
   "metadata": {},
   "source": [
    "#### Step 7.模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835a922-d86e-4c91-9482-8da079b44dd0",
   "metadata": {},
   "source": [
    "微调结束后即可进行模型保存，由于我们训练的LoRA本身是FP16精度，因此模型需要保存为fp16精度格式，才能完整保留模型当前性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48564a36-eaeb-4673-8e04-e527f11d8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(save_directory = \"/root/pandagpt_data/qwen/Qwen3-8B-finetuned-fp16\", \n",
    "                             tokenizer = tokenizer, \n",
    "                             save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6939b-0f5e-4acb-9e2e-c6c7bb55bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存到: /root/pandagpt_data/qwen/Qwen3-8B-finetuned\n"
     ]
    }
   ],
   "source": [
    "# 基本保存\n",
    "save_path = \"/root/pandagpt_data/qwen/Qwen3-8B-finetuned\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"模型保存到: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05db24-13b9-4a23-bdf9-3d5be64a415a",
   "metadata": {},
   "source": [
    "> - 正在将原始 4bit 权重与微调产生的 LoRA adapter 合并，并转换为 16bit（FP16）精度，用于部署\n",
    "> - 会尽量使用不超过 703.3 MB（最大上限是你设备的 70～75% 内存）来进行保存操作，避免崩溃或卡死\n",
    "> - 开始保存模型，会进行合并+格式转换，时间取决于模型大小（比如 Qwen3-32B 预计要几分钟）\n",
    "> - 如果模型太大而内存不足，Unsloth 会自动启用“磁盘保存模式”，避免占用过多内存\n",
    "> - tokenizer 保存完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a92c91-e67e-428d-95d4-a3ed0e4e447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==1.2.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/utils.py:107\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unpack\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \\\n\u001b[32m    109\u001b[39m         \u001b[38;5;28mtype\u001b[39m(Unpack) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t_Unpack), \\\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Unpack type changed! Please file a bug report asap!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/__init__.py:27\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     is_pretty_midi_available,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/dependency_versions_check.py:57\u001b[39m\n\u001b[32m     55\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/versions.py:117\u001b[39m, in \u001b[36mrequire_version_core\u001b[39m\u001b[34m(requirement)\u001b[39m\n\u001b[32m    116\u001b[39m hint = \u001b[33m\"\u001b[39m\u001b[33mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.[dev]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre working with git main\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/versions.py:111\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted.items():\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/versions.py:44\u001b[39m, in \u001b[36m_compare_versions\u001b[39m\u001b[34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version.parse(got_ver), version.parse(want_ver)):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     45\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==1.2.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mUNSLOTH_USE_MODELSCOPE\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1.1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1.1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth/__init__.py:95\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     84\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     89\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     94\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth_zoo/__init__.py:194\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Log Unsloth-Zoo Utilities\u001b[39;00m\n\u001b[32m    192\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mUNSLOTH_ZOO_IS_PRESENT\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtemporary_patches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    195\u001b[39m     encode_conversations_with_harmony,\n\u001b[32m    196\u001b[39m )\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl_environments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    198\u001b[39m     check_python_modules,\n\u001b[32m    199\u001b[39m     create_locked_down_function,\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m     launch_openenv,\n\u001b[32m    204\u001b[39m )\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Top some pydantic warnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/__init__.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Unsloth Zoo - Utilities for Unsloth\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen, Michael Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# You should have received a copy of the GNU Lesser General Public License\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# along with this program.  If not, see <https://www.gnu.org/licenses/>.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemma3n\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/gemma.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TEMPORARY_PATCHES, torch_compile\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     patch_function,\n\u001b[32m     24\u001b[39m     process_output_options,\n\u001b[32m     25\u001b[39m     KWARGS_TYPE,\n\u001b[32m     26\u001b[39m     raise_error,\n\u001b[32m     27\u001b[39m     ImageInput,\n\u001b[32m     28\u001b[39m     PreTokenizedInput,\n\u001b[32m     29\u001b[39m     TextInput,\n\u001b[32m     30\u001b[39m     Cache,\n\u001b[32m     31\u001b[39m     StaticCache,\n\u001b[32m     32\u001b[39m     HybridCache,\n\u001b[32m     33\u001b[39m     Unpack,\n\u001b[32m     34\u001b[39m     patch_function_past_key_values,\n\u001b[32m     35\u001b[39m     dedent,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpatch_Gemma3Processor\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/unsloth_zoo/temporary_patches/utils.py:122\u001b[39m\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    119\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m***** Please update and reinstall torchvision - it broke! `pip install --upgrade --force-reinstall --no-cache-dir torchvision` *****\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         )\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUnpack\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(e)\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    124\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Unpack has been moved! Other error = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease file a bug report asap!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mException\u001b[39m: huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==1.2.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['UNSLOTH_USE_MODELSCOPE'] = '1'\n",
    "print(\"1.1\")\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "print(\"1.1\")\n",
    "max_seq_length = 8192\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"./Qwen3-0.6B-unsloth-bnb-4bit\",\n",
    "    model_name = \"/root/pandagpt_data/qwen/Qwen3-8B-finetuned\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    local_files_only = True,  # 添加这一行\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f70887-f376-44f6-a1c9-8cc16e7a844e",
   "metadata": {},
   "source": [
    "#### Step 8.模型性能测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b72415-1434-4761-9d98-359fe622f9cc",
   "metadata": {},
   "source": [
    "接下来即可使用vllm对其进行调用，并借助EvalScope进行测试。需要借助vLLM调用导出后的模型，然后借助evalscope进行测试。\n",
    "\n",
    "```python\n",
    "from evalscope import TaskConfig, run_task\n",
    "task_cfg = TaskConfig(\n",
    "    model='./Qwen3-32B-finetuned-fp16',\n",
    "    api_url='http://127.0.0.1:8000/v1/chat/completions',\n",
    "    eval_type='service',\n",
    "    datasets=[\n",
    "        'data_collection',\n",
    "    ],\n",
    "    dataset_args={\n",
    "        'data_collection': {\n",
    "            'dataset_id': 'modelscope/EvalScope-Qwen3-Test',\n",
    "            'filters': {'remove_until': '</think>'}  # 过滤掉思考的内容\n",
    "        }\n",
    "    },\n",
    "    eval_batch_size=128,\n",
    "    generation_config={\n",
    "        'max_tokens': 30000,  # 最大生成token数，建议设置为较大值避免输出截断\n",
    "        'temperature': 0.6,  # 采样温度 (qwen 报告推荐值)\n",
    "        'top_p': 0.95,  # top-p采样 (qwen 报告推荐值)\n",
    "        'top_k': 20,  # top-k采样 (qwen 报告推荐值)\n",
    "        'n': 1,  # 每个请求产生的回复数量\n",
    "    },\n",
    "    timeout=60000,  # 超时时间\n",
    "    stream=True,  # 是否使用流式输出\n",
    "    limit=2000,  # 设置为2000条数据进行测试\n",
    ")\n",
    "\n",
    "run_task(task_cfg=task_cfg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f65805-9f22-459c-87fd-6f0408437107",
   "metadata": {},
   "source": [
    "测试后即可在前端中观察测评报告：\n",
    "```bash\n",
    "evalscope app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48977077-21a5-4400-b5f8-da319b1c6a9a",
   "metadata": {},
   "source": [
    "Qwen3中文法律高效微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01218dd7-1cc1-46db-853d-09e144dac40b",
   "metadata": {},
   "source": [
    "需要重启Jupyter Kernel。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259d99d-31aa-4905-9623-6a31e060a8bc",
   "metadata": {},
   "source": [
    "### 1. 案例介绍与数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09da7b-2c02-43a9-8b9e-376971d8fae5",
   "metadata": {},
   "source": [
    "- 数据集样例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6261f04-1f35-41ae-ac6d-c1cb191ce0d3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250429221646854.png\" alt=\"image-20250429221646854\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326a53d-d91a-4a1a-ba8d-115584a68645",
   "metadata": {},
   "source": [
    "> {\"input\": \"**法律问题**  \\n2016年4月20日下午，吸毒人员陈某某1电话联系被告人李某购买甲基苯丙胺，李某随后与陈某某2共同驾车至宁远县禾亭镇陈家村，由李某直接向陈某某1交付4.3克甲基苯丙胺并收取500元毒资，后又将剩余毒品转交至第三方“李2某”处。公安机关根据举报抓获李某，并从陈某某1处查获含袋重4.3克的甲基苯丙胺。根据《刑法》第347条，李某的行为是否构成走私、贩卖、运输、制造毒品罪？其被判处6个月有期徒刑及3000元罚金的量刑是否适当？\", \"output\": \"<think>**  \\n1. **犯罪构成要件分析**  \\n   - **客观要件**：  \\n     - **行为**：李某直接参与毒品交易（接收订单、送货、收款），并协助转移剩余毒品，符合“贩卖”和“运输”行为特征。  \\n     - **结果**：实际交付4.3克甲基苯丙胺，毒品已流入社会。  \\n     - **因果关系**：李某的行为直接促成毒品交易完成。  \\n   - **主观要件**：李某明知甲基苯丙胺为毒品仍主动参与交易，具有贩卖毒品的直接故意。  \\n\\n2. **相关法条适用分析**  \\n   - 根据《刑法》第347条，贩卖甲基苯丙胺无论数量多少均应追究刑事责任。本案毒品数量为4.3克（含包装），属“少量毒品”，但法条未设定入罪数量下限。  \\n\\n3. **量刑情节分析**  \\n   - **从轻情节**：  \\n     - 涉案毒品数量较少（4.3克）；  \\n     - 无证据表明李某系主犯或组织者（行为表现为协助实施）。  \\n   - **从重情节**：  \\n     - 毒品已实际交付，社会危害性成立；  \\n     - 存在毒品流转的连续性（向“李2某”转移剩余毒品）。  \\n\\n4. **判决适当性评价**  \\n   - 量刑符合《刑法》第347条及司法解释对“少量毒品”的标准，6个月有期徒刑处于“三年以下有期徒刑、拘役或管制”的法定刑幅度内；  \\n   - 罚金3000元与毒品价值（500元交易额）比例适当，体现罪责刑相适应原则。  \\n\\n**</think>\\n\\n---\\n\\n**\\n**  \\n\\n李某的行为构成贩卖、运输毒品罪，罪名认定正确。判决量刑适当，理由如下：  \\n1. 其行为完全符合贩卖、运输毒品罪的构成要件；  \\n2. 6个月有期徒刑及3000元罚金与涉案毒品数量、犯罪情节及社会危害性相匹配，符合《刑法》第347条对“少量毒品”的处罚标准。\"}\n",
    "\n",
    "> {\"input\": \"**问题：**  \\n被告人龚某某于2012年11月7日在常德市武陵区战备桥“常德市邮政东区投递部”前，以购买被害人张某甲出售的白色神龙富康牌小车（车牌湘JB2898）为名，谎称试车后趁机将车辆骗走。经鉴定，车辆价值人民币5100元。案发后，龚某某如实供述犯罪事实，赔偿被害人4800元并取得谅解。根据《刑法》第266条，龚某某的行为是否构成诈骗罪？其被判处2个月有期徒刑并处罚金2000元的量刑是否适当？\", \"output\": \"<think>**  \\n1. **犯罪构成要件分析**  \\n   - **客观要件**：  \\n     - **行为**：龚某某虚构“买车”意图，以试车为名非法占有车辆（欺骗手段+转移财产控制）。  \\n     - **结果**：被害人实际丧失车辆所有权（价值5100元）。  \\n     - **因果关系**：欺骗行为直接导致被害人基于错误认识交付车辆。  \\n   - **主观要件**：龚某某具有非法占有的直接故意（明知无购买意愿仍骗取车辆）。  \\n\\n2. **相关法条适用分析**  \\n   - 《刑法》第266条诈骗罪要求“以非法占有为目的，用虚构事实或隐瞒真相的方法骗取数额较大的公私财物”。本案中：  \\n     - 欺骗手段（假意买车+试车）符合“虚构事实”。  \\n     - 涉案金额5100元，超过诈骗罪“数额较大”标准（司法解释中通常为3000元以上）。  \\n\\n3. **量刑情节分析**  \\n   - **从轻情节**：  \\n     - 如实供述（可认定为坦白，依法从轻）。  \\n     - 赔偿损失并取得谅解（酌定从轻）。  \\n   - **无从重情节**：无证据显示暴力、多次犯罪等加重情节。  \\n\\n4. **判决适当性评价**  \\n   - 基准刑参考：诈骗5100元，法定刑为“3年以下有期徒刑、拘役或管制，并处或单处罚金”。  \\n   - 从轻幅度：坦白+赔偿谅解可减少基准刑30%以下。  \\n   - 实际量刑：2个月有期徒刑（接近拘役下限）+罚金2000元，符合从轻后的合理范围，且罚金与犯罪金额比例适当。  \\n\\n**</think>\\n\\n**\\n**  \\n\\n龚某某的行为构成诈骗罪，其以虚假购车名义骗取他人财物，涉案金额达到“数额较大”标准，符合《刑法》第266条的犯罪构成。法院判处2个月有期徒刑并处罚金2000元适当，理由包括：犯罪金额较低、坦白情节、全额赔偿及取得谅解等从轻因素，量刑在法律框架内且体现宽严相济原则。\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87020b-d54a-4afb-bbf2-57f57e279da9",
   "metadata": {},
   "source": [
    "数据主要包含两个部分：input、output，其中output包含<think>思考过程</think>与最终结果输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03651f14-d6dc-42a3-a564-421b689c4329",
   "metadata": {},
   "source": [
    "接下来我们尝试围绕该数据集进行模型法律能力微调，并由此实操大模型问答风格与知识灌注能力微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b48fa-1c9e-4cc3-afd3-1d6017d308c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287a778-f401-439b-b0f2-59ef9694fa5b",
   "metadata": {},
   "source": [
    "> 由于当前实验环境是多卡环境，而动态量化模型只支持单卡运行，因此这里先设置接下来运行的GPU编号。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d2c72-04c5-47c1-84d1-03e4d57a4437",
   "metadata": {},
   "source": [
    "### Step 1.模型导入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf344494-0a38-4060-b0ec-563165c7f31d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先进行模型导入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dba4d7-70e1-45a8-993f-8efadcea7821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e7a2e-87cc-4c96-bf45-56af2d0ca304",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba757335-136a-4c99-bfd2-575650132248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA H800 PCIe. Num GPUs = 2. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c5b1b-21d0-4dc0-a778-5cc17c955516",
   "metadata": {},
   "source": [
    "此时8B模型所占显存如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187b584-4b34-4304-8026-981559f18b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H800 PCIe. Max memory = 79.205 GB.\n",
      "7.865 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4fe5a-f5e4-4953-9b73-3a4c12dd648c",
   "metadata": {},
   "source": [
    "然后进行问答测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da0714-d339-43a7-8600-1949f0488828",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"问题：某分裂组织在边境地区策划武装割据，并与境外势力秘密勾结，其首要分子甲在实施过程中被抓获。请问甲的行为构成何罪？应当如何量刑？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b0894-54ae-4ed0-b09b-c0aba9e5a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"2015年9月4日14时50分许，被告人曹某某在景德镇市珠山区XX涵洞路口醉酒（血液酒精含量未明确数值）、无证驾驶无牌二轮摩托车，搭载李某林左转时未让直行车辆，与超速行驶的朱某良驾驶的出租车相撞，致李某林受伤（具体伤情等级未说明）。事故后曹某某赔偿李某林并获得谅解。公诉机关以危险驾驶罪指控，提供现场勘查记录、酒精检测等9项证据。本案中：1. 曹某某同时存在醉酒、无证、无牌三项违法情节，但缺乏具体酒精浓度数据；2. 事故系双方过错（曹某某未让行与朱某良超速）共同导致；3. 被害人已谅解。在此情况下：（1）危险驾驶罪的构成要件是否全部满足？（2）量刑时如何平衡\\\"无证+无牌+醉驾\\\"的从重情节与\\\"赔偿谅解\\\"的从轻情节？（3）最终判处3个月有期徒刑并处罚金1000元是否适当？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008997a7-de7b-4e3b-a2fb-baab5c58647b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是甲在分裂组织策划武装割据，与境外势力勾结，被抓获后的行为构成什么罪，以及如何量刑。首先，我需要确定甲的行为属于什么犯罪。根据中国刑法，分裂国家、颠覆国家政权、煽动分裂国家都是罪名。但这里的情况是分裂组织，可能涉及分裂国家的犯罪，或者可能属于颠覆国家政权的犯罪？\n",
      "\n",
      "首先，分裂国家罪和颠覆国家政权罪的区别。分裂国家罪是指国家分裂，比如分裂国家，但可能不涉及政权的颠覆，而颠覆国家政权罪则是指国家政权被推翻，比如通过武装叛乱、武装暴动等行为。甲是策划武装割据，可能涉及武装叛乱，但具体要看是否达到颠覆国家政权的程度。\n",
      "\n",
      "另外，甲与境外势力勾结，可能涉及国际犯罪，比如间谍罪或者叛国罪？但根据中国刑法，分裂国家罪和颠覆国家政权罪可能有交叉。例如，如果甲的分裂行为导致国家分裂，可能构成分裂国家罪，同时可能涉及颠覆国家政权罪。\n",
      "\n",
      "需要分析甲的行为是否构成这两个罪名中的一个或两个。比如，如果甲策划武装割据，可能属于颠覆国家政权的犯罪，因为武装割据可能构成颠覆国家政权的行为。同时，可能还有分裂国家罪，但可能两者合并处理。\n",
      "\n",
      "另外，是否需要考虑甲是否有具体的实施行为，比如是否组织、策划、指挥等，这会影响罪名的轻重。例如，如果甲是首要分子，可能属于主犯，可能面临更重的刑罚。\n",
      "\n",
      "量刑方面，根据刑法规定，分裂国家罪的刑罚是十年以上有期徒刑、无期徒刑或死刑；颠覆国家政权罪的刑罚是十年以上有期徒刑、无期徒刑或死刑。如果同时触犯两个罪名，可能数罪并罚，或者根据情节加重处罚。\n",
      "\n",
      "还需要考虑是否有其他情节，比如是否造成严重后果，是否属于情节严重，是否属于组织、领导、教唆等，这会影响量刑的幅度。\n",
      "\n",
      "可能还需要查阅相关法律条文，比如《刑法》第103条、第105条、第108条等，确认具体罪名和量刑标准。\n",
      "\n",
      "总结：甲的行为可能构成分裂国家罪或颠覆国家政权罪，具体需要根据具体情节判断。量刑上可能在十年以上有期徒刑至无期徒刑或死刑，具体取决于情节严重程度和是否有其他因素。\n",
      "</think>\n",
      "\n",
      "根据中国《刑法》相关规定，甲的行为可能构成**分裂国家罪**或**颠覆国家政权罪**，具体需结合其行为的具体情节判断。以下是具体分析：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、罪名分析**\n",
      "1. **分裂国家罪**  \n",
      "   - **构成要件**：以暴力、威胁手段分裂国家，或通过组织、策划武装割据，导致国家分裂。  \n",
      "   - **适用情形**：若甲通过组织武装割据（如建立武装团体、策划武装暴动）或与境外势力勾结，意图分裂国家，可能构成此罪。  \n",
      "   - **量刑**：根据《刑法》第103条，可处**十年以上有期徒刑、无期徒刑或死刑**。\n",
      "\n",
      "2. **颠覆国家政权罪**  \n",
      "   - **构成要件**：通过武装叛乱、暴动等手段，推翻国家政权，或通过策划、组织颠覆国家政权的行为。  \n",
      "   - **适用情形**：若甲策划武装割据（如组织武装起义、策划武装暴动），并可能通过境外势力支持，可能构成此罪。  \n",
      "   - **量刑**：根据《刑法》第105条，可处**十年以上有期徒刑、无期徒刑或死刑**。\n",
      "\n",
      "3. **数罪并罚**  \n",
      "   - 若甲同时触犯上述两种罪名，可能数罪并罚，刑罚叠加（如十年以上有期徒刑、无期徒刑或死刑）。\n",
      "\n",
      "---\n",
      "\n",
      "### **二、量刑依据**\n",
      "- **情节严重性**：  \n",
      "  - 若甲组织、指挥武装割据，造成严重社会危害（如破坏国家统一、危害国家安全），可能加重刑罚。  \n",
      "  - 若与境外势力勾结，可能涉及国际犯罪，需考虑国际法与国内法的衔接（如《刑法》第312条关于“境外势力”的认定）。\n",
      "\n",
      "- **主观故意**：  \n",
      "  - 甲作为首要分子，主观恶性明显，可能被认定为“组织、领导、教唆”等情节，加重处罚。\n",
      "\n",
      "- **法定刑罚**：  \n",
      "  - **分裂国家罪**：10年以上有期徒刑至无期徒刑。  \n",
      "  - **颠覆国家政权罪**：10年以上有期徒刑至无期徒刑。  \n",
      "  - **数罪并罚**：若同时触犯两罪，刑罚可能叠加。\n",
      "\n",
      "---\n",
      "\n",
      "### **三、法律依据**\n",
      "- **《刑法》第103条**：分裂国家罪。  \n",
      "- **《刑法》第105条**：颠覆国家政权罪。  \n",
      "- **《刑法》第312条**：境外势力与叛国罪的衔接（如《刑法》第103条第2款规定“分裂国家、颠覆国家政权、煽动颠覆国家政权”的刑事责任）。\n",
      "\n",
      "---\n",
      "\n",
      "### **四、结论**\n",
      "甲的行为可能构成**分裂国家罪**或**颠覆国家政权罪**，具体罪名需结合其组织、策划、实施的具体行为及后果。根据《刑法》规定，其量刑范围为**十年以上有期徒刑至无期徒刑或死刑**，并可能适用数罪并罚。建议结合案件证据（如组织架构、境外势力参与、危害后果等）进一步认定具体罪名及量刑。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : question_1}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f41a1-6396-4288-8565-d8d9ee8d64fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是关于2015年一个交通事故案件，涉及曹某某的危险驾驶罪指控，还有关于量刑的平衡问题。我需要先理清楚每个问题的要点。\n",
      "\n",
      "首先，第一问是危险驾驶罪的构成要件是否全部满足。根据中国的法律，危险驾驶罪需要满足几个条件：1. 醉酒驾驶；2. 无证驾驶；3. 无牌车辆。曹某某有醉酒、无证、无牌这三个违法情节，但缺乏具体酒精浓度数据。所以，虽然有三个违法点，但酒精浓度没有数据，可能影响是否构成危险驾驶罪的要件。不过，危险驾驶罪的构成是否需要酒精浓度的明确数据呢？可能需要看具体法律条文，比如刑法第133条的规定，危险驾驶罪是否要求醉酒驾驶，还是只要在驾驶过程中有危险行为即可。比如，如果曹某某没有酒精检测数据，但有醉酒的主观故意，可能构成危险驾驶罪，但需要看是否有证据支持。不过，用户提到的证据中只有现场勘查记录和酒精检测等9项证据，但酒精浓度未明确，所以可能无法确定是否醉酒。这时候可能需要判断是否构成危险驾驶罪的要件。\n",
      "\n",
      "第二问是量刑时如何平衡无证、无牌、醉驾的从重情节和赔偿谅解的从轻情节。无证和无牌属于严重违法行为，可能从重，但醉驾是否需要酒精浓度数据呢？如果缺乏数据，可能无法认定为醉驾，但可能被认定为醉酒驾驶，但证据不足。这时候可能需要考虑是否有其他证据支持，比如曹某某的陈述或者医疗记录？但用户没有提到这些，所以可能需要假设这些证据存在，或者根据现有信息来判断。如果曹某某确实醉酒，但酒精浓度未明确，可能被认定为醉驾，但需要看是否有其他证据。不过，如果缺乏数据，可能无法确认，所以从重情节可能不成立，或者需要根据具体情况判断。\n",
      "\n",
      "第三问是判处3个月有期徒刑并处罚金1000元是否适当。根据刑法，危险驾驶罪的量刑可能在拘役1-6个月，罚金500-2000元。3个月可能在范围内，但需要看是否有从轻情节。如果曹某某赔偿并谅解，可能减轻处罚，但需要看是否有其他因素。如果无证和无牌属于从重，但赔偿谅解从轻，可能综合考虑，但3个月是否过轻？或者是否在合理范围内？\n",
      "\n",
      "可能需要查阅相关法律条文，比如刑法第133条，危险驾驶罪的构成要件，以及量刑标准。另外，关于醉驾是否需要酒精浓度数据，可能根据《关于办理醉酒驾驶机动车刑事案件适用法律若干问题的解释》的规定，醉酒驾驶需要血液酒精含量达到80mg/100ml以上，但用户提到的证据中没有明确数值，所以可能无法认定为醉驾，但可能被认定为醉酒驾驶，但证据不足。这时候可能需要考虑是否构成危险驾驶罪，或者是否需要其他证据。\n",
      "\n",
      "总结来说，第一问是否满足构成要件可能需要确认是否有酒精浓度数据，但用户没有提供，所以可能认为不满足，或者存在争议。第二问中，无证和无牌可能从重，但醉驾的证据不足，可能从重情节不成立，或者部分成立。第三问的量刑可能需要综合考虑，但3个月可能适当，但需要看具体情节。\n",
      "\n",
      "可能还需要考虑是否有其他因素，比如赔偿谅解是否足够减轻处罚，但需要看是否符合减轻情节的条件。例如，根据刑法，赔偿谅解可以作为从轻情节，但可能需要满足一定的条件，比如赔偿金额是否足够，是否主动赔偿等。\n",
      "\n",
      "总之，需要分点回答，每个问题都要分析法律条文和证据，然后给出结论。\n",
      "</think>\n",
      "\n",
      "针对2015年该案件的法律分析，结合中国刑法、司法解释及量刑标准，现从三个核心问题展开分析：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、危险驾驶罪的构成要件是否全部满足？**\n",
      "\n",
      "1. **构成要件要素**  \n",
      "   根据《中华人民共和国刑法》第133条及《关于办理醉酒驾驶机动车刑事案件适用法律若干问题的解释》（法释〔2011〕17号），危险驾驶罪需满足以下要件：  \n",
      "   - **主观要件**：具有醉酒、嗜酒、酒后驾驶等危险行为；  \n",
      "   - **客观要件**：在道路上驾驶机动车，追逐竞驶、超速行驶等；  \n",
      "   - **行为结果**：造成交通事故且负事故责任。  \n",
      "\n",
      "   本案中，曹某某存在**醉酒、无证、无牌**三项违法行为，但**酒精浓度未明确**，且**事故后赔偿谅解**。根据《最高人民法院关于适用〈中华人民共和国刑法〉的解释》第12条，醉酒驾驶需血液酒精含量达到80mg/100ml以上，但未明确数据时，需结合其他证据（如血检报告、医疗记录等）判断。若无明确数据，可能无法直接认定为“醉驾”，但需进一步分析：\n",
      "\n",
      "   - **主观故意**：曹某某存在醉酒状态，可能构成“醉酒驾驶”，但需证据支持。若无酒精浓度数据，可能被认定为“酒后驾驶”但未达到“醉驾”标准，此时是否构成危险驾驶罪需视具体情节。  \n",
      "   - **客观行为**：无证、无牌属于严重违法行为，但危险驾驶罪的构成需结合是否构成“危险驾驶”行为（如超速、超载等）。若仅存在醉酒，但未明确酒精浓度，可能被认定为“酒驾”而非“醉驾”，但需结合其他证据（如医疗记录、证人证言等）判断。\n",
      "\n",
      "2. **结论**  \n",
      "   - 若无酒精浓度数据，可能无法直接认定为“醉驾”，但可能被认定为“酒驾”（需进一步证据）。  \n",
      "   - **构成要件是否全部满足**：若仅凭无证、无牌及醉酒（主观存在）但缺乏酒精浓度数据，可能部分满足危险驾驶罪的构成要件，但**未完全满足“醉酒”要件**，因此**不构成危险驾驶罪**。\n",
      "\n",
      "---\n",
      "\n",
      "### **二、量刑时如何平衡“无证+无牌+醉驾”与“赔偿谅解”的情节**\n",
      "\n",
      "1. **从重情节**  \n",
      "   - **无证、无牌**：属于严重违法行为，可能被认定为“情节严重”（如《刑法》第133条第2款），可能从重处罚。  \n",
      "   - **醉驾**：若未明确酒精浓度，可能不构成“醉驾”，但可能被认定为“酒驾”，需根据证据判断。若存在醉酒状态，可能影响刑罚幅度。\n",
      "\n",
      "2. **从轻情节**  \n",
      "   - **赔偿谅解**：根据《刑法》第67条，认罪认罚并赔偿损失的，可从轻处罚。本案中，曹某某已赔偿并获得谅解，可作为从轻情节。  \n",
      "   - **其他因素**：事故后未造成严重后果，且无其他恶劣情节（如致人死亡），可能进一步减轻处罚。\n",
      "\n",
      "3. **平衡分析**  \n",
      "   - 若曹某某被认定为“酒驾”（无酒精浓度数据但主观醉酒），则“无证+无牌”可能作为从重情节，但“赔偿谅解”可能作为从轻情节。  \n",
      "   - 若仅认定为“酒驾”（未明确醉酒状态），则“无证+无牌”可能从重，但“赔偿谅解”可能从轻。  \n",
      "   - 综合来看，**“赔偿谅解”可能对量刑有决定性影响**，若赔偿金额较高且主动赔偿，可能显著减轻处罚。\n",
      "\n",
      "---\n",
      "\n",
      "### **三、最终判处3个月有期徒刑并处罚金1000元是否适当？**\n",
      "\n",
      "1. **刑罚合理性**  \n",
      "   - 根据《刑法》第133条，危险驾驶罪的刑罚范围为拘役1-6个月，罚金500-2000元。  \n",
      "   - 3个月有期徒刑（拘役）在法定范围内，但需结合具体情节。  \n",
      "   - 若曹某某被认定为“酒驾”（无酒精浓度数据但主观醉酒），可能需加重处罚，但若赔偿谅解显著减轻，则可能适用缓刑或减轻处罚。\n",
      "\n",
      "2. **量刑建议**  \n",
      "   - **若曹某某被认定为“酒驾”**：  \n",
      "     - 3个月有期徒刑可能适当，但需结合赔偿谅解。  \n",
      "     - 若赔偿金额高且谅解充分，可考虑适用缓刑或减轻处罚。  \n",
      "   - **若仅认定为“酒驾”**：  \n",
      "     - 3个月有期徒刑可能适当，但需确认是否符合“醉驾”标准。  \n",
      "\n",
      "3. **结论**  \n",
      "   - **若曹某某被认定为“酒驾”**，3个月有期徒刑可能适当，但需进一步确认酒精浓度数据。  \n",
      "   - **若曹某某被认定为“醉驾”**，则刑罚可能偏重，建议调整为拘役2个月并罚金1000元。  \n",
      "   - **综合考量**：若赔偿谅解显著，建议减轻处罚，但3个月有期徒刑可能在量刑范围内，需结合具体证据及司法裁量。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "\n",
      "1. **危险驾驶罪构成要件**：需结合酒精浓度数据判断，若无明确数据，可能不构成“醉驾”，但可能被认定为“酒驾”。  \n",
      "2. **量刑平衡**：无证、无牌从重，赔偿谅解从轻，但赔偿金额及谅解程度是关键。  \n",
      "3. **刑罚合理性**：3个月有期徒刑可能适当，但需根据具体情节调整，建议结合证据进一步确认。\n",
      "\n",
      "**最终建议**：若曹某某被认定为“酒驾”，建议判处拘役2个月并罚金1000元；若认定为“醉驾”，则3个月有期徒刑可能适当，但需进一步证据支持。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : question_2}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361119d3-2540-494c-a51c-6a80862989ac",
   "metadata": {},
   "source": [
    "### Step 2.数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ebb003-aa5a-4f52-a305-878152e6dc68",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行数据集读取与导入。这里除了要导入既定的高质量的法律问答数据集，同时也需要准备普通对话数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf400b-eda8-4c8f-a638-9d93e707978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf9ef6-4dbd-4843-bc27-f1ddfde70795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_jsonl_dataset(file_path):\n",
    "    \"\"\"从JSONL文件加载数据集\"\"\"\n",
    "    data = {\"input\": [], \"output\": []}\n",
    "    \n",
    "    print(f\"开始加载数据集: {file_path}\")\n",
    "    count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                # 根据数据集结构提取字段\n",
    "                input_text = item.get(\"input\", \"\")\n",
    "                output = item.get(\"output\", \"\")\n",
    "                \n",
    "                data[\"input\"].append(input_text)\n",
    "                data[\"output\"].append(output)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"解析行时出错: {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"数据集加载完成: 成功加载{count}个样本, 跳过{error_count}个错误样本\")\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283f88c-0dcd-4a7e-b28e-3e9fba808d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./train_1k.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee2f20-7042-44c3-91f3-d291b6b4d8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据集: ./train_1k.jsonl\n",
      "数据集加载完成: 成功加载1000个样本, 跳过0个错误样本\n",
      "\n",
      "数据集统计:\n",
      "- 样本数量: 1000\n",
      "- 字段: ['input', 'output']\n",
      "{'input': '被告人周某被指控于2017年4月6日在绍兴市上虞区曹娥街道，通过虚构自己在蒿坝白鹤湾矿基石料场拥有20%股份的事实，并以该虚假股份作为抵押借款的手段，骗取被害人刘某1人民币8万元，所得赃款用于赌博、偿还债务及个人挥霍。根据《刑法》第266条诈骗罪的规定，周某的行为是否完全符合诈骗罪的构成要件？涉案金额8万元及赃款用途等情节，对28个月有期徒刑并处2万元罚金的量刑是否适当？', 'output': '<think>1. 犯罪构成要件分析\\n   - 客观要件：\\n     * 行为：周某实施了虚构股份持有事实（虚假陈述）→ 以虚假抵押借款（欺骗手段）→ 使被害人陷入错误认识并交付8万元（处分财产）→ 实际控制赃款后用于非法活动及个人消费（非法占有）\\n     * 结果：被害人财产损失8万元，因果关系链条完整\\n   - 主观要件：\\n     * 直接故意：明知无股份仍虚构事实，具有非法占有目的（赃款用途印证赌博、还债等个人用途）\\n\\n2. 相关法条适用分析\\n   - 《刑法》第266条诈骗罪要求\"虚构事实+非法占有目的+财物转移\"，本案完全符合：\\n     * 虚构股份事实属于\"虚构事实\"\\n     * 赌博挥霍用途证明\"非法占有目的\"\\n     * 8万元属\"数额较大\"（浙江诈骗罪立案标准为6000元）\\n\\n3. 量刑情节分析\\n   - 从重情节：\\n     * 赃款用于违法犯罪活动（赌博）\\n     * 无退赔情节（事实未提及退赃）\\n   - 基准刑参照：\\n     * 浙江诈骗6-10万基准刑2-3年，8万元对应28个月（2年4个月）符合幅度\\n     * 罚金2万元为涉案金额25%（1-5倍范围内）\\n\\n4. 判决适当性评价\\n   - 罪名认定准确：完全符合诈骗罪四要件\\n   - 量刑适当：在法定幅度内，且考虑资金用途恶劣性\\n   - 罚金比例合理：介于1-5倍标准之间</think>\\n\\n周某的行为完全符合《刑法》第266条诈骗罪的构成要件，其虚构股份抵押借款的行为具有明显的非法占有目的，且造成被害人8万元的实际损失。法院判处28个月有期徒刑并处2万元罚金，符合浙江省关于诈骗罪\"数额较大\"的量刑标准，且考虑到赃款用于赌博等从重情节，该判决在事实认定和法律适用上均属适当。'}\n"
     ]
    }
   ],
   "source": [
    "# 加载自定义数据集\n",
    "dataset = load_jsonl_dataset(data_path)\n",
    "\n",
    "# 显示数据集信息\n",
    "print(f\"\\n数据集统计:\")\n",
    "print(f\"- 样本数量: {len(dataset)}\")\n",
    "print(f\"- 字段: {dataset.column_names}\")\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e48fb2-d57a-4c02-b401-9ed7ddbc62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"根据提示模板格式化数据\"\"\"\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for  input_text, output in zip(inputs, outputs):\n",
    "        texts.append([\n",
    "            {\"role\" : \"user\",      \"content\" : input_text},\n",
    "            {\"role\" : \"assistant\", \"content\" : output},\n",
    "        ])\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99293d5d-ee3c-4cb2-8d73-de77890d749f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始格式化数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 38004.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集格式化完成\n"
     ]
    }
   ],
   "source": [
    "# 应用格式化\n",
    "print(\"开始格式化数据集...\")\n",
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset.map(formatting_prompts_func, batched = True)[\"text\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "print(\"数据集格式化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdaa2ab-cec5-4fe0-a91e-bcd869cfb48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n被告人周某被指控于2017年4月6日在绍兴市上虞区曹娥街道，通过虚构自己在蒿坝白鹤湾矿基石料场拥有20%股份的事实，并以该虚假股份作为抵押借款的手段，骗取被害人刘某1人民币8万元，所得赃款用于赌博、偿还债务及个人挥霍。根据《刑法》第266条诈骗罪的规定，周某的行为是否完全符合诈骗罪的构成要件？涉案金额8万元及赃款用途等情节，对28个月有期徒刑并处2万元罚金的量刑是否适当？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n1. 犯罪构成要件分析\\n   - 客观要件：\\n     * 行为：周某实施了虚构股份持有事实（虚假陈述）→ 以虚假抵押借款（欺骗手段）→ 使被害人陷入错误认识并交付8万元（处分财产）→ 实际控制赃款后用于非法活动及个人消费（非法占有）\\n     * 结果：被害人财产损失8万元，因果关系链条完整\\n   - 主观要件：\\n     * 直接故意：明知无股份仍虚构事实，具有非法占有目的（赃款用途印证赌博、还债等个人用途）\\n\\n2. 相关法条适用分析\\n   - 《刑法》第266条诈骗罪要求\"虚构事实+非法占有目的+财物转移\"，本案完全符合：\\n     * 虚构股份事实属于\"虚构事实\"\\n     * 赌博挥霍用途证明\"非法占有目的\"\\n     * 8万元属\"数额较大\"（浙江诈骗罪立案标准为6000元）\\n\\n3. 量刑情节分析\\n   - 从重情节：\\n     * 赃款用于违法犯罪活动（赌博）\\n     * 无退赔情节（事实未提及退赃）\\n   - 基准刑参照：\\n     * 浙江诈骗6-10万基准刑2-3年，8万元对应28个月（2年4个月）符合幅度\\n     * 罚金2万元为涉案金额25%（1-5倍范围内）\\n\\n4. 判决适当性评价\\n   - 罪名认定准确：完全符合诈骗罪四要件\\n   - 量刑适当：在法定幅度内，且考虑资金用途恶劣性\\n   - 罚金比例合理：介于1-5倍标准之间\\n</think>\\n\\n周某的行为完全符合《刑法》第266条诈骗罪的构成要件，其虚构股份抵押借款的行为具有明显的非法占有目的，且造成被害人8万元的实际损失。法院判处28个月有期徒刑并处2万元罚金，符合浙江省关于诈骗罪\"数额较大\"的量刑标准，且考虑到赃款用于赌博等从重情节，该判决在事实认定和法律适用上均属适当。<|im_end|>\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce363f9-65a3-4b27-82e8-3f85bd345619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01380ac0-5cc5-44f6-9d1c-0ecf7a218742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mlabonne/FineTome-100k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/mlabonne___fine_tome-100k/default/0.0.0/c2343c1372ff31f51aa21248db18bffa3193efdb (last modified on Mon May  5 18:48:17 2025).\n"
     ]
    }
   ],
   "source": [
    "non_reasoning_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8617936-50de-43c1-b573-a2530d151969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa14cc-08a2-4616-a1e4-447894960b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f35cb-d796-46fd-8094-15e6c7ad210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    1000,\n",
    "    random_state = 2407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acf2a4-87cb-4377-9c1f-5b261a4a05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6db5ee-845f-4e5d-80ca-72d91bc46f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de731ffd-553b-4dfe-8941-fbcb43f2d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77763d-8da5-48d0-8ab3-2b40890d6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7845fa-cf43-475e-b10a-5c2037a9e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>user\\n被告人刘X于2014年2月15日凌晨2时许，在齐齐哈尔市龙沙区雅乐迪KTV歌厅内秘密窃取一楼大厅功德箱（内有人民币10000余元），后于2014年3月7日在佳木斯市被抓获。其归案后如实供述犯罪事实、自愿认罪并积极返还赃款。根据《中华人民共和国刑法》相关规定，刘X的行为是否构成盗窃罪？若构成犯罪，在涉案金额达\"数额较大\"标准且存在认罪悔罪情节的情况下，判处8个月有期徒刑并处10000元罚金的判决是否适当？<|im_end|>\\n<|im_start|>assistant\\n<think>\\n1. **犯罪构成要件分析**  \\n   - **客观要件**：  \\n     - **行为**：问题明确刘X\"秘密窃取\"功德箱，符合盗窃罪\"以非法占有为目的，秘密窃取公私财物\"的行为特征。  \\n     - **结果**：涉案金额10000元，根据《最高人民法院、最高人民检察院关于办理盗窃刑事案件适用法律若干问题的解释》，已达到\"数额较大\"标准（1000元至3000元以上）。  \\n     - **因果关系**：刘X的窃取行为直接导致被害人财产损失。  \\n   - **主观要件**：问题中\"秘密窃取\"的表述及盗走功德箱的行为，可推定其具有非法占有的直接故意。  \\n\\n2. **相关法条适用分析**  \\n   - 根据《刑法》第264条，盗窃公私财物数额较大的，处3年以下有期徒刑、拘役或管制，并处或单处罚金。本案金额10000元符合该条款的\"数额较大\"标准。  \\n   - 刘X\"如实供述、自愿认罪、返还赃款\"的情节，符合《刑法》第67条第3款（坦白从宽）及司法解释关于退赃从轻处罚的规定。  \\n\\n3. **量刑情节分析**  \\n   - **从轻情节**：  \\n     - 坦白认罪（《刑法》第67条第3款）；  \\n     - 积极退赃（司法解释规定可减少基准刑30%以下）。  \\n   - **无从重情节**：问题未提及累犯、暴力手段等加重情节。  \\n\\n4. **判决适当性评价**  \\n   - 基准刑：盗窃数额较大的法定刑为3年以下，司法实践中类似案件基准刑通常为6-12个月。  \\n   - 调整幅度：坦白+退赃可减少基准刑40%以下，实际判处8个月（接近基准刑下限）符合量刑规范化要求。  \\n   - 罚金：10000元与涉案金额相当，符合《刑法》第52条\"根据犯罪情节决定罚金数额\"的规定。\\n</think>\\n\\n刘X的行为构成盗窃罪，其秘密窃取功德箱内10000元的行为完全符合盗窃罪的构成要件。法院综合考虑其坦白、退赃等从轻情节，在法定幅度内判处8个月有期徒刑并处10000元罚金，量刑适当，符合《刑法》第264条及相关量刑规则。<|im_end|>\\n',\n",
       " '__index_level_0__': 195}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4dd3e0-8046-406f-80ee-1f397615f4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>user\\nWrite a Python program that implements a basic calculator with support for addition, subtraction, multiplication, and division operations. The calculator should accept user input in the form of two numbers and an operator symbol (+, -, *, /) and display the result.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nHere\\'s a simple Python program that implements a basic calculator with support for addition, subtraction, multiplication, and division operations:\\n\\n```python\\ndef add(x, y):\\n    return x + y\\n\\ndef subtract(x, y):\\n    return x - y\\n\\ndef multiply(x, y):\\n    return x * y\\n\\ndef divide(x, y):\\n    if y == 0:\\n        raise ValueError(\"Cannot divide by zero\")\\n    return x / y\\n\\ndef main():\\n    print(\"Select operation:\")\\n    print(\"1. Add\")\\n    print(\"2. Subtract\")\\n    print(\"3. Multiply\")\\n    print(\"4. Divide\")\\n\\n    choice = input(\"Enter the number of the operation you want to perform (1/2/3/4): \")\\n\\n    num1 = float(input(\"Enter first number: \"))\\n    num2 = float(input(\"Enter second number: \"))\\n\\n    if choice == \\'1\\':\\n        result = add(num1, num2)\\n        print(f\"The result is {result}\")\\n    elif choice == \\'2\\':\\n        result = subtract(num1, num2)\\n        print(f\"The result is {result}\")\\n    elif choice == \\'3\\':\\n        result = multiply(num1, num2)\\n        print(f\"The result is {result}\")\\n    elif choice == \\'4\\':\\n        try:\\n            result = divide(num1, num2)\\n            print(f\"The result is {result}\")\\n        except ValueError as e:\\n            print(e)\\n    else:\\n        print(\"Invalid input\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nYou can run this code in your terminal or any Python environment. The program will prompt you to choose an operation, enter two numbers, and then display the result of the chosen operation on those numbers.<|im_end|>\\n',\n",
       " '__index_level_0__': 1756}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62191371-3145-4242-9630-5910f8e39c4b",
   "metadata": {},
   "source": [
    "### Step 3.执行微调流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3549b3-376d-4916-bfcf-76c87a3f84b3",
   "metadata": {},
   "source": [
    "- 进行LoRA参数注入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9eb6c-d45d-4d00-84c9-1c6e3bc5387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4268a-d50c-4c0c-861a-eddf5b1ba703",
   "metadata": {},
   "source": [
    "- 设置微调参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ad946-04c3-41ed-b2b9-fc772241f755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=176): 100%|██████████| 2000/2000 [00:53<00:00, 37.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = None, # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e08468-84e3-4829-9558-1daadca2dcac",
   "metadata": {},
   "source": [
    "此时显存占用如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b962f-629f-4194-82fd-6e380a6be6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H800 PCIe. Max memory = 79.205 GB.\n",
      "7.865 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405f18a-9348-4a8b-9eff-e0d2d09923f2",
   "metadata": {},
   "source": [
    "- 微调执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116a398-ab73-417d-8dad-2a38ed8f46f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 87,293,952/8,000,000,000 (1.09% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2323365771\u001b[0m (\u001b[33m2323365771-ff\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Qwen3/wandb/run-20250508_200514-44vqya4r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2323365771-ff/huggingface/runs/44vqya4r' target=\"_blank\">unsloth_training_checkpoints</a></strong> to <a href='https://wandb.ai/2323365771-ff/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2323365771-ff/huggingface' target=\"_blank\">https://wandb.ai/2323365771-ff/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2323365771-ff/huggingface/runs/44vqya4r' target=\"_blank\">https://wandb.ai/2323365771-ff/huggingface/runs/44vqya4r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:33, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.996800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.236800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.143400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cec4dc-640a-430b-8a4a-5937827ca1a1",
   "metadata": {},
   "source": [
    "- 微调期间显存占用检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9823e52-9659-459a-8b97-7c1cf83773c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.5196 seconds used for training.\n",
      "1.69 minutes used for training.\n",
      "Peak reserved memory = 10.135 GB.\n",
      "Peak reserved memory for training = 2.27 GB.\n",
      "Peak reserved memory % of max memory = 12.796 %.\n",
      "Peak reserved memory for training % of max memory = 2.866 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dee92-31a8-4912-bf4f-23691f7210ff",
   "metadata": {},
   "source": [
    "最后是问答效果测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ec1c5-b2ec-4522-bfd5-f318aec5af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"问题：某分裂组织在边境地区策划武装割据，并与境外势力秘密勾结，其首要分子甲在实施过程中被抓获。请问甲的行为构成何罪？应当如何量刑？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140db72-1fe1-4806-8276-a9e89b346f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"2015年9月4日14时50分许，被告人曹某某在景德镇市珠山区XX涵洞路口醉酒（血液酒精含量未明确数值）、无证驾驶无牌二轮摩托车，搭载李某林左转时未让直行车辆，与超速行驶的朱某良驾驶的出租车相撞，致李某林受伤（具体伤情等级未说明）。事故后曹某某赔偿李某林并获得谅解。公诉机关以危险驾驶罪指控，提供现场勘查记录、酒精检测等9项证据。本案中：1. 曹某某同时存在醉酒、无证、无牌三项违法情节，但缺乏具体酒精浓度数据；2. 事故系双方过错（曹某某未让行与朱某良超速）共同导致；3. 被害人已谅解。在此情况下：（1）危险驾驶罪的构成要件是否全部满足？（2）量刑时如何平衡\\\"无证+无牌+醉驾\\\"的从重情节与\\\"赔偿谅解\\\"的从轻情节？（3）最终判处3个月有期徒刑并处罚金1000元是否适当？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd09e22-c092-4331-92ba-0b41b5e985ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是关于分裂组织策划武装割据，与境外势力勾结，首要分子甲被抓获，问甲的行为构成何罪，如何量刑。首先，我需要确定中国刑法中相关的罪名。根据中国法律，分裂国家、破坏国家统一属于严重犯罪，可能涉及分裂国家罪、煽动分裂国家罪，还有可能涉及恐怖活动相关的罪名。\n",
      "\n",
      "首先，分裂国家罪和煽动分裂国家罪是《刑法》第103条规定的，如果组织、策划、实施分裂国家、破坏国家统一的行为，或者煽动他人从事这些行为，会构成相关罪名。另外，如果涉及境外势力，可能还涉及“勾结境外势力”或者“资助恐怖活动组织”等罪名。\n",
      "\n",
      "然后，根据《刑法》第103条，分裂国家罪的刑罚是三年以上十年以下有期徒刑，如果情节特别严重，可以处十年以上有期徒刑或者无期徒刑。而煽动分裂国家罪的刑罚是三年以下有期徒刑、拘役、管制或者剥夺政治权利，如果情节严重的，可以处三年以上七年以下有期徒刑。不过，这里甲是首要分子，可能属于组织、领导、参加恐怖活动组织或者恐怖活动集团的，可能适用更重的刑罚。\n",
      "\n",
      "另外，如果甲的行为涉及与境外势力勾结，可能还构成“勾结境外机构、组织、个人危害中华人民共和国主权、领土完整和安全罪”，根据《刑法》第113条，这种情况下，刑罚可以是十年以上有期徒刑、无期徒刑或者死刑。同时，如果甲的行为属于恐怖活动，可能涉及《刑法》第120条的组织、领导、参加恐怖组织罪，或者第120条之一的资助恐怖活动罪，这些罪名的刑罚通常较重，比如组织、领导恐怖组织的，处三年以上十年以下有期徒刑，情节严重的，处十年以上有期徒刑或者无该。\n",
      "\n",
      "不过，这里的关键是甲的行为是否属于恐怖活动，或者是否属于分裂国家的犯罪。如果甲的行为是策划武装割据，可能更倾向于分裂国家罪，而如果涉及与境外势力勾结，可能同时构成勾结境外势力罪。此外，是否属于“恐怖活动”需要具体分析，比如是否有使用暴力、威胁等手段，是否针对国家政权等。\n",
      "\n",
      "另外，根据《刑法》第103条，对于分裂国家罪，如果行为人是首要分子，可能被认定为“情节特别严重”，从而适用更重的刑罚。同时，如果甲在实施过程中有其他严重情节，比如造成重大损失、危害公共安全等，也可能影响量刑。\n",
      "\n",
      "需要注意的是，中国刑法对于分裂国家、破坏国家统一的犯罪有严格的法律规定，且刑罚较重。此外，与境外势力勾结的行为，如果构成犯罪，会进一步加重刑罚。因此，甲可能同时触犯多个罪名，需要根据具体行为进行数罪并罚。\n",
      "\n",
      "不过，用户的问题可能希望得到一个综合的答案，所以需要综合考虑分裂国家罪、勾结境外势力罪，以及可能的恐怖活动相关罪名。同时，量刑方面，根据情节严重程度，可能面临十年以上有期徒刑、无期徒刑甚至死刑。但具体量刑还要看案件的具体细节，比如是否有预谋、是否造成严重后果等。\n",
      "\n",
      "可能还需要参考相关司法解释，比如最高人民法院、最高人民检察院关于办理分裂国家、破坏国家统一案件的司法解释，以及关于恐怖活动犯罪的司法解释，以确定具体的定罪和量刑标准。\n",
      "\n",
      "总结来说，甲的行为可能构成分裂国家罪、勾结境外势力罪，以及可能的恐怖活动相关罪名，量刑会根据情节严重程度，可能面临十年以上有期徒刑、无期徒刑或死刑。但需要具体分析案件细节，比如是否有实际的暴力行为、是否造成严重后果、是否属于首要分子等。\n",
      "</think>\n",
      "\n",
      "根据中国刑法及相关司法解释，某分裂组织在边境地区策划武装割据并勾结境外势力的行为，涉及多个罪名，需结合具体情节综合认定。以下是法律分析及量刑建议：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、可能构成的罪名**\n",
      "1. **分裂国家罪（《刑法》第103条）**  \n",
      "   - **构成要件**：组织、策划、实施分裂国家、破坏国家统一的行为。  \n",
      "   - **适用情形**：若甲作为首要分子，策划、指挥武装割据活动，直接威胁国家统一，构成此罪。  \n",
      "   - **刑罚**：  \n",
      "     - 情节一般：3年以上10年以下有期徒刑；  \n",
      "     - 情节特别严重（如造成重大损失、严重危害国家安全等）：10年以上有期徒刑、无期徒刑或死刑。\n",
      "\n",
      "2. **勾结境外机构、组织、个人危害国家安全罪（《刑法》第113条）**  \n",
      "   - **构成要件**：与境外势力勾结，实施危害国家主权、领土完整和安全的行为。  \n",
      "   - **适用情形**：若甲与境外势力有明确勾结（如提供情报、资金、技术支持等），且行为直接危害国家安全。  \n",
      "   - **刑罚**：10年以上有期徒刑、无期徒刑或死刑。\n",
      "\n",
      "3. **组织、领导恐怖活动组织罪（《刑法》第120条）**  \n",
      "   - **构成要件**：组织、领导恐怖活动组织或恐怖活动集团。  \n",
      "   - **适用情形**：若武装割据行为符合“恐怖活动”的特征（如使用暴力、威胁手段、针对国家政权等）。  \n",
      "   - **刑罚**：3年以上10年以下有期徒刑；情节严重的，10年以上有期徒刑或无期徒刑。\n",
      "\n",
      "4. **煽动分裂国家罪（《刑法》第103条）**  \n",
      "   - **构成要件**：煽动他人分裂国家、破坏国家统一。  \n",
      "   - **适用情形**：若甲通过宣传、鼓动等方式煽动他人参与分裂活动。  \n",
      "   - **刑罚**：3年以下有期徒刑、拘役、管制或剥夺政治权利；情节严重的，3年以上7年以下有期徒刑。\n",
      "\n",
      "5. **其他可能涉及的罪名**  \n",
      "   - **资助恐怖活动组织罪（《刑法》第120条之一）**：若甲为武装割据提供资金、物资支持。  \n",
      "   - **非法获取国家秘密罪**：若涉及窃取、泄露国家机密。  \n",
      "   - **非法持有枪支弹药罪**：若使用武器实施武装割据。\n",
      "\n",
      "---\n",
      "\n",
      "### **二、量刑考量因素**\n",
      "1. **犯罪主体地位**：  \n",
      "   - 甲作为**首要分子**，组织、策划、指挥犯罪活动，量刑时需从重处罚（《刑法》第26条）。  \n",
      "   - 若甲在犯罪中起**组织、领导作用**，可能被认定为“情节特别严重”。\n",
      "\n",
      "2. **犯罪行为的严重性**：  \n",
      "   - **武装割据**是否造成人员伤亡、财产损失、社会秩序混乱等后果；  \n",
      "   - **与境外势力勾结**的深度和危害程度（如是否直接参与策划、提供关键支持）；  \n",
      "   - 是否涉及**恐怖活动**的特征（如使用暴力、威胁手段、针对国家政权等）。\n",
      "\n",
      "3. **是否具有从重情节**：  \n",
      "   - 多次实施犯罪、拒不认罪悔罪、煽动群众、造成重大社会影响等。  \n",
      "   - 若涉及**国家工作人员**或**特定身份人员**，可能加重处罚。\n",
      "\n",
      "4. **司法解释与政策导向**：  \n",
      "   - 根据《最高人民法院、最高人民检察院关于办理分裂国家、破坏国家统一案件适用法律若干问题的解释》，分裂国家罪的“情节特别严重”包括“造成严重后果”“组织大规模分裂活动”等。  \n",
      "   - 对于**恐怖活动犯罪**，司法实践中倾向于从重处罚，尤其涉及境外势力勾结的案件。\n",
      "\n",
      "---\n",
      "\n",
      "### **三、量刑建议**\n",
      "1. **若仅构成分裂国家罪**：  \n",
      "   - 情节一般：3-10年有期徒刑；  \n",
      "   - 情节特别严重（如造成重大损失、组织大规模武装活动）：10年以上有期徒刑、无期徒刑或死刑。\n",
      "\n",
      "2. **若同时构成勾结境外势力罪**：  \n",
      "   - 该罪刑罚为10年以上有期徒刑、无期徒刑或死刑，通常与分裂国家罪数罪并罚。\n",
      "\n",
      "3. **若涉及恐怖活动犯罪**：  \n",
      "   - 组织、领导恐怖活动组织罪：3-10年有期徒刑；  \n",
      "   - 若情节特别严重（如造成重大伤亡、社会动荡）：10年以上有期徒刑或无期徒刑。\n",
      "\n",
      "4. **综合量刑**：  \n",
      "   - 若甲同时构成分裂国家罪、勾结境外势力罪和恐怖活动罪，法院可能择一重罪（如勾结境外势力罪）从重处罚，或数罪并罚。  \n",
      "   - 实践中，此类案件通常面临**十年以上有期徒刑甚至死刑**，具体需结合证据和司法认定。\n",
      "\n",
      "---\n",
      "\n",
      "### **四、法律依据**\n",
      "1. **《中华人民共和国刑法》**  \n",
      "   - 第103条（分裂国家罪、煽动分裂国家罪）；  \n",
      "   - 第113条（勾结境外势力罪）；  \n",
      "   - 第120条（组织、领导恐怖活动组织罪）；  \n",
      "   - 第120条之一（资助恐怖活动罪）。\n",
      "\n",
      "2. **司法解释**  \n",
      "   - 最高人民法院、最高人民检察院关于办理分裂国家、破坏国家统一案件的司法解释；  \n",
      "   - 关于恐怖活动犯罪的司法解释。\n",
      "\n",
      "---\n",
      "\n",
      "### **五、结论**\n",
      "甲的行为可能同时构成**分裂国家罪**、**勾结境外势力罪**及**恐怖活动相关罪名**，具体罪名需结合其行为性质、手段、后果及是否符合恐怖活动特征综合认定。量刑上，若情节特别严重，可能面临**10年以上有期徒刑、无期徒刑或死刑**，司法实践中倾向于从严惩处。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : question_1}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2b8e8-9efe-4a47-bffd-95bacbc633ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好的，我现在需要分析这个关于曹某某危险驾驶罪的案例。首先，我得回忆一下危险驾驶罪的构成要件，然后看看本案中是否满足这些要件。然后，关于量刑的问题，需要考虑从重和从轻的情节，以及最终判决是否合理。让我一步步来思考。\n",
      "\n",
      "首先，危险驾驶罪的构成要件。根据中国刑法第133条之一，危险驾驶罪包括醉酒驾驶机动车、超速驾驶、追逐竞驶、严重超载等行为。这里曹某某的情况是醉酒驾驶，无证驾驶，无牌摩托车，同时发生事故，导致他人受伤。但题目中提到血液酒精含量未明确数值，所以可能需要考虑是否达到醉驾的标准。\n",
      "\n",
      "危险驾驶罪的构成要件通常包括：1. 主观上具有故意，即明知醉酒驾驶；2. 客观上实施了醉酒驾驶机动车的行为；3. 行为具有危险性，可能危害公共安全。这里曹某某醉酒驾驶，即使没有酒精浓度数据，但根据案件描述，他确实醉酒了，所以可能被认定为醉驾。但如果没有具体数值，可能需要进一步证据，比如现场检测结果，或者是否有其他证据表明他处于醉酒状态。比如，现场勘查记录、酒精检测等证据，可能已经足够证明醉驾，即使数值未明确，但可能有检测报告，或者目击者证词等。\n",
      "\n",
      "接下来，第二个问题，量刑时如何平衡从重和从轻情节。曹某某有三个违法情节：醉驾、无证、无牌，这些通常会被视为从重情节。但被害人已经谅解，且曹某某赔偿了损失，这可能属于从轻情节。需要考虑这些情节的权重，以及是否符合法律规定。\n",
      "\n",
      "第三个问题，是否判处3个月有期徒刑并处罚金1000元是否适当。这需要根据量刑指导意见，比如危险驾驶罪的量刑起点，以及从重、从轻情节的调整。比如，如果醉驾且造成事故，可能在量刑上会更重，但如果有赔偿和谅解，可能减轻。\n",
      "\n",
      "现在，我需要更详细地分析每个部分。\n",
      "\n",
      "首先，危险驾驶罪的构成要件是否满足。根据刑法，醉酒驾驶属于危险驾驶罪，但需要确认是否达到醉驾的标准。虽然题目中说血液酒精含量未明确数值，但可能有其他证据，比如现场检测报告，或者目击者描述，比如曹某某明显醉酒，所以可能被认定为醉驾。因此，危险驾驶罪的构成要件应该满足，因为醉驾是构成要件之一，而其他情节如无证、无牌可能属于加重情节，但并不影响构成要件的满足。\n",
      "\n",
      "第二个问题，量刑平衡。根据最高人民法院的量刑指导意见，对于醉驾，如果造成交通事故，且负事故责任，可能会从重处罚。但如果有赔偿和谅解，可能从轻。此外，无证、无牌属于从重情节，但赔偿谅解属于从轻情节。需要综合考虑这些因素。例如，如果事故责任是双方，那么可能责任比例不同，但曹某某未让行，可能负主要责任，所以可能需要从重。但赔偿和谅解可能减轻处罚。\n",
      "\n",
      "第三个问题，是否适当。根据危险驾驶罪的量刑，一般情况下，醉驾且造成事故，可能在拘役1-6个月，并处罚金。如果曹某某有赔偿和谅解，可能判处较轻的刑罚，比如3个月，并处罚金。但需要看是否有其他因素，比如是否造成严重后果，比如李某林受伤，但未说明伤情等级，如果伤情较轻，可能量刑较轻。此外，无证、无牌属于加重情节，但可能和醉驾一起考虑，所以3个月有期徒刑并处罚金1000元可能在合理范围内。\n",
      "\n",
      "不过，可能存在争议，比如是否因为醉驾和事故导致更重的刑罚，但赔偿和谅解可能减轻。需要进一步确认是否有其他因素，比如是否属于累犯、是否有前科等，但题目中没有提到。所以综合来看，这个判决可能适当。\n",
      "</think>\n",
      "\n",
      "### 一、危险驾驶罪的构成要件是否全部满足？\n",
      "\n",
      "**结论：构成危险驾驶罪。**\n",
      "\n",
      "1. **客观行为要件**  \n",
      "   根据《刑法》第133条之一，危险驾驶罪的客观行为包括**醉酒驾驶机动车**、**无证驾驶**、**无牌驾驶**等。本案中，曹某某存在以下行为：  \n",
      "   - **醉酒驾驶**：尽管血液酒精含量未明确数值，但结合现场勘查记录、酒精检测等证据，可推定其处于醉酒状态（如目击者证言、车辆状态等）。根据司法实践，即使未明确酒精数值，若存在醉酒驾驶的合理推定（如行为异常、车辆失控等），仍可认定为醉驾。  \n",
      "   - **无证驾驶**：未取得机动车驾驶证，直接违反《道路交通安全法》。  \n",
      "   - **无牌驾驶**：未悬挂机动车号牌，进一步加重违法性。  \n",
      "   - **事故责任**：曹某某未让直行车辆，导致与超速出租车相撞，造成李某林受伤。此行为具有**现实危险性**，符合危险驾驶罪的“危害公共安全”要件。  \n",
      "\n",
      "2. **主观故意要件**  \n",
      "   曹某某明知自身醉酒仍驾驶机动车，且未取得驾驶资格，主观上具有**放任危害结果发生**的故意，符合危险驾驶罪的主观要件。  \n",
      "\n",
      "3. **危害后果**  \n",
      "   虽未明确李某林的伤情等级，但事故已造成人身伤害，且曹某某承担事故责任（未让行），进一步证明其行为对公共安全的威胁。  \n",
      "\n",
      "**结论**：曹某某的行为符合危险驾驶罪的构成要件，且事故后果与危险驾驶行为存在直接关联。\n",
      "\n",
      "---\n",
      "\n",
      "### 二、量刑时如何平衡从重与从轻情节？\n",
      "\n",
      "**平衡原则**：  \n",
      "根据《最高人民法院关于常见犯罪的量刑指导意见》，需综合考虑**从重情节**（如醉驾、无证、无牌）与**从轻情节**（如赔偿谅解）的权重，结合事故责任比例、社会危害性等因素，确定刑罚。\n",
      "\n",
      "1. **从重情节**  \n",
      "   - **醉酒驾驶**：醉驾是危险驾驶罪的核心要件，且可能引发严重后果，通常需从重处罚。  \n",
      "   - **无证驾驶**：无证驾驶属于法律禁止性行为，直接加重违法性。  \n",
      "   - **无牌驾驶**：未悬挂号牌表明对交通管理的漠视，进一步体现主观恶性。  \n",
      "   - **事故责任**：曹某某未让行，导致事故，负主要责任，需承担更重的刑罚。  \n",
      "\n",
      "2. **从轻情节**  \n",
      "   - **赔偿与谅解**：曹某某赔偿被害人损失并获得谅解，可减轻社会危害性，体现悔罪态度。  \n",
      "   - **事故后果较轻**：李某林受伤但未明确伤情等级（如未达到重伤），未造成死亡或严重后果，可酌情从轻。  \n",
      "\n",
      "3. **量刑权重**  \n",
      "   - **从重情节**：醉驾、无证、无牌、事故责任，需在量刑起点上加重。  \n",
      "   - **从轻情节**：赔偿与谅解可抵消部分从重因素，但需结合事故责任比例。  \n",
      "   - **综合判断**：若事故责任比例为曹某某为主，但赔偿谅解显著，可将刑罚控制在**拘役1-6个月**范围内。\n",
      "\n",
      "---\n",
      "\n",
      "### 三、最终判处3个月有期徒刑并处罚金1000元是否适当？\n",
      "\n",
      "**结论：量刑适当，但需结合具体情节进一步分析。**\n",
      "\n",
      "1. **量刑起点与调整**  \n",
      "   - **基准刑**：醉驾且造成事故，根据司法解释，可能判处**拘役1-6个月**，并处罚金。  \n",
      "   - **从重情节**：无证、无牌、事故责任，可能增加刑期（如从拘役1个月增至3个月）。  \n",
      "   - **从轻情节**：赔偿与谅解可减少刑期（如从3个月减至1-2个月）。  \n",
      "\n",
      "2. **具体考量**  \n",
      "   - **事故责任**：曹某某未让行，可能承担主要责任，但未造成死亡或重伤，减轻了刑罚的严厉性。  \n",
      "   - **赔偿与谅解**：被害人谅解可显著减轻刑罚，符合“宽严相济”政策。  \n",
      "   - **社会危害性**：未造成重大伤亡，且曹某某主动赔偿，社会危害性较低。  \n",
      "\n",
      "3. **判决合理性**  \n",
      "   - **3个月有期徒刑**：符合危险驾驶罪的量刑范围，且与事故责任、赔偿谅解等因素相平衡。  \n",
      "   - **处罚金1000元**：符合“数罪并罚”或“单处罚金”的规定，且金额适中，体现惩戒与教育功能。  \n",
      "\n",
      "**结论**：综合考虑曹某某的违法情节、赔偿谅解及事故后果，3个月有期徒刑并处罚金1000元的判决在法律框架内是适当的，体现了罪责刑相适应原则。\n",
      "\n",
      "---\n",
      "\n",
      "### 总结\n",
      "\n",
      "1. **构成要件**：曹某某的行为符合危险驾驶罪的构成要件。  \n",
      "2. **量刑平衡**：需综合从重（醉驾、无证、无牌、事故责任）与从轻（赔偿谅解）情节，最终量刑应体现罪责刑相适应。  \n",
      "3. **判决适当性**：3个月有期徒刑并处罚金1000元在法律允许范围内，符合司法实践中的量刑标准。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : question_2}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 20488, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c058e-b87e-4514-bd44-3d7bded7d499",
   "metadata": {},
   "source": [
    "能够看出问答语气风格有明显优化，表述更加完整，答案也更加专业。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python unsloth",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
