analysis_report: false
api_url: http://127.0.0.1:8000/v1/chat/completions
chat_template: null
dataset_args:
  general_qa:
    aggregation: mean
    dataset_id: /home/crq/llm_scripts/evalscope/temp_huatuo
    default_subset: default
    description: A general question answering dataset for custom evaluation. For detailed
      instructions on how to use this benchmark, please refer to the [User Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa).
    eval_split: test
    extra_params: {}
    few_shot_num: 0
    few_shot_prompt_template: null
    few_shot_random: false
    filters: null
    force_redownload: false
    metric_list:
    - BLEU
    - Rouge
    name: general_qa
    output_types:
    - generation
    pretty_name: General-QA
    prompt_template: '你是一个专业的医生。请回答：

      {question}

      答案：'
    query_template: null
    review_timeout: null
    sandbox_config: {}
    shuffle: false
    shuffle_choices: false
    subset_list:
    - default
    system_prompt: null
    tags:
    - QA
    - Custom
    train_split: null
dataset_dir: /home/crq/.cache/modelscope/hub/datasets
dataset_hub: modelscope
datasets:
- general_qa
debug: false
eval_backend: Native
eval_batch_size: 8
eval_config: null
eval_type: openai_api
evalscope_version: 1.4.1
generation_config:
  batch_size: 8
  max_tokens: 4096
  stream: true
  temperature: 0.0
  top_p: 1.0
ignore_errors: false
judge_model_args: {}
judge_strategy: auto
judge_worker_num: 1
limit: 100
model: Qwen3-4B-Thinking-2507-eval
model_args: {}
model_id: Qwen3-4B-Thinking-2507-eval
model_task: text_generation
no_timestamp: false
repeats: 1
rerun_review: false
sandbox_manager_config: {}
sandbox_type: docker
seed: 42
stream: null
timeout: null
use_cache: null
use_sandbox: false
work_dir: ./outputs/20260107_230147
