2026-01-07 23:32:12 - evalscope - INFO: Running with native backend
2026-01-07 23:32:12 - evalscope - INFO: Dump task config to ./outputs/20260107_233212/configs/task_config_97a8ad.yaml
2026-01-07 23:32:12 - evalscope - INFO: {
    "model": "Qwen3-4B-Thinking-2507-eval",
    "model_id": "Qwen3-4B-Thinking-2507-eval",
    "model_args": {},
    "model_task": "text_generation",
    "chat_template": null,
    "datasets": [
        "general_qa"
    ],
    "dataset_args": {
        "general_qa": {
            "name": "general_qa",
            "dataset_id": "/home/crq/llm_scripts/evalscope/temp_huatuo",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "default"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "test",
            "prompt_template": "你是一个专业的医生。请回答：\n{question}\n答案：",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "General-QA",
            "description": "A general question answering dataset for custom evaluation. For detailed instructions on how to use this benchmark, please refer to the [User Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa).",
            "tags": [
                "QA",
                "Custom"
            ],
            "filters": null,
            "metric_list": [
                "BLEU",
                "Rouge"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        }
    },
    "dataset_dir": "/home/crq/.cache/modelscope/hub/datasets",
    "dataset_hub": "modelscope",
    "repeats": 1,
    "generation_config": {
        "batch_size": 8,
        "stream": true,
        "max_tokens": 4096,
        "top_p": 1.0,
        "temperature": 0.0
    },
    "eval_type": "openai_api",
    "eval_backend": "Native",
    "eval_config": null,
    "limit": 100,
    "eval_batch_size": 8,
    "use_cache": null,
    "rerun_review": false,
    "work_dir": "./outputs/20260107_233212",
    "no_timestamp": false,
    "ignore_errors": false,
    "debug": false,
    "seed": 42,
    "api_url": "http://127.0.0.1:8000/v1/chat/completions",
    "timeout": null,
    "stream": null,
    "judge_strategy": "auto",
    "judge_worker_num": 1,
    "judge_model_args": {},
    "analysis_report": false,
    "use_sandbox": false,
    "sandbox_type": "docker",
    "sandbox_manager_config": {},
    "evalscope_version": "1.4.1"
}
2026-01-07 23:32:12 - evalscope - INFO: Start loading benchmark dataset: general_qa
2026-01-07 23:32:12 - evalscope - WARNING: No specific dataset file found, loading the first found file: /home/crq/llm_scripts/evalscope/temp_huatuo/test.jsonl
2026-01-07 23:32:12 - evalscope - INFO: Start evaluating 1 subsets of the general_qa: ['default']
2026-01-07 23:32:12 - evalscope - INFO: Evaluating subset: default
2026-01-07 23:32:12 - evalscope - INFO: Getting predictions for subset: default
2026-01-07 23:32:12 - evalscope - INFO: Processing 100 samples, if data is large, it may take a while.
2026-01-07 23:32:12 - evalscope - INFO: Loading model for prediction...
2026-01-07 23:32:12 - evalscope - INFO: Creating model Qwen3-4B-Thinking-2507-eval with eval_type=openai_api base_url=http://127.0.0.1:8000/v1/chat/completions, config={'retries': 5, 'retry_interval': 10, 'batch_size': 8, 'stream': True, 'max_tokens': 4096, 'top_p': 1.0, 'temperature': 0.0}, model_args={}
2026-01-07 23:32:12 - evalscope - INFO: Model loaded successfully.
2026-01-07 23:33:12 - evalscope - INFO: Predicting[general_qa@default]:    0%| 0/100 [Elapsed: 01:00 < Remaining: ?, ?it/s]
2026-01-07 23:33:32 - evalscope - INFO: Evaluating [general_qa]   0%| 0/1 [Elapsed: 01:19 < Remaining: ?, ?subset/s]
