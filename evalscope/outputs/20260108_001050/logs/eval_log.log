2026-01-08 00:10:51 - evalscope - INFO: Running with native backend
2026-01-08 00:10:51 - evalscope - INFO: Dump task config to ./outputs/20260108_001050/configs/task_config_de17cf.yaml
2026-01-08 00:10:51 - evalscope - INFO: {
    "model": "Qwen3-4B-Instruct-2507-eval",
    "model_id": "Qwen3-4B-Instruct-2507-eval",
    "model_args": {},
    "model_task": "text_generation",
    "chat_template": null,
    "datasets": [
        "mmlu_pro",
        "ifeval",
        "ceval",
        "cmmlu",
        "live_code_bench",
        "math_500",
        "aime24",
        "gpqa_diamond"
    ],
    "dataset_args": {
        "mmlu_pro": {
            "name": "mmlu_pro",
            "dataset_id": "TIGER-Lab/MMLU-Pro",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "computer science",
                "math",
                "chemistry",
                "engineering",
                "law",
                "biology",
                "health",
                "physics",
                "business",
                "philosophy",
                "economics",
                "other",
                "psychology",
                "history"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": "validation",
            "eval_split": "test",
            "prompt_template": "Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: [LETTER]' (without quotes) where [LETTER] is one of {letters}. Think step by step before answering.\n\nQuestion:\n{question}\nOptions:\n{choices}\n",
            "few_shot_prompt_template": "The following are multiple choice questions (with answers) about {subject}. Think step by step and then finish your answer with 'ANSWER: [LETTER]' (without quotes) where [LETTER] is the correct letter choice.\n\n{examples}\nAnswer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: [LETTER]' (without quotes) where [LETTER] is one of {letters}. Think step by step before answering.\n\nQuestion:\n{question}\nOptions:\n{choices}\n",
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "MMLU-Pro",
            "description": "MMLU-Pro is a benchmark for evaluating language models on multiple-choice questions across various subjects. It includes questions from different domains, where the model must select the correct answer from given options.",
            "tags": [
                "MCQ",
                "Knowledge"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "ifeval": {
            "name": "ifeval",
            "dataset_id": "opencompass/ifeval",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "default"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "train",
            "prompt_template": "",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "IFEval",
            "description": "IFEval is a benchmark for evaluating instruction-following language models, focusing on their ability to understand and respond to various prompts. It includes a diverse set of tasks and metrics to assess model performance comprehensively.",
            "tags": [
                "InstructionFollowing"
            ],
            "filters": null,
            "metric_list": [
                "prompt_level_strict",
                "inst_level_strict",
                "prompt_level_loose",
                "inst_level_loose"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "ceval": {
            "name": "ceval",
            "dataset_id": "ceval-exam",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "computer_network",
                "operating_system",
                "computer_architecture",
                "college_programming",
                "college_physics",
                "college_chemistry",
                "advanced_mathematics",
                "probability_and_statistics",
                "discrete_mathematics",
                "electrical_engineer",
                "metrology_engineer",
                "high_school_mathematics",
                "high_school_physics",
                "high_school_chemistry",
                "high_school_biology",
                "middle_school_mathematics",
                "middle_school_biology",
                "middle_school_physics",
                "middle_school_chemistry",
                "veterinary_medicine",
                "college_economics",
                "business_administration",
                "marxism",
                "mao_zedong_thought",
                "education_science",
                "teacher_qualification",
                "high_school_politics",
                "high_school_geography",
                "middle_school_politics",
                "middle_school_geography",
                "modern_chinese_history",
                "ideological_and_moral_cultivation",
                "logic",
                "law",
                "chinese_language_and_literature",
                "art_studies",
                "professional_tour_guide",
                "legal_professional",
                "high_school_chinese",
                "high_school_history",
                "middle_school_history",
                "civil_servant",
                "sports_science",
                "plant_protection",
                "basic_medicine",
                "clinical_medicine",
                "urban_and_rural_planner",
                "accountant",
                "fire_engineer",
                "environmental_impact_assessment_engineer",
                "tax_accountant",
                "physician"
            ],
            "default_subset": "default",
            "few_shot_num": 5,
            "few_shot_random": false,
            "train_split": "dev",
            "eval_split": "val",
            "prompt_template": "以下是中国关于{subject}的单项选择题，请选出其中的正确答案。你的回答的最后一行应该是这样的格式：\"答案：[LETTER]\"（不带引号），其中 [LETTER] 是 A、B、C、D 中的一个。\n\n问题：{question}\n选项：\n{choices}\n",
            "few_shot_prompt_template": "以下是一些示例问题：\n\n{fewshot}\n\n",
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "C-Eval",
            "description": "C-Eval is a benchmark designed to evaluate the performance of AI models on Chinese exams across various subjects, including STEM, social sciences, and humanities. It consists of multiple-choice questions that test knowledge and reasoning abilities in these areas.",
            "tags": [
                "Knowledge",
                "MCQ",
                "Chinese"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "cmmlu": {
            "name": "cmmlu",
            "dataset_id": "evalscope/cmmlu",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "agronomy",
                "anatomy",
                "ancient_chinese",
                "arts",
                "astronomy",
                "business_ethics",
                "chinese_civil_service_exam",
                "chinese_driving_rule",
                "chinese_food_culture",
                "chinese_foreign_policy",
                "chinese_history",
                "chinese_literature",
                "chinese_teacher_qualification",
                "college_actuarial_science",
                "college_education",
                "college_engineering_hydrology",
                "college_law",
                "college_mathematics",
                "college_medical_statistics",
                "clinical_knowledge",
                "college_medicine",
                "computer_science",
                "computer_security",
                "conceptual_physics",
                "construction_project_management",
                "economics",
                "education",
                "elementary_chinese",
                "elementary_commonsense",
                "elementary_information_and_technology",
                "electrical_engineering",
                "elementary_mathematics",
                "ethnology",
                "food_science",
                "genetics",
                "global_facts",
                "high_school_biology",
                "high_school_chemistry",
                "high_school_geography",
                "high_school_mathematics",
                "high_school_physics",
                "high_school_politics",
                "human_sexuality",
                "international_law",
                "journalism",
                "jurisprudence",
                "legal_and_moral_basis",
                "logical",
                "machine_learning",
                "management",
                "marketing",
                "marxist_theory",
                "modern_chinese",
                "nutrition",
                "philosophy",
                "professional_accounting",
                "professional_law",
                "professional_medicine",
                "professional_psychology",
                "public_relations",
                "security_study",
                "sociology",
                "sports_science",
                "traditional_chinese_medicine",
                "virology",
                "world_history",
                "world_religions"
            ],
            "default_subset": "default",
            "few_shot_num": 5,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "test",
            "prompt_template": "回答下面的单项选择题，请选出其中的正确答案。你的回答的最后一行应该是这样的格式：\"答案：[LETTER]\"（不带引号），其中 [LETTER] 是 {letters} 中的一个。请在回答前进行一步步思考。\n\n问题：{question}\n选项：\n{choices}\n",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "C-MMLU",
            "description": "C-MMLU is a benchmark designed to evaluate the performance of AI models on Chinese language tasks, including reading comprehension, text classification, and more.",
            "tags": [
                "Knowledge",
                "MCQ",
                "Chinese"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "live_code_bench": {
            "name": "live_code_bench",
            "dataset_id": "AI-ModelScope/code_generation_lite",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "v5_v6"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "test",
            "prompt_template": "### Question:\n{question_content}\n\n{format_prompt} ### Answer: (use the provided format with backticks)\n\n",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "Live-Code-Bench",
            "description": "Live Code Bench is a benchmark for evaluating code generation models on real-world coding tasks. It includes a variety of programming problems with test cases to assess the model's ability to generate correct and efficient code solutions. **By default the code is executed in local environment. We recommend using sandbox execution to safely run and evaluate the generated code, please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html) for more details.**",
            "tags": [
                "Coding"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean_and_pass_at_k",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": 6,
            "extra_params": {
                "start_date": "2025-01-01",
                "end_date": "2025-04-30",
                "debug": false
            },
            "sandbox_config": {
                "image": "python:3.11-slim",
                "tools_config": {
                    "shell_executor": {},
                    "python_executor": {}
                }
            }
        },
        "math_500": {
            "name": "math_500",
            "dataset_id": "AI-ModelScope/MATH-500",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "Level 1",
                "Level 2",
                "Level 3",
                "Level 4",
                "Level 5"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "test",
            "prompt_template": "{question}\nPlease reason step by step, and put your final answer within \\boxed{{}}.",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "MATH-500",
            "description": "MATH-500 is a benchmark for evaluating mathematical reasoning capabilities of AI models. It consists of 500 diverse math problems across five levels of difficulty, designed to test a model's ability to solve complex mathematical problems by generating step-by-step solutions and providing the correct final answer.",
            "tags": [
                "Math",
                "Reasoning"
            ],
            "filters": null,
            "metric_list": [
                {
                    "acc": {
                        "numeric": true
                    }
                }
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "aime24": {
            "name": "aime24",
            "dataset_id": "HuggingFaceH4/aime_2024",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "default"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "train",
            "prompt_template": "{question}\nPlease reason step by step, and put your final answer within \\boxed{{}}.",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "AIME-2024",
            "description": "The AIME 2024 benchmark is based on problems from the American Invitational Mathematics Examination, a prestigious high school mathematics competition. This benchmark tests a model's ability to solve challenging mathematics problems by generating step-by-step solutions and providing the correct final answer.",
            "tags": [
                "Math",
                "Reasoning"
            ],
            "filters": null,
            "metric_list": [
                {
                    "acc": {
                        "numeric": true
                    }
                }
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "gpqa_diamond": {
            "name": "gpqa_diamond",
            "dataset_id": "AI-ModelScope/gpqa_diamond",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "default"
            ],
            "default_subset": "default",
            "few_shot_num": 0,
            "few_shot_random": false,
            "train_split": null,
            "eval_split": "train",
            "prompt_template": "Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: [LETTER]' (without quotes) where [LETTER] is one of {letters}. Think step by step before answering.\n\n{question}\n\n{choices}",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "GPQA-Diamond",
            "description": "GPQA is a dataset for evaluating the reasoning ability of large language models (LLMs) on complex mathematical problems. It contains questions that require step-by-step reasoning to arrive at the correct answer.",
            "tags": [
                "Knowledge",
                "MCQ"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        }
    },
    "dataset_dir": "/home/crq/.cache/modelscope/hub/datasets",
    "dataset_hub": "modelscope",
    "repeats": 1,
    "generation_config": {
        "timeout": 300000,
        "batch_size": 8,
        "stream": true,
        "max_tokens": 2048,
        "top_p": 1.0,
        "temperature": 0.0
    },
    "eval_type": "openai_api",
    "eval_backend": "Native",
    "eval_config": null,
    "limit": 100,
    "eval_batch_size": 8,
    "use_cache": null,
    "rerun_review": false,
    "work_dir": "./outputs/20260108_001050",
    "no_timestamp": false,
    "ignore_errors": false,
    "debug": false,
    "seed": 42,
    "api_url": "http://127.0.0.1:8000/v1/chat/completions",
    "timeout": null,
    "stream": null,
    "judge_strategy": "auto",
    "judge_worker_num": 1,
    "judge_model_args": {},
    "analysis_report": false,
    "use_sandbox": false,
    "sandbox_type": "docker",
    "sandbox_manager_config": {},
    "evalscope_version": "1.4.1"
}
2026-01-08 00:10:51 - evalscope - INFO: Start loading benchmark dataset: mmlu_pro
2026-01-08 00:10:52 - evalscope - INFO: Start evaluating 14 subsets of the mmlu_pro: ['computer science', 'math', 'chemistry', 'engineering', 'law', 'biology', 'health', 'physics', 'business', 'philosophy', 'economics', 'other', 'psychology', 'history']
2026-01-08 00:10:52 - evalscope - INFO: Evaluating subset: computer science
2026-01-08 00:10:52 - evalscope - INFO: Getting predictions for subset: computer science
2026-01-08 00:10:52 - evalscope - INFO: Processing 100 samples, if data is large, it may take a while.
2026-01-08 00:10:52 - evalscope - INFO: Loading model for prediction...
2026-01-08 00:10:52 - evalscope - INFO: Creating model Qwen3-4B-Instruct-2507-eval with eval_type=openai_api base_url=http://127.0.0.1:8000/v1/chat/completions, config={'timeout': 300000, 'retries': 5, 'retry_interval': 10, 'batch_size': 8, 'stream': True, 'max_tokens': 2048, 'top_p': 1.0, 'temperature': 0.0}, model_args={}
2026-01-08 00:10:52 - evalscope - INFO: Model loaded successfully.
2026-01-08 00:10:53 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:10:54 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:05 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:16 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:27 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:27 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:27 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:28 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:28 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:28 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:28 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:28 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:39 - evalscope - ERROR: {
  "input": [
    {
      "id": "5eb6976a",
      "content": "Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: [LETTER]' (without quotes) where [LETTER] is one of A,B,C,D,E,F,G. Think step by step before answering.\n\nQuestion:\nWhich of the following is NOT a property of bitmap graphics?\nOptions:\nA) They can support millions of colors\nB) Realistic lighting and shading can be done.\nC) Bitmaps can be made transparent\nD) Fast hardware exists to move blocks of pixels efficiently.\nE) Bitmap graphics can be created in multiple layers\nF) All line segments can be displayed as straight.\nG) Polygons can be filled with solid colors and textures.\n",
      "source": null,
      "metadata": null,
      "internal": null,
      "role": "user",
      "tool_call_id": null
    }
  ],
  "choices": [
    "They can support millions of colors",
    "Realistic lighting and shading can be done.",
    "Bitmaps can be made transparent",
    "Fast hardware exists to move blocks of pixels efficiently.",
    "Bitmap graphics can be created in multiple layers",
    "All line segments can be displayed as straight.",
    "Polygons can be filled with solid colors and textures."
  ],
  "target": "F",
  "id": 3,
  "group_id": 3,
  "tools": null,
  "subset_key": "computer science",
  "metadata": {
    "cot_content": "",
    "subject": "computer science",
    "question_id": 10359
  },
  "sandbox": null,
  "files": null,
  "setup": null
} prediction failed: due to Connection error.
Traceback:
Traceback (most recent call last):
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    raise exc
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/utils/function_utils.py", line 257, in run_in_threads_with_progress
    res = future.result()
          ^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py", line 183, in worker
    return self._predict_sample(sample, model_prediction_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/evaluator/evaluator.py", line 225, in _predict_sample
    task_state = self.benchmark.run_inference(model=self.model, sample=sample, output_dir=model_prediction_dir)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py", line 453, in run_inference
    model_output = self._on_inference(model, sample)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/api/benchmark/adapters/default_data_adapter.py", line 404, in _on_inference
    model_output = model.generate(input=sample.input, tools=sample.tools)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/api/model/model.py", line 198, in generate
    output = self.api.generate(
             ^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/models/openai_compatible.py", line 93, in generate
    completion = retry_call(
                 ^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/evalscope/utils/function_utils.py", line 106, in retry_call
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/crq/.conda/envs/evalscope/lib/python3.11/site-packages/openai/_base_client.py", line 1014, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

2026-01-08 00:11:39 - evalscope - INFO: Predicting[mmlu_pro@computer science]:    1%| 1/100 [Elapsed: 00:47 < Remaining: 1:17:37, 47.04s/it]
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:40 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:51 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:51 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:11:52 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:02 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:03 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:13 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:14 - evalscope - WARNING: Attempt 4 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:26 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:27 - evalscope - WARNING: Attempt 1 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:38 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:39 - evalscope - WARNING: Attempt 2 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:49 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:49 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:49 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:49 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:50 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:50 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:50 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:50 - evalscope - WARNING: Attempt 3 / 5 failed: Connection error.. Retrying...
2026-01-08 00:12:54 - evalscope - INFO: Evaluating [mmlu_pro]   0%| 0/14 [Elapsed: 02:01 < Remaining: ?, ?subset/s]
